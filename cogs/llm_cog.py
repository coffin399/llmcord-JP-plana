import discord
from discord.ext import commands
from discord import app_commands
import yaml
import openai
import json
import logging
import asyncio
import io
import base64

try:
    from plugins.search_agent import SearchAgent
except ImportError:
    logging.error(
        "plugins/search_agent.py ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€SearchAgentã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“ã€‚æ¤œç´¢æ©Ÿèƒ½ã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")
    SearchAgent = None

logger = logging.getLogger(__name__)
SUPPORTED_IMAGE_EXTENSIONS = ('.png', '.jpeg', '.jpg', '.gif', '.webp')


class LLMCog(commands.Cog, name="LLM"):
    def __init__(self, bot: commands.Bot):
        self.bot = bot
        logger.debug(f"LLMCog __init__: self.bot.config ã®ã‚¿ã‚¤ãƒ—: {type(self.bot.config)}")
        if isinstance(self.bot.config, dict): logger.debug(
            f"LLMCog __init__: self.bot.config ã®ã‚­ãƒ¼: {list(self.bot.config.keys())}")
        if not hasattr(self.bot, 'config') or not self.bot.config:
            logger.error("LLMCog: Botã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã« 'config' å±æ€§ãŒãªã„ã‹ç©ºã§ã™ã€‚è¨­å®šã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã€‚")
            raise commands.ExtensionFailed(self.qualified_name, "Botã®configãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        self.config = self.bot.config
        if 'llm' not in self.config:
            logger.error("config.yamlã« 'llm' ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚LLM Cogã‚’é–‹å§‹ã§ãã¾ã›ã‚“ã€‚")
            raise commands.ExtensionFailed(self.qualified_name, "'llm' è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        self.llm_config = self.config['llm']
        if not isinstance(self.llm_config, dict):
            logger.error(
                f"config.yamlã® 'llm' ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¾æ›¸ã§ã¯ã‚ã‚Šã¾ã›ã‚“ (ã‚¿ã‚¤ãƒ—: {type(self.llm_config)})ã€‚LLM Cogã‚’é–‹å§‹ã§ãã¾ã›ã‚“ã€‚")
            raise commands.ExtensionFailed(self.qualified_name, "'llm' è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        if hasattr(self.bot, 'cfg'): logger.warning("LLMCog: self.bot.cfg ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚ä¸Šæ›¸ãã—ã¾ã™ã€‚")
        self.bot.cfg = self.llm_config
        self.chat_histories = {}
        self.main_llm_client = None
        main_model_str = self.llm_config.get('model')
        if main_model_str: self.main_llm_client = self._initialize_llm_client(main_model_str,
                                                                              provider_config_section='providers')
        if not self.main_llm_client: logger.error(
            "ãƒ¡ã‚¤ãƒ³LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ä¸»è¦æ©Ÿèƒ½ãŒç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")
        self.search_agent = None
        if 'search' in self.llm_config.get('active_tools', []) and SearchAgent:
            search_agent_settings = self.llm_config.get('search_agent', {})
            if not search_agent_settings.get('api_key') or not search_agent_settings.get('model'):
                logger.error(
                    "SearchAgentã®è¨­å®š (llm.search_agent.api_key ã¾ãŸã¯ llm.search_agent.model) ãŒä¸è¶³ã€‚æ¤œç´¢æ©Ÿèƒ½ç„¡åŠ¹ã€‚")
            else:
                try:
                    self.search_agent = SearchAgent(self.bot); logger.info("SearchAgentãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚")
                except Exception as e:
                    logger.error(f"SearchAgentã®åˆæœŸåŒ–å¤±æ•—: {e}", exc_info=True); self.search_agent = None
        elif not SearchAgent:
            logger.info("SearchAgentã‚¯ãƒ©ã‚¹ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸å¯ã€‚æ¤œç´¢æ©Ÿèƒ½ç„¡åŠ¹ã€‚")
        else:
            logger.info("llm_configã®active_toolsã«'search'æœªæŒ‡å®šã€‚SearchAgentåˆæœŸåŒ–ã›ãšã€‚")
        if not self.llm_config.get('system_prompt'): logger.warning("system_prompt ãŒ llm_config ã«ãªã—ã€‚")

    def _initialize_llm_client(self, model_string: str, provider_config_section: str, api_key_override: str = None):
        try:
            if '/' not in model_string: logger.error(
                f"ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«å½¢å¼: '{model_string}'ã€‚'ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å/ãƒ¢ãƒ‡ãƒ«å' ã®å½¢å¼å¿…é ˆã€‚"); return None
            provider_name, model_name = model_string.split('/', 1)
            providers_settings = self.llm_config.get(provider_config_section, {});
            provider_specific_config = providers_settings.get(provider_name)
            if not provider_specific_config: logger.error(
                f"LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ '{provider_name}' è¨­å®šãŒ llm_config.{provider_config_section} ã«ãªã—ã€‚"); return None
            base_url = provider_specific_config.get('base_url');
            api_key_from_provider = provider_specific_config.get('api_key')
            actual_api_key = api_key_override if api_key_override is not None else api_key_from_provider
            is_local_provider = provider_name.lower() in ['ollama', 'oobabooga', 'jan', 'lmstudio']
            if not actual_api_key:
                if is_local_provider:
                    actual_api_key = "local-dummy-key"; logger.info(
                        f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ '{provider_name}' ãƒ¢ãƒ‡ãƒ« '{model_name}' ã«ãƒ€ãƒŸãƒ¼APIã‚­ãƒ¼ä½¿ç”¨ã€‚")
                else:
                    logger.error(
                        f"ãƒªãƒ¢ãƒ¼ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ '{provider_name}' ãƒ¢ãƒ‡ãƒ« '{model_name}' ã®APIã‚­ãƒ¼ãªã—ã€‚"); return None
            if not base_url and provider_name.lower() != "openai":
                logger.warning(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ '{provider_name}' ãƒ™ãƒ¼ã‚¹URLãªã—ã€‚OpenAIãƒ‡ãƒ•ã‚©ãƒ«ãƒˆAPIãƒ™ãƒ¼ã‚¹ä½¿ç”¨ã€‚")
            elif not base_url and provider_name.lower() == "openai":
                if not (actual_api_key and (
                        actual_api_key.startswith("sk-") or actual_api_key.startswith("gsk_"))): logger.warning(
                    f"OpenAIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§APIã‚­ãƒ¼å½¢å¼éæ¨™æº–ã€‚ãƒ™ãƒ¼ã‚¹URLãªã—ã§å•é¡Œã®å¯èƒ½æ€§ã€‚")
            elif not base_url:
                logger.error(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ '{provider_name}' ãƒ™ãƒ¼ã‚¹URLãªã—ã€‚ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ä¸å¯ã€‚"); return None
            client = openai.AsyncOpenAI(base_url=base_url, api_key=actual_api_key);
            client.model_name_for_api_calls = model_name
            logger.info(
                f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ '{provider_name}'ã€ãƒ¢ãƒ‡ãƒ« '{model_name}' LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº† (Base URL: {base_url or 'ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ'})");
            return client
        except Exception as e:
            logger.error(f"LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ '{model_string}' åˆæœŸåŒ–ä¸­ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True); return None

    def get_tools_definition(self):
        active_tool_names = self.llm_config.get('active_tools', []);
        if not active_tool_names: return None
        tools_definitions = []
        if 'search' in active_tool_names and self.search_agent and hasattr(self.search_agent, 'tool_spec'):
            tools_definitions.append(self.search_agent.tool_spec)
            logger.debug(
                f"SearchAgentã®tool_specã‚’ãƒ„ãƒ¼ãƒ«å®šç¾©ã«è¿½åŠ : {json.dumps(self.search_agent.tool_spec, indent=2)}")
        return tools_definitions if tools_definitions else None

    async def _process_attachments(self, message: discord.Message) -> list:
        image_inputs = [];
        max_images = self.llm_config.get('max_images', 1);
        processed_image_count = 0
        user_informed_about_max_images = False
        for attachment in message.attachments:
            if processed_image_count >= max_images:
                if not user_informed_about_max_images:
                    try:
                        await message.channel.send(
                            self.llm_config.get('error_msg', {}).get('msg_max_image_size',
                                                                     f"âš ï¸ æœ€å¤§ç”»åƒæ•°ã¯ {max_images} æšã§ã™ã€‚è¶…éåˆ†ã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚").format(
                                max_images=max_images),
                            silent=False
                        )
                        user_informed_about_max_images = True
                    except Exception as e_send:
                        logger.warning(f"æœ€å¤§ç”»åƒæ•°è¶…éã®é€šçŸ¥ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡å¤±æ•—: {e_send}")
                logger.info(
                    f"æœ€å¤§ç”»åƒæ•° ({max_images}æš) ã«é”ã—ãŸãŸã‚ã€æ®‹ã‚Šã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« ({attachment.filename}) ã¯ç„¡è¦–ã—ã¾ã™ã€‚")
                break
            if attachment.content_type and attachment.content_type.startswith('image/'):
                if attachment.filename.lower().endswith(SUPPORTED_IMAGE_EXTENSIONS):
                    image_inputs.append({"type": "image_url", "image_url": {"url": attachment.url}})
                    processed_image_count += 1
                    logger.info(f"æ·»ä»˜ç”»åƒã‚’LLMå…¥åŠ›ã«è¿½åŠ : {attachment.filename} (URL: {attachment.url})")
                else:
                    logger.info(f"MIMEã‚¿ã‚¤ãƒ—ã¯ç”»åƒã ãŒã€ã‚µãƒãƒ¼ãƒˆå¤–ã®æ‹¡å¼µå­ã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {attachment.filename}")
            else:
                logger.info(
                    f"ç”»åƒã§ã¯ãªã„ã€ã¾ãŸã¯ä¸æ˜ãªcontent_typeã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {attachment.filename} (Type: {attachment.content_type})")
        return image_inputs

    @commands.Cog.listener()
    async def on_message(self, message: discord.Message):
        if message.author.bot: return
        if not self.bot.user.mentioned_in(message): return

        # --- @everyone/@hereãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã®ãƒã‚§ãƒƒã‚¯ ---
        if message.mention_everyone:
            logger.info(
                f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç„¡è¦–: @everyone/@hereãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ (User: {message.author.id}, Channel: {message.channel.id})")
            return
        # --- ã“ã“ã¾ã§è¿½åŠ  ---

        allowed_channel_ids = self.config.get('allowed_channel_ids', [])
        if allowed_channel_ids and message.channel.id not in allowed_channel_ids:
            logger.debug(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç„¡è¦–: éè¨±å¯ch {message.channel.id} (User: {message.author.id})")
            return
        allowed_role_ids = self.config.get('allowed_role_ids', [])
        if allowed_role_ids and isinstance(message.author, discord.Member):
            if not any(role.id in allowed_role_ids for role in message.author.roles):
                logger.debug(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç„¡è¦–: éè¨±å¯ãƒ­ãƒ¼ãƒ« (User: {message.author.id})")
                return

        log_message_parts = [
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å—ä¿¡: User='{message.author.name}({message.author.id})'",
            f"Channel='{message.channel.name}({message.channel.id})'",
            f"Guild='{message.guild.name}({message.guild.id if message.guild else 'DM'})'"
        ]
        user_text_content_for_llm = message.content
        for mention_pattern in [f'<@!{self.bot.user.id}>', f'<@{self.bot.user.id}>']:
            user_text_content_for_llm = user_text_content_for_llm.replace(mention_pattern, '').strip()
        if user_text_content_for_llm: log_message_parts.append(f"ãƒ†ã‚­ã‚¹ãƒˆ: '{user_text_content_for_llm}'")

        if message.attachments:
            attachment_summary = [f"{att.filename} ({att.content_type or 'unknown type'})" for att in
                                  message.attachments]
            log_message_parts.append(f"æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« ({len(message.attachments)}ä»¶): {', '.join(attachment_summary)}")
        logger.info("\n".join(log_message_parts))

        history_key = message.channel.id
        if history_key not in self.chat_histories: self.chat_histories[history_key] = []

        image_contents_for_llm = await self._process_attachments(message)

        if not user_text_content_for_llm and not image_contents_for_llm:
            reply_text = self.llm_config.get('error_msg', {}).get('empty_mention_reply', "ã¯ã„ã€ã”ç”¨ä»¶ã¯ä½•ã§ã—ã‚‡ã†ã‹ï¼Ÿ")
            await message.channel.send(reply_text, silent=False)
            return

        max_text_len = self.llm_config.get('max_text', 100000)
        if len(user_text_content_for_llm) > max_text_len:
            error_template = self.llm_config.get('error_msg', {}).get('msg_max_text_size',
                                                                      "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·ã™ãã€‚æœ€å¤§ {max_text:,} å­—ã€‚")
            await message.channel.send(error_template.format(max_text=max_text_len), silent=False)
            return

        if not self.main_llm_client:
            error_msg = self.llm_config.get('error_msg', {}).get('general_error', "LLMæœªè¨­å®šã€‚å‡¦ç†ä¸å¯ã€‚")
            await message.channel.send(error_msg, silent=False)
            return

        user_input_content_parts = []
        if user_text_content_for_llm: user_input_content_parts.append(
            {"type": "text", "text": user_text_content_for_llm})
        if image_contents_for_llm: user_input_content_parts.extend(image_contents_for_llm)

        if not user_input_content_parts:
            logger.warning("LLMã«æ¸¡ã™contentãŒç©ºã§ã™ã€‚ã“ã‚Œã¯äºˆæœŸã—ãªã„çŠ¶æ³ã§ã™ã€‚")
            await message.channel.send(
                self.llm_config.get('error_msg', {}).get('general_error', "å‡¦ç†ã™ã‚‹å†…å®¹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"),
                silent=False)
            return

        user_message_for_api = {"role": "user", "content": user_input_content_parts}

        system_prompt_content = self.llm_config.get('system_prompt', "ã‚ãªãŸã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€‚")
        messages_for_llm_api = [{"role": "system", "content": system_prompt_content}]
        current_channel_history = list(self.chat_histories[history_key]);
        current_channel_history.append(user_message_for_api)
        max_hist_pairs = self.llm_config.get('max_messages', 10);
        max_hist_entries = max_hist_pairs * 2
        if len(current_channel_history) > max_hist_entries:
            num_to_remove = len(current_channel_history) - max_hist_entries
            current_channel_history = current_channel_history[num_to_remove:]
        messages_for_llm_api.extend(current_channel_history)

        try:
            async with message.channel.typing():
                current_llm_call_messages_api_format = messages_for_llm_api;
                llm_reply_text_content = None
                for i in range(self.llm_config.get('max_tool_iterations', 3)):
                    logger.debug(
                        f"LLMå‘¼ã³å‡ºã— (åå¾© {i + 1}): messages = {json.dumps(current_llm_call_messages_api_format, indent=2, ensure_ascii=False, default=str)}")
                    tools_def = self.get_tools_definition();
                    tool_choice_val = "auto" if tools_def else None
                    extra_params = self.llm_config.get('extra_api_parameters', {})
                    response = await self.main_llm_client.chat.completions.create(
                        model=self.main_llm_client.model_name_for_api_calls,
                        messages=current_llm_call_messages_api_format,
                        tools=tools_def, tool_choice=tool_choice_val,
                        temperature=extra_params.get('temperature', 0.7),
                        max_tokens=extra_params.get('max_tokens', 4096)
                    )
                    response_message = response.choices[0].message
                    current_llm_call_messages_api_format.append(response_message.model_dump(exclude_none=True))
                    if response_message.tool_calls:
                        logger.info(f"LLMãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—è¦æ±‚: {response_message.tool_calls}")
                        if not self.search_agent:
                            logger.error("SearchAgentæœªåˆæœŸåŒ–ã ãŒãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—è¦æ±‚ã‚ã‚Šã€‚")
                            for tool_call in response_message.tool_calls: current_llm_call_messages_api_format.append(
                                {"tool_call_id": tool_call.id, "role": "tool", "name": tool_call.function.name,
                                 "content": f"ã‚¨ãƒ©ãƒ¼: ãƒ„ãƒ¼ãƒ« '{tool_call.function.name}' åˆ©ç”¨ä¸å¯ã€‚"})
                            continue
                        for tool_call in response_message.tool_calls:
                            function_name = tool_call.function.name;
                            function_args_str = tool_call.function.arguments;
                            tool_output_content = ""
                            if function_name == self.search_agent.name:
                                try:
                                    function_args = json.loads(
                                        function_args_str); tool_output_content = await self.search_agent.run(
                                        arguments=function_args, bot=self.bot)
                                except Exception as e_tool:
                                    tool_output_content = f"ã‚¨ãƒ©ãƒ¼: ãƒ„ãƒ¼ãƒ« '{function_name}' å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e_tool}"; logger.error(
                                        tool_output_content, exc_info=True)
                            else:
                                tool_output_content = f"ã‚¨ãƒ©ãƒ¼: æœªå¯¾å¿œãƒ„ãƒ¼ãƒ« '{function_name}'"
                            current_llm_call_messages_api_format.append(
                                {"tool_call_id": tool_call.id, "role": "tool", "name": function_name,
                                 "content": str(tool_output_content)})
                        continue
                    else:
                        llm_reply_text_content = response_message.content; break
                if not llm_reply_text_content: llm_reply_text_content = self.llm_config.get('error_msg', {}).get(
                    'tool_loop_timeout', "ãƒ„ãƒ¼ãƒ«å‡¦ç†è¤‡é›‘ã™ãã€‚â†å¤šåˆ†èª°ã‹ã®ã‚³ãƒ¼ãƒ‰ã‚’LLMãŒå­¦ç¿’ã—ã¦ã‚‹")

            if llm_reply_text_content:
                logger.info(
                    f"LLMæœ€çµ‚å¿œç­” (User: {message.author.id}, Channel: {message.channel.id}): {llm_reply_text_content[:200]}...")
                self.chat_histories[history_key].append(user_message_for_api)
                self.chat_histories[history_key].append({"role": "assistant", "content": llm_reply_text_content})
                if len(self.chat_histories[history_key]) > max_hist_entries:
                    num_to_remove = len(self.chat_histories[history_key]) - max_hist_entries
                    self.chat_histories[history_key] = self.chat_histories[history_key][num_to_remove:]
                for chunk in self._split_message(llm_reply_text_content):
                    await message.channel.send(chunk, silent=False)
            else:
                logger.warning("LLMãŒç©ºã®æœ€çµ‚å¿œç­”ã€‚")
                await message.channel.send(self.llm_config.get('error_msg', {}).get('general_error', "AIç©ºå¿œç­”ã€‚"),
                                           silent=False)
        except openai.APIConnectionError as e:
            logger.error(f"LLM APIæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            await message.channel.send(self.llm_config.get('error_msg', {}).get('general_error', "AIæ¥ç¶šä¸å¯ã€‚"),
                                       silent=False)
        except openai.RateLimitError:
            logger.warning(f"LLM APIãƒ¬ãƒ¼ãƒˆåˆ¶é™è¶…éã€‚")
            await message.channel.send(self.llm_config.get('error_msg', {}).get('ratelimit_error', "AIæ··é›‘ä¸­ã€‚"),
                                       silent=False)
        except openai.APIStatusError as e:
            response_text = e.response.text if e.response else 'N/A';
            logger.error(f"LLM APIã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¨ãƒ©ãƒ¼: {e.status_code} - {response_text}");
            error_key = 'ratelimit_error' if e.status_code == 429 else 'general_error';
            error_template = self.llm_config.get('error_msg', {}).get(error_key, "APIã‚¨ãƒ©ãƒ¼({status_code})ã€‚");
            detail_msg = ""
            try:
                if e.response and e.response.text: error_body = json.loads(e.response.text)
                if 'error' in error_body:
                    if isinstance(error_body['error'], dict) and 'message' in error_body['error']:
                        detail_msg = f" è©³ç´°: {error_body['error']['message']}"
                    elif isinstance(error_body['error'], str):
                        detail_msg = f" è©³ç´°: {error_body['error']}"
                elif 'message' in error_body:
                    detail_msg = f" è©³ç´°: {error_body['message']}"
            except:
                pass
            await message.channel.send(error_template.format(status_code=e.status_code) + detail_msg, silent=False)
        except Exception as e:
            logger.error(f"on_messageã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            await message.channel.send(self.llm_config.get('error_msg', {}).get('general_error', "äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ã€‚"),
                                       silent=False)

    def _split_message(self, text_content: str, max_length: int = 1990):
        if not text_content: return [""]
        lines = text_content.splitlines(keepends=True);
        chunks, current_chunk = [], ""
        for line in lines:
            if len(current_chunk) + len(line) > max_length:
                if current_chunk: chunks.append(current_chunk)
                current_chunk = line
                while len(current_chunk) > max_length: chunks.append(
                    current_chunk[:max_length]); current_chunk = current_chunk[max_length:]
            else:
                current_chunk += line
        if current_chunk: chunks.append(current_chunk)
        return chunks if chunks else [""]

    @app_commands.command(name="llm_help", description="LLM (AIå¯¾è©±) æ©Ÿèƒ½ã«é–¢ã™ã‚‹è©³ç´°ãªãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
    async def llm_help_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        embed = discord.Embed(title="ğŸ’¡ LLM (AIå¯¾è©±) æ©Ÿèƒ½ ãƒ˜ãƒ«ãƒ—",
                              description=f"{self.bot.user.name if self.bot.user else 'å½“Bot'} ã®AIå¯¾è©±æ©Ÿèƒ½ã«ã¤ã„ã¦ã®èª¬æ˜ã§ã™ã€‚",
                              color=discord.Color.purple())
        if self.bot.user and self.bot.user.avatar: embed.set_thumbnail(url=self.bot.user.avatar.url)
        embed.add_field(
            name="åŸºæœ¬çš„ãªä½¿ã„æ–¹",
            value=f"â€¢ Botã«ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ (`@{self.bot.user.name if self.bot.user else 'Bot'}`) ã—ã¦è©±ã—ã‹ã‘ã‚‹ã¨ã€AIãŒå¿œç­”ã—ã¾ã™ã€‚\n"
                  f"â€¢ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«ç”»åƒã‚’æ·»ä»˜ã™ã‚‹ã¨ã€AIãŒç”»åƒã®å†…å®¹ã‚‚ç†è§£ã—ã‚ˆã†ã¨ã—ã¾ã™ï¼ˆå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®å ´åˆï¼‰ã€‚",
            inline=False
        )
        model_name = self.llm_config.get('model', 'æœªè¨­å®š');
        max_hist = self.llm_config.get('max_messages', 'æœªè¨­å®š')
        max_text_val_help = self.llm_config.get('max_text', 'æœªè¨­å®š')
        max_text_str_help = f"{max_text_val_help:,}" if isinstance(max_text_val_help, int) else str(max_text_val_help)
        embed.add_field(
            name="ç¾åœ¨ã®AIè¨­å®š",
            value=f"â€¢ **ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:** `{model_name}`\n"
                  f"â€¢ **ä¼šè©±å±¥æ­´ã®æœ€å¤§ä¿æŒæ•°:** {max_hist} ãƒšã‚¢\n"
                  f"â€¢ **æœ€å¤§å…¥åŠ›æ–‡å­—æ•°:** {max_text_str_help} æ–‡å­—\n"
                  f"â€¢ **ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹æœ€å¤§ç”»åƒæšæ•°:** {self.llm_config.get('max_images', 'æœªè¨­å®š')} æš",
            inline=False
        )
        active_tools_list = self.llm_config.get('active_tools', []);
        tools_description = ""
        if 'search' in active_tools_list and self.search_agent:
            tools_description += f"â€¢ **ã‚¦ã‚§ãƒ–æ¤œç´¢ (Search):** AIãŒå¿…è¦ã¨åˆ¤æ–­ã—ãŸå ´åˆã€æƒ…å ±ã‚’æ¤œç´¢ã—ã¦å¿œç­”ã«åˆ©ç”¨ã—ã¾ã™ã€‚\n"
            search_model = self.llm_config.get('search_agent', {}).get('model', 'æœªè¨­å®š')
            tools_description += f"  *æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«: `{search_model}`*\n"
        if not tools_description: tools_description = "ç¾åœ¨ã€ç‰¹åˆ¥ãªè¿½åŠ æ©Ÿèƒ½ï¼ˆãƒ„ãƒ¼ãƒ«ï¼‰ã¯æœ‰åŠ¹ã«ãªã£ã¦ã„ã¾ã›ã‚“ã€‚"
        embed.add_field(name="AIã®è¿½åŠ æ©Ÿèƒ½ (ãƒ„ãƒ¼ãƒ«)", value=tools_description, inline=False)
        embed.add_field(
            name="æ³¨æ„ç‚¹",
            value="â€¢ AIã¯å¸¸ã«æ­£ã—ã„æƒ…å ±ã‚’æä¾›ã™ã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚é‡è¦ãªæƒ…å ±ã¯å¿…ãšã”è‡ªèº«ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
                  "â€¢ ä¼šè©±ã¯ãƒãƒ£ãƒ³ãƒãƒ«ã”ã¨ã«ç‹¬ç«‹ã—ã¦è¨˜æ†¶ã•ã‚Œã¾ã™ã€‚\n"
                  "â€¢ ã‚ã¾ã‚Šã«ã‚‚é•·ã„ä¼šè©±ã‚„è¤‡é›‘ã™ãã‚‹æŒ‡ç¤ºã¯ã€AIãŒæ··ä¹±ã™ã‚‹åŸå› ã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n"
                  "â€¢ å€‹äººæƒ…å ±ã‚„æ©Ÿå¯†æ€§ã®é«˜ã„æƒ…å ±ã¯é€ä¿¡ã—ãªã„ã§ãã ã•ã„ã€‚",
            inline=False
        )
        embed.set_footer(text="ç¾åœ¨é–‹ç™ºä¸­ã§ã™ã€‚ä»•æ§˜ãŒå¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        await interaction.followup.send(embed=embed, ephemeral=False)
        logger.info(f"/llm_help ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸã€‚ (User: {interaction.user.id}, Guild: {interaction.guild_id})")

    @app_commands.command(name="llm_help_en",
                          description="Displays detailed help for LLM (AI Chat) features in English.")
    async def llm_help_en_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        bot_name = self.bot.user.name if self.bot.user else "This Bot"
        embed = discord.Embed(title="ğŸ’¡ LLM (AI Chat) Feature Help",
                              description=f"This is an explanation of the AI chat features for {bot_name}.",
                              color=discord.Color.purple())
        if self.bot.user and self.bot.user.avatar: embed.set_thumbnail(url=self.bot.user.avatar.url)
        embed.add_field(
            name="Basic Usage",
            value=f"â€¢ Mention the bot (`@{bot_name}`) and send a message to get a response from the AI.\n"
                  f"â€¢ If you attach images along with your message, the AI will try to understand their content (if using a compatible model).",
            inline=False
        )
        model_name_en = self.llm_config.get('model', 'Not set');
        max_hist_en = self.llm_config.get('max_messages', 'Not set')
        max_text_en_val = self.llm_config.get('max_text', 'Not set')
        max_text_en_str = f"{max_text_en_val:,}" if isinstance(max_text_en_val, int) else str(max_text_en_val)
        max_images_en = self.llm_config.get('max_images', 'Not set')
        settings_value = (f"â€¢ **Model in Use:** `{model_name_en}`\n"
                          f"â€¢ **Max Conversation History:** {max_hist_en} pairs (user and AI response form one pair)\n"
                          f"â€¢ **Max Input Text Length:** {max_text_en_str} characters\n"
                          f"â€¢ **Max Images Processed at Once:** {max_images_en} image(s)")
        embed.add_field(name="Current AI Settings", value=settings_value, inline=False)
        active_tools_list_en = self.llm_config.get('active_tools', []);
        tools_description_en = ""
        if 'search' in active_tools_list_en and self.search_agent:
            tools_description_en += f"â€¢ **Web Search (Search):** If the AI deems it necessary, it will search the internet for information to use in its response.\n"
            search_model_en = self.llm_config.get('search_agent', {}).get('model', 'Not set')
            tools_description_en += f"  *Search Agent Model: `{search_model_en}`*\n"
        if not tools_description_en: tools_description_en = "Currently, no special additional features (tools) are enabled."
        embed.add_field(name="AI's Additional Features (Tools)", value=tools_description_en, inline=False)
        embed.add_field(
            name="Tips & Important Notes",
            value="â€¢ The AI does not always provide correct information. Always verify important information yourself.\n"
                  "â€¢ Conversations are remembered separately for each channel.\n"
                  "â€¢ Excessively long conversations or overly complex instructions can confuse the AI.\n"
                  "â€¢ Do not send personal or sensitive information.",
            inline=False
        )
        embed.set_footer(text="This feature is under development and specifications may change.")
        await interaction.followup.send(embed=embed, ephemeral=False)
        logger.info(f"/llm_help_en was executed. (User: {interaction.user.id}, Guild: {interaction.guild_id})")


async def setup(bot: commands.Bot):
    if not hasattr(bot, 'config') or not bot.config:
        logger.error("LLMCog: Botã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã« 'config' å±æ€§ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ç©ºã§ã™ã€‚Cogã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã€‚")
        raise commands.ExtensionFailed("LLMCog", "Botã®configãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    try:
        cog_instance = LLMCog(bot)
        await bot.add_cog(cog_instance)
        logger.info("LLMCogãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚")
    except commands.ExtensionFailed as e:
        logger.error(f"LLMCogã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ (ExtensionFailed): {e.name} - {e.original}", exc_info=e.original)
        raise
    except Exception as e:
        logger.error(f"LLMCogã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        raise