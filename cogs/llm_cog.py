from __future__ import annotations
import discord
from discord.ext import commands
from discord import app_commands
import openai
import json
import logging
import asyncio
import io
import base64
import re
import aiohttp
import yaml
import os
from typing import List, Dict, Any, Tuple, Optional
from collections import deque

try:
    import aiofiles
except ImportError:
    aiofiles = None
    logging.warning("aiofiles library not found. Channel model settings will be saved synchronously. "
                    "Install with: pip install aiofiles")

try:
    from plugins.search_agent import SearchAgent
except ImportError:
    logging.error("Could not import SearchAgent. Search functionality will be disabled.")
    SearchAgent = None

logger = logging.getLogger(__name__)

# Constants
SUPPORTED_IMAGE_EXTENSIONS = ('.png', '.jpeg', '.jpg', '.gif', '.webp')
IMAGE_URL_PATTERN = re.compile(
    r'https?://[^\s]+\.(?:' + '|'.join(ext.lstrip('.') for ext in SUPPORTED_IMAGE_EXTENSIONS) + r')(?:\?[^\s]*)?',
    re.IGNORECASE
)
DISCORD_MESSAGE_MAX_LENGTH = 1990


class LLMCog(commands.Cog, name="LLM"):
    """A cog for interacting with Large Language Models, with tool support."""

    def __init__(self, bot: commands.Bot):
        self.bot = bot
        if not hasattr(self.bot, 'config') or not self.bot.config:
            raise commands.ExtensionFailed(self.qualified_name, "Bot config not loaded.")
        self.config = self.bot.config
        self.llm_config = self.config.get('llm')
        if not isinstance(self.llm_config, dict):
            raise commands.ExtensionFailed(self.qualified_name, "The 'llm' section in config is missing or invalid.")
        self.http_session = aiohttp.ClientSession()
        self.bot.cfg = self.llm_config
        self.conversation_threads: Dict[int, List[Dict[str, Any]]] = {}
        self.message_to_thread: Dict[int, int] = {}
        self.llm_clients: Dict[str, openai.AsyncOpenAI] = {}

        self.channel_settings_path = "data/channel_llm_models.json"
        self.channel_models: Dict[str, str] = self._load_channel_models()
        logger.info(
            f"Loaded {len(self.channel_models)} channel-specific model settings from '{self.channel_settings_path}'.")

        default_model_string = self.llm_config.get('model')
        if default_model_string:
            main_llm_client = self._initialize_llm_client(default_model_string)
            if main_llm_client:
                self.llm_clients[default_model_string] = main_llm_client
                logger.info(f"Default LLM client '{default_model_string}' initialized and cached.")
            else:
                logger.error("Failed to initialize main LLM client. Core functionality may be disabled.")
        else:
            logger.error("Default LLM model is not configured in config.yaml.")

        self.search_agent = self._initialize_search_agent()

    async def cog_unload(self):
        await self.http_session.close()
        logger.info("LLMCog's aiohttp session has been closed.")

    def _load_channel_models(self) -> Dict[str, str]:
        """ãƒãƒ£ãƒ³ãƒãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            if os.path.exists(self.channel_settings_path):
                with open(self.channel_settings_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return {str(k): v for k, v in data.items()}
        except (IOError, json.JSONDecodeError) as e:
            logger.error(f"Failed to load channel models file '{self.channel_settings_path}': {e}")
        return {}

    async def _save_channel_models(self) -> None:
        """ãƒãƒ£ãƒ³ãƒãƒ«è¨­å®šã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹"""
        try:
            os.makedirs(os.path.dirname(self.channel_settings_path), exist_ok=True)
            if aiofiles:
                async with aiofiles.open(self.channel_settings_path, 'w', encoding='utf-8') as f:
                    await f.write(json.dumps(self.channel_models, indent=4))
            else:
                with open(self.channel_settings_path, 'w', encoding='utf-8') as f:
                    json.dump(self.channel_models, f, indent=4)
        except IOError as e:
            logger.error(f"Failed to save channel models file '{self.channel_settings_path}': {e}")

    def _initialize_llm_client(self, model_string: Optional[str]) -> Optional[openai.AsyncOpenAI]:
        if not model_string or '/' not in model_string:
            logger.error(f"Invalid model format: '{model_string}'. Expected 'provider_name/model_name'.")
            return None
        try:
            provider_name, model_name = model_string.split('/', 1)
            provider_config = self.llm_config.get('providers', {}).get(provider_name)
            if not provider_config:
                logger.error(f"Configuration for LLM provider '{provider_name}' not found.")
                return None
            client = openai.AsyncOpenAI(base_url=provider_config.get('base_url'),
                                        api_key=provider_config.get('api_key') or "local-dummy-key")
            client.model_name_for_api_calls = model_name
            logger.info(f"Initialized LLM client for provider '{provider_name}' with model '{model_name}'.")
            return client
        except Exception as e:
            logger.error(f"Error initializing LLM client for '{model_string}': {e}", exc_info=True)
            return None

    async def _get_llm_client_for_channel(self, channel_id: int) -> Optional[openai.AsyncOpenAI]:
        channel_id_str = str(channel_id)
        model_string = self.channel_models.get(channel_id_str) or self.llm_config.get('model')
        if not model_string:
            logger.error("No default model is configured.")
            return None
        if model_string in self.llm_clients:
            return self.llm_clients[model_string]
        logger.info(f"Initializing a new LLM client for model '{model_string}' for channel {channel_id}")
        client = self._initialize_llm_client(model_string)
        if client:
            self.llm_clients[model_string] = client
        return client

    def _initialize_search_agent(self) -> Optional[SearchAgent]:
        if 'search' not in self.llm_config.get('active_tools', []) or not SearchAgent:
            return None
        search_config = self.llm_config.get('search_agent', {})
        if not search_config.get('api_key'):
            logger.error("SearchAgent config (api_key) is missing. Search will be disabled.")
            return None
        try:
            agent = SearchAgent(self.bot)
            logger.info("SearchAgent initialized successfully.")
            return agent
        except Exception as e:
            logger.error(f"Failed to initialize SearchAgent: {e}", exc_info=True)
            return None

    def get_tools_definition(self) -> Optional[List[Dict[str, Any]]]:
        definitions = []
        active_tools = self.llm_config.get('active_tools', [])
        if 'search' in active_tools and self.search_agent and hasattr(self.search_agent, 'tool_spec'):
            definitions.append(self.search_agent.tool_spec)
        return definitions or None

    async def _get_conversation_thread_id(self, message: discord.Message) -> int:
        if message.id in self.message_to_thread:
            return self.message_to_thread[message.id]
        current_msg = message
        visited_ids = set()
        while current_msg.reference and current_msg.reference.message_id:
            if current_msg.id in visited_ids: break
            visited_ids.add(current_msg.id)
            try:
                parent_msg = current_msg.reference.resolved or await message.channel.fetch_message(
                    current_msg.reference.message_id)
                if parent_msg.author != self.bot.user: break
                current_msg = parent_msg
            except (discord.NotFound, discord.HTTPException):
                break
        thread_id = current_msg.id
        self.message_to_thread[message.id] = thread_id
        return thread_id

    async def _collect_conversation_history(self, message: discord.Message) -> List[Dict[str, Any]]:
        history = []
        current_msg = message
        visited_ids = set()
        while current_msg.reference and current_msg.reference.message_id:
            if current_msg.reference.message_id in visited_ids: break
            visited_ids.add(current_msg.reference.message_id)
            try:
                parent_msg = current_msg.reference.resolved or await message.channel.fetch_message(
                    current_msg.reference.message_id)
                if parent_msg.author != self.bot.user:
                    image_contents, text_content = await self._prepare_multimodal_content(parent_msg)
                    text_content = text_content.replace(f'<@!{self.bot.user.id}>', '').replace(f'<@{self.bot.user.id}>',
                                                                                               '').strip()
                    if text_content or image_contents:
                        user_content_parts = [{"type": "text", "text": text_content}] if text_content else []
                        user_content_parts.extend(image_contents)
                        history.append({"role": "user", "content": user_content_parts})
                else:
                    thread_id = await self._get_conversation_thread_id(parent_msg)
                    if thread_id in self.conversation_threads:
                        for msg in self.conversation_threads[thread_id]:
                            if msg.get("role") == "assistant" and msg.get("message_id") == parent_msg.id:
                                history.append({"role": "assistant", "content": msg["content"]})
                                break
                current_msg = parent_msg
            except (discord.NotFound, discord.HTTPException):
                break
        history.reverse()
        max_history_entries = self.llm_config.get('max_messages', 10) * 2
        return history[-max_history_entries:] if len(history) > max_history_entries else history

    async def _process_image_url(self, url: str) -> Optional[Dict[str, Any]]:
        try:
            async with self.http_session.get(url) as response:
                if response.status == 200:
                    image_bytes = await response.read()
                    encoded_image = base64.b64encode(image_bytes).decode('utf-8')
                    mime_type = response.content_type
                    if not mime_type or not mime_type.startswith('image/'):
                        ext = url.split('.')[-1].lower().split('?')[0]
                        mime_type = f'image/{ext}' if ext in ('png', 'jpeg', 'gif', 'webp') else 'image/jpeg'
                    return {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{encoded_image}"}}
                else:
                    logger.warning(f"Failed to download image from {url} (Status: {response.status})")
                    return None
        except Exception as e:
            logger.error(f"Error processing image URL {url}: {e}", exc_info=True)
            return None

    async def _prepare_multimodal_content(self, message: discord.Message) -> Tuple[List[Dict[str, Any]], str]:
        image_inputs, processed_urls = [], set()
        messages_to_scan = [message]
        if message.reference and isinstance(message.reference.resolved, discord.Message):
            messages_to_scan.append(message.reference.resolved)
        source_urls = []
        for msg in messages_to_scan:
            source_urls.extend(url for url in IMAGE_URL_PATTERN.findall(msg.content) if url not in processed_urls)
            processed_urls.update(source_urls)
            source_urls.extend(att.url for att in msg.attachments if att.content_type and att.content_type.startswith(
                'image/') and att.url not in processed_urls)
            processed_urls.update(att.url for att in msg.attachments)
        max_images = self.llm_config.get('max_images', 1)
        for url in source_urls[:max_images]:
            if image_data := await self._process_image_url(url):
                image_inputs.append(image_data)
        if len(source_urls) > max_images:
            logger.info(f"Reached max image limit ({max_images}). Ignoring {len(source_urls) - max_images} images.")
            try:
                error_msg_template = self.llm_config.get('error_msg', {}).get('msg_max_image_size',
                                                                              "âš ï¸ Max images ({max_images}) reached.")
                await message.channel.send(error_msg_template.format(max_images=max_images), delete_after=10,
                                           silent=True)
            except discord.HTTPException:
                pass
        clean_text = IMAGE_URL_PATTERN.sub('', message.content).strip()
        return image_inputs, clean_text

    @commands.Cog.listener()
    async def on_message(self, message: discord.Message):
        if message.author.bot:
            return
        is_mentioned = self.bot.user.mentioned_in(message) and not message.mention_everyone
        is_reply_to_bot = (
                message.reference and
                isinstance(message.reference.resolved, discord.Message) and
                message.reference.resolved.author == self.bot.user
        )
        if not (is_mentioned or is_reply_to_bot):
            return
        if (
                allowed_channels := self.config.get('allowed_channel_ids',
                                                    [])) and message.channel.id not in allowed_channels:
            return
        try:
            llm_client = await self._get_llm_client_for_channel(message.channel.id)
            if not llm_client:
                error_msg = self.llm_config.get('error_msg', {}).get('general_error',
                                                                     "LLM client is not available for this channel.")
                await message.reply(error_msg, silent=True)
                return
        except Exception as e:
            logger.error(f"Failed to get LLM client for channel {message.channel.id}: {e}", exc_info=True)
            await message.reply(self._handle_llm_exception(e), silent=True)
            return
        image_contents, text_content = await self._prepare_multimodal_content(message)
        text_content = text_content.replace(f'<@!{self.bot.user.id}>', '').replace(f'<@{self.bot.user.id}>', '').strip()
        if not text_content and not image_contents:
            error_key = 'empty_reply' if is_reply_to_bot and not is_mentioned else 'empty_mention_reply'
            default_msg = "ä½•ã‹ãŠè©±ã—ãã ã•ã„ã€‚" if error_key == 'empty_reply' else "ã¯ã„ã€ä½•ã‹å¾¡ç”¨ã§ã—ã‚‡ã†ã‹ï¼Ÿ"
            await message.reply(self.llm_config.get('error_msg', {}).get(error_key, default_msg), silent=True)
            return
        guild_log = f"guild='{message.guild.name}({message.guild.id})'" if message.guild else "guild='DM'"
        channel_log = f"channel='{message.channel.name}({message.channel.id})'" if hasattr(message.channel,
                                                                                           'name') and message.channel.name else f"channel(id)={message.channel.id}"
        author_log = f"author='{message.author.name}({message.author.id})'"
        log_context = f"{guild_log}, {channel_log}, {author_log}"

        model_in_use = llm_client.model_name_for_api_calls
        logger.info(
            f"Received LLM request | {log_context} | model='{model_in_use}' | image_count={len(image_contents)} | text='{text_content[:150]}...' | is_reply={is_reply_to_bot}")

        thread_id = await self._get_conversation_thread_id(message)
        system_prompt = self.llm_config.get('system_prompt', "You are a helpful assistant.")
        messages_for_api: List[Dict[str, Any]] = [{"role": "system", "content": system_prompt}]
        messages_for_api.extend(await self._collect_conversation_history(message))
        user_content_parts = [{"type": "text", "text": text_content}] if text_content else []
        user_content_parts.extend(image_contents)
        user_message_for_api = {"role": "user", "content": user_content_parts}
        messages_for_api.append(user_message_for_api)
        try:
            async with message.channel.typing():
                llm_response = await self._get_llm_response(messages_for_api, log_context, llm_client,
                                                            message.channel.id)
            if llm_response:
                logger.info(
                    f"Sending LLM response | {log_context} | model='{model_in_use}' | response='{llm_response.replace(chr(10), ' ')[:150]}...'")
                if thread_id not in self.conversation_threads: self.conversation_threads[thread_id] = []
                self.conversation_threads[thread_id].append(user_message_for_api)
                sent_message = await self._send_reply_chunks(message, llm_response)
                if sent_message:
                    assistant_message = {"role": "assistant", "content": llm_response, "message_id": sent_message.id}
                    self.conversation_threads[thread_id].append(assistant_message)
                    self.message_to_thread[sent_message.id] = thread_id
                self._cleanup_old_threads()
            else:
                logger.warning(f"Received empty response from LLM | {log_context}")
                await message.reply(self.llm_config.get('error_msg', {}).get('general_error',
                                                                             "Received an empty response from the AI."),
                                    silent=True)
        except Exception as e:
            await message.reply(self._handle_llm_exception(e), silent=True)

    def _cleanup_old_threads(self):
        max_threads = 100
        if len(self.conversation_threads) > max_threads:
            threads_to_remove = list(self.conversation_threads.keys())[:len(self.conversation_threads) - max_threads]
            for thread_id in threads_to_remove:
                del self.conversation_threads[thread_id]
                self.message_to_thread = {k: v for k, v in self.message_to_thread.items() if v != thread_id}

    async def _get_llm_response(self, messages: List[Dict[str, Any]], log_context: str,
                                client: openai.AsyncOpenAI, channel_id: int) -> str:
        current_messages = messages.copy()
        max_iterations = self.llm_config.get('max_tool_iterations', 5)
        extra_params = self.llm_config.get('extra_api_parameters', {})
        for iteration in range(max_iterations):
            tools_def = self.get_tools_definition()
            api_kwargs = {
                "model": client.model_name_for_api_calls,
                "messages": current_messages,
                "temperature": extra_params.get('temperature', 0.7),
                "max_tokens": extra_params.get('max_tokens', 4096)
            }
            if tools_def:
                api_kwargs["tools"] = tools_def
                api_kwargs["tool_choice"] = "auto"
            try:
                response = await client.chat.completions.create(**api_kwargs)
                response_message = response.choices[0].message
                current_messages.append(response_message.model_dump(exclude_none=True))
                if response_message.tool_calls:
                    logger.info(
                        f"Processing {len(response_message.tool_calls)} tool call(s) in iteration {iteration + 1}")
                    await self._process_tool_calls(response_message.tool_calls, current_messages, log_context,
                                                   channel_id)
                    continue
                else:
                    return response_message.content or ""
            except Exception as e:
                logger.error(f"Error during LLM API call in iteration {iteration + 1}: {e}")
                raise
        logger.warning(f"Tool processing exceeded max iterations ({max_iterations})")
        return self.llm_config.get('error_msg', {}).get('tool_loop_timeout', "Tool processing exceeded max iterations.")

    async def _process_tool_calls(self, tool_calls: List[Any], messages: List[Dict[str, Any]],
                                  log_context: str, channel_id: int) -> None:
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            if self.search_agent and function_name == self.search_agent.name:
                try:
                    function_args = json.loads(tool_call.function.arguments)
                    query_text = function_args.get('query', 'N/A')
                    logger.info(f"Executing SearchAgent | {log_context} | query='{query_text}'")
                    search_results = await self.search_agent.run(arguments=function_args, bot=self.bot,
                                                                 channel_id=channel_id)
                    tool_response = {"tool_call_id": tool_call.id, "role": "tool", "name": function_name,
                                     "content": str(search_results)}
                    messages.append(tool_response)
                    logger.info(f"SearchAgent completed | {log_context} | result_length={len(str(search_results))}")
                except Exception as e:
                    logger.error(f"Error during tool call for {function_name}: {e}", exc_info=True)
                    error_content = f"Error: Invalid JSON arguments - {str(e)}" if isinstance(e,
                                                                                              json.JSONDecodeError) else f"Error executing search: {str(e)}"
                    messages.append(
                        {"tool_call_id": tool_call.id, "role": "tool", "name": function_name, "content": error_content})
            else:
                logger.warning(f"Received a call for an unsupported tool: {function_name} | {log_context}")
                messages.append({"tool_call_id": tool_call.id, "role": "tool", "name": function_name,
                                 "content": f"Error: Tool '{function_name}' is not available."})

    def _handle_llm_exception(self, e: Exception) -> str:
        error_detail = ""
        if isinstance(e, openai.RateLimitError):
            logger.warning(f"LLM API rate limit exceeded: {e.status_code} - {e.response.text if e.response else 'N/A'}")
            base_msg_key, default_msg = 'ratelimit_error', "âš ï¸ ç”ŸæˆAIãŒç¾åœ¨éå¸¸ã«æ··é›‘ã—ã¦ã„ã¾ã™ã€‚(Code: {status_code})"
        elif isinstance(e, (openai.APIConnectionError, openai.APITimeoutError)):
            logger.error(f"LLM API connection error: {e}")
            return self.llm_config.get('error_msg', {}).get('general_error', "Failed to connect to the AI service.")
        elif isinstance(e, openai.APIStatusError):
            logger.error(f"LLM API status error: {e.status_code} - {e.response.text if e.response else 'N/A'}")
            base_msg_key, default_msg = 'api_status_error', "AIã¨ã®é€šä¿¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚(Code: {status_code})"
        else:
            logger.error(f"An unexpected error occurred during LLM interaction: {e}", exc_info=True)
            return self.llm_config.get('error_msg', {}).get('general_error', "An unexpected error occurred.")

        if hasattr(e, 'response') and e.response:
            try:
                error_data = e.response.json()
                detail = error_data.get('detail') or error_data.get('message') or error_data.get('title')
                error_detail = f"\n> **Details**: {detail}" if detail else f"\n> **Response**: `{str(error_data)[:500]}`"
            except json.JSONDecodeError:
                error_detail = f"\n> **Raw Response**: `{e.response.text[:500]}`"

        base_message = self.llm_config.get('error_msg', {}).get(base_msg_key, default_msg).format(
            status_code=e.status_code)
        return f"{base_message}{error_detail}"[:DISCORD_MESSAGE_MAX_LENGTH]

    async def _send_reply_chunks(self, message: discord.Message, text_content: str) -> Optional[discord.Message]:
        if not text_content: return None
        chunks = self._split_message(text_content, DISCORD_MESSAGE_MAX_LENGTH)
        first_chunk = chunks.pop(0)
        first_message = None
        try:
            first_message = await message.reply(first_chunk, silent=True)
        except discord.HTTPException as e:
            logger.warning(f"Failed to reply, falling back to sending message. Error: {e}")
            first_message = await message.channel.send(first_chunk, silent=True)
        for chunk in chunks:
            await message.channel.send(chunk, silent=True)
        return first_message

    def _split_message(self, text_content: str, max_length: int) -> List[str]:
        if not text_content: return []
        chunks, current_chunk = [], io.StringIO()
        for line in text_content.splitlines(keepends=True):
            if current_chunk.tell() + len(line) > max_length:
                if chunk_val := current_chunk.getvalue(): chunks.append(chunk_val)
                current_chunk = io.StringIO()
                while len(line) > max_length:
                    chunks.append(line[:max_length])
                    line = line[max_length:]
            current_chunk.write(line)
        if final_chunk := current_chunk.getvalue(): chunks.append(final_chunk)
        return chunks if chunks else [""]

    # --- ã“ã“ã‹ã‚‰ã‚³ãƒãƒ³ãƒ‰å®šç¾© ---

    async def model_autocomplete(self, interaction: discord.Interaction, current: str) -> List[
        app_commands.Choice[str]]:
        available_models = self.llm_config.get('available_models', [])
        return [
                   app_commands.Choice(name=model, value=model)
                   for model in available_models if current.lower() in model.lower()
               ][:25]

    @app_commands.command(
        name="switch-models",
        description="ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚/Switches the AI model used for this channel."
    )
    @app_commands.describe(
        model="ä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
    )
    @app_commands.autocomplete(model=model_autocomplete)
    async def switch_model_slash(self, interaction: discord.Interaction, model: str):
        await interaction.response.defer(ephemeral=False)
        available_models = self.llm_config.get('available_models', [])
        if model not in available_models:
            await interaction.followup.send(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ« '{model}' ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
            return

        channel_id_str = str(interaction.channel_id)
        self.channel_models[channel_id_str] = model

        try:
            await self._save_channel_models()
            await self._get_llm_client_for_channel(interaction.channel_id)
            await interaction.followup.send(f"âœ… ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ãŒ `{model}` ã«åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¾ã—ãŸã€‚",
                                            ephemeral=False)
            logger.info(f"Model for channel {interaction.channel_id} switched to '{model}' by {interaction.user.name}")
        except Exception as e:
            logger.error(f"Failed to save channel model settings: {e}", exc_info=True)
            await interaction.followup.send("âŒ è¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    # --- æ–°è¦è¿½åŠ : /switch-models-default ã‚³ãƒãƒ³ãƒ‰ ---
    @app_commands.command(
        name="switch-models-default",
        description="ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚Switch to default"
    )
    async def switch_model_default_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)

        model_to_set = "mistral/mistral-medium-latest"
        available_models = self.llm_config.get('available_models', [])

        # æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if model_to_set not in available_models:
            await interaction.followup.send(
                f"âš ï¸ æ¨å¥¨ãƒ¢ãƒ‡ãƒ« `{model_to_set}` ãŒè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§åˆ©ç”¨å¯èƒ½ã«ãªã£ã¦ã„ã¾ã›ã‚“ã€‚\n"
                f"ç®¡ç†è€…ã« `config.yaml` ã® `available_models` ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹ã‚ˆã†ä¾é ¼ã—ã¦ãã ã•ã„ã€‚",
                ephemeral=True
            )
            return

        channel_id_str = str(interaction.channel_id)
        self.channel_models[channel_id_str] = model_to_set

        try:
            await self._save_channel_models()
            await self._get_llm_client_for_channel(interaction.channel_id)
            await interaction.followup.send(
                f"âœ… ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ãŒæ¨å¥¨è¨­å®šã® `{model_to_set}` ã«åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¾ã—ãŸã€‚",
                ephemeral=False)
            logger.info(
                f"Model for channel {interaction.channel_id} switched to default '{model_to_set}' by {interaction.user.name}")
        except Exception as e:
            logger.error(f"Failed to save channel model settings for default model: {e}", exc_info=True)
            await interaction.followup.send("âŒ è¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    # --- æ–°è¦è¿½åŠ ã“ã“ã¾ã§ ---

    @app_commands.command(
        name="switch-models-default-server",
        description="ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒ¼ãƒãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã«æˆ»ã—ã¾ã™ã€‚"
    )
    async def reset_model_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        channel_id_str = str(interaction.channel_id)

        if channel_id_str in self.channel_models:
            del self.channel_models[channel_id_str]
            try:
                await self._save_channel_models()
                default_model = self.llm_config.get('model', 'æœªè¨­å®š')
                await interaction.followup.send(
                    f"âœ… ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (`{default_model}`) ã«æˆ»ã—ã¾ã—ãŸã€‚", ephemeral=False)
                logger.info(f"Model for channel {interaction.channel_id} reset to default by {interaction.user.name}")
            except Exception as e:
                logger.error(f"Failed to save channel model settings after reset: {e}", exc_info=True)
                await interaction.followup.send("âŒ è¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        else:
            await interaction.followup.send("â„¹ï¸ ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«ã¯å°‚ç”¨ã®ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", ephemeral=True)

    @switch_model_slash.error
    async def switch_model_error(self, interaction: discord.Interaction, error: app_commands.AppCommandError):
        logger.error(f"Error in /switch-model command: {error}", exc_info=True)
        if not interaction.response.is_done():
            await interaction.response.send_message(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}", ephemeral=True)
        else:
            await interaction.followup.send(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}", ephemeral=True)

    @app_commands.command(name="llm_help", description="LLM (AIå¯¾è©±) æ©Ÿèƒ½ã®ãƒ˜ãƒ«ãƒ—ã¨åˆ©ç”¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
    async def llm_help_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        bot_user = self.bot.user or interaction.client.user
        bot_name = bot_user.name if bot_user else "å½“Bot"
        embed = discord.Embed(title=f"ğŸ’¡ {bot_name} AIå¯¾è©±æ©Ÿèƒ½ãƒ˜ãƒ«ãƒ—ï¼†ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
                              description=f"{bot_name}ã®AIå¯¾è©±æ©Ÿèƒ½ã«ã¤ã„ã¦ã®èª¬æ˜ã¨åˆ©ç”¨è¦ç´„ã§ã™ã€‚",
                              color=discord.Color.purple())
        if bot_user and bot_user.avatar: embed.set_thumbnail(url=bot_user.avatar.url)
        embed.add_field(
            name="åŸºæœ¬çš„ãªä½¿ã„æ–¹",
            value=(
                f"â€¢ Botã«ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ (`@{bot_name}`) ã—ã¦è©±ã—ã‹ã‘ã‚‹ã¨ã€AIãŒå¿œç­”ã—ã¾ã™ã€‚\n"
                f"â€¢ **Botã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿”ä¿¡ã™ã‚‹ã“ã¨ã§ã‚‚ä¼šè©±ã‚’ç¶šã‘ã‚‰ã‚Œã¾ã™ï¼ˆãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ä¸è¦ï¼‰ã€‚**\n"
                f"â€¢ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«ç”»åƒã‚’æ·»ä»˜ã€ã¾ãŸã¯ç”»åƒURLã‚’è²¼ã‚Šä»˜ã‘ã‚‹ã¨ã€AIãŒç”»åƒã®å†…å®¹ã‚‚ç†è§£ã—ã‚ˆã†ã¨ã—ã¾ã™ã€‚"
            ),
            inline=False
        )
        channel_model_str = self.channel_models.get(str(interaction.channel_id))
        model_display = f"`{channel_model_str}` (ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«å°‚ç”¨)" if channel_model_str else f"`{self.llm_config.get('model', 'æœªè¨­å®š')}` (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)"
        active_tools = self.llm_config.get('active_tools', [])
        tools_info = "â€¢ ãªã—" if not active_tools else "â€¢ " + ", ".join(active_tools)
        embed.add_field(name="ç¾åœ¨ã®AIè¨­å®š",
                        value=f"â€¢ **ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:** {model_display}\nâ€¢ **ä¼šè©±å±¥æ­´ã®æœ€å¤§ä¿æŒæ•°:** {self.llm_config.get('max_messages', 'æœªè¨­å®š')} ãƒšã‚¢\nâ€¢ **ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹æœ€å¤§ç”»åƒæšæ•°:** {self.llm_config.get('max_images', 'æœªè¨­å®š')} æš\nâ€¢ **åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:** {tools_info}",
                        inline=False)
        embed.add_field(name="--- ğŸ“œ AIåˆ©ç”¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ ---",
                        value="AIæ©Ÿèƒ½ã‚’å®‰å…¨ã«ã”åˆ©ç”¨ã„ãŸã ããŸã‚ã€ä»¥ä¸‹ã®å†…å®¹ã‚’å¿…ãšã”ç¢ºèªãã ã•ã„ã€‚", inline=False)
        embed.add_field(name="1. ç›®çš„ã¨å¯¾è±¡AI", value=(
            "**ã€ç›®çš„ã€‘** æœ¬ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¯ã€Botã®AIæ©Ÿèƒ½ã‚’å®‰å…¨ã«ã”åˆ©ç”¨ã„ãŸã ããŸã‚ã«ã€æŠ€è¡“çš„ãƒ»æ³•çš„ãƒªã‚¹ã‚¯ã‚’ä½æ¸›ã•ã›ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¾ã™ã€‚\n" "**ã€å¯¾è±¡AIã€‘** æœ¬Botã¯ã€å†…éƒ¨çš„ã«Mistral AIã‚„Google Geminiãªã©ã®ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£è£½ç”ŸæˆAIãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚"),
                        inline=False)
        embed.add_field(name="âš ï¸ 2. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ™‚ã®æ³¨æ„", value=(
            "ä»¥ä¸‹ã®æƒ…å ±ã¯ã€AIã®å­¦ç¿’ã‚„æ„å›³ã—ãªã„æ¼æ´©ã«ç¹‹ãŒã‚‹å±é™ºæ€§ãŒã‚ã‚‹ãŸã‚ã€**çµ¶å¯¾ã«å…¥åŠ›ã—ãªã„ã§ãã ã•ã„ã€‚**\n" "1. **å€‹äººæƒ…å ±ãƒ»ç§˜å¯†æƒ…å ±:** æ°åã€é€£çµ¡å…ˆã€NDAå¯¾è±¡æƒ…å ±ã€è‡ªçµ„ç¹”ã®æ©Ÿå¯†æƒ…å ±ãªã©\n" "2. **ç¬¬ä¸‰è€…ã®çŸ¥çš„è²¡ç”£:** è¨±å¯ã®ãªã„è‘—ä½œç‰©(æ–‡ç« ,ã‚³ãƒ¼ãƒ‰ç­‰)ã€ç™»éŒ²å•†æ¨™ã€æ„åŒ (ãƒ­ã‚´,ãƒ‡ã‚¶ã‚¤ãƒ³)ãªã©"),
                        inline=False)
        embed.add_field(name="âœ… 3. ç”Ÿæˆç‰©åˆ©ç”¨æ™‚ã®æ³¨æ„", value=(
            "1. **å†…å®¹ã®ä¸æ­£ç¢ºã•:** ç”Ÿæˆç‰©ã«ã¯è™šå½ã‚„åè¦‹ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚**å¿…ãšãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã€è‡ªå·±ã®è²¬ä»»ã§åˆ©ç”¨ã—ã¦ãã ã•ã„ã€‚**\n" "2. **æ¨©åˆ©ä¾µå®³ãƒªã‚¹ã‚¯:** ç”Ÿæˆç‰©ãŒæ„å›³ã›ãšæ—¢å­˜ã®è‘—ä½œç‰©ç­‰ã¨é¡ä¼¼ã—ã€ç¬¬ä¸‰è€…ã®æ¨©åˆ©ã‚’ä¾µå®³ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n" "3. **è‘—ä½œæ¨©ã®ä¸ç™ºç”Ÿ:** AIã«ã‚ˆã‚‹ç”Ÿæˆç‰©ã«è‘—ä½œæ¨©ã¯ç™ºç”Ÿã—ãªã„ã€ã¾ãŸã¯æ¨©åˆ©ãŒé™å®šçš„ã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n" "4. **AIãƒãƒªã‚·ãƒ¼ã®éµå®ˆ:** åŸºç›¤ã¨ãªã‚‹AIï¼ˆMistral AI, Geminiç­‰ï¼‰ã®åˆ©ç”¨è¦ç´„ã‚„ãƒãƒªã‚·ãƒ¼ã‚‚é©ç”¨ã•ã‚Œã¾ã™ã€‚"),
                        inline=False)
        embed.add_field(name="ğŸš« 4. ç¦æ­¢äº‹é …ã¨åŒæ„", value=(
            "æ³•ä»¤ã‚„å…¬åºè‰¯ä¿—ã«åã™ã‚‹åˆ©ç”¨ã€ä»–è€…ã®æ¨©åˆ©ã‚’ä¾µå®³ã™ã‚‹åˆ©ç”¨ã€å·®åˆ¥çš„ãƒ»æš´åŠ›çš„ãƒ»æ€§çš„ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç”Ÿæˆã¯å›ºãç¦ã˜ã¾ã™ã€‚\n\n" "**æœ¬Botã®åˆ©ç”¨ã‚’ã‚‚ã£ã¦ã€æœ¬ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«åŒæ„ã—ãŸã‚‚ã®ã¨ã¿ãªã—ã¾ã™ã€‚**"),
                        inline=False)
        embed.set_footer(text="ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¯äºˆå‘Šãªãå¤‰æ›´ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")
        await interaction.followup.send(embed=embed, ephemeral=False)

    @app_commands.command(name="llm_help_en",
                          description="Displays help and usage guidelines for LLM (AI Chat) features.")
    async def llm_help_en_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        bot_user = self.bot.user or interaction.client.user
        bot_name = bot_user.name if bot_user else "This Bot"
        embed = discord.Embed(title=f"ğŸ’¡ {bot_name} AI Chat Help & Guidelines",
                              description=f"Explanation and terms of use for the AI chat features of {bot_name}.",
                              color=discord.Color.purple())
        if bot_user and bot_user.avatar: embed.set_thumbnail(url=bot_user.avatar.url)
        embed.add_field(
            name="Basic Usage",
            value=(
                f"â€¢ Mention the bot (`@{bot_name}`) to get a response from the AI.\n"
                f"â€¢ **You can also continue the conversation by replying to the bot's messages (no mention needed).**\n"
                f"â€¢ Attach images or paste image URLs with your message, and the AI will try to understand them."
            ),
            inline=False
        )
        channel_model_str = self.channel_models.get(str(interaction.channel_id))
        model_display = f"`{channel_model_str}` (Channel-specific)" if channel_model_str else f"`{self.llm_config.get('model', 'Not set')}` (Default)"
        active_tools = self.llm_config.get('active_tools', [])
        tools_info = "â€¢ None" if not active_tools else "â€¢ " + ", ".join(active_tools)
        settings_value = (
            f"â€¢ **Model in Use:** {model_display}\n" f"â€¢ **Max Conversation History:** {self.llm_config.get('max_messages', 'Not set')} pairs\n" f"â€¢ **Max Images Processed at Once:** {self.llm_config.get('max_images', 'Not set')} image(s)\n" f"â€¢ **Available Tools:** {tools_info}")
        embed.add_field(name="Current AI Settings", value=settings_value, inline=False)
        embed.add_field(name="--- ğŸ“œ AI Usage Guidelines ---",
                        value="Please review the following to ensure safe use of the AI features.", inline=False)
        embed.add_field(name="1. Purpose & Target AI", value=(
            "**Purpose:** This guideline aims to reduce technical and legal risks to ensure the safe use of the bot's AI features.\n" "**Target AI:** This bot internally uses third-party generative AI models such as Mistral AI and Google Gemini."),
                        inline=False)
        embed.add_field(name="âš ï¸ 2. Precautions for Data Input", value=(
            "**NEVER input the following information**, as it poses a risk of being used for AI training or unintentional leakage.\n" "1. **Personal/Confidential Info:** Name, contact details, NDA-protected info, your organization's sensitive data, etc.\n" "2. **Third-Party IP:** Copyrighted works (text, code), trademarks, or designs without permission."),
                        inline=False)
        embed.add_field(name="âœ… 3. Precautions for Using Generated Output", value=(
            "1. **Inaccuracy:** The output may contain falsehoods. **Always fact-check and use it at your own risk.**\n" "2. **Rights Infringement Risk:** The output may unintentionally resemble existing works, potentially infringing on third-party rights.\n" "3. **No Copyright:** Copyright may not apply to AI-generated output, or rights may be limited.\n" "4. **Adherence to Policies:** The terms of the underlying AI (e.g., Mistral AI, Gemini) also apply."),
                        inline=False)
        embed.add_field(name="ğŸš« 4. Prohibited Uses & Agreement", value=(
            "Use that violates laws, infringes on rights, or generates discriminatory, violent, or explicit content is strictly prohibited.\n\n" "**By using this bot, you are deemed to have agreed to these guidelines.**"),
                        inline=False)
        embed.set_footer(text="These guidelines are subject to change without notice.")
        await interaction.followup.send(embed=embed, ephemeral=False)

    @app_commands.command(
        name="clear_history",
        description="ç¾åœ¨ã®ä¼šè©±ã‚¹ãƒ¬ãƒƒãƒ‰ã®å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚"
    )
    async def clear_history_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=True)
        cleared_count = 0
        threads_to_clear = set()
        try:
            async for msg in interaction.channel.history(limit=200):
                if msg.id in self.message_to_thread:
                    threads_to_clear.add(self.message_to_thread[msg.id])
        except (discord.Forbidden, discord.HTTPException):
            await interaction.followup.send("âš ï¸ ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        for thread_id in threads_to_clear:
            if thread_id in self.conversation_threads:
                del self.conversation_threads[thread_id]
                self.message_to_thread = {k: v for k, v in self.message_to_thread.items() if v != thread_id}
                cleared_count += 1
        if cleared_count > 0:
            await interaction.followup.send(
                f"âœ… ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«é–¢é€£ã™ã‚‹ {cleared_count} å€‹ã®ä¼šè©±ã‚¹ãƒ¬ãƒƒãƒ‰ã®å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
        else:
            await interaction.followup.send("â„¹ï¸ ã‚¯ãƒªã‚¢å¯¾è±¡ã®ä¼šè©±å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


async def setup(bot: commands.Bot):
    """Sets up the LLMCog."""
    try:
        await bot.add_cog(LLMCog(bot))
        logger.info("LLMCog loaded successfully.")
    except Exception as e:
        logger.critical(f"Failed to set up LLMCog: {e}", exc_info=True)
        raise