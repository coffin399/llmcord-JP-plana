from __future__ import annotations

import asyncio
import base64
import io
import json
import logging
import os
import re
import time
from datetime import datetime, timezone, timedelta
from types import SimpleNamespace
from typing import List, Dict, Any, Tuple, Optional, AsyncGenerator

import aiohttp
import discord
import openai
from discord import app_commands
from discord.ext import commands

from PLANA.llm.error.errors import (
    LLMExceptionHandler,
    SearchAgentError,
    SearchAPIRateLimitError,
    SearchAPIServerError
)

try:
    from PLANA.llm.plugins.search_agent import SearchAgent
except ImportError:
    logging.error("Could not import SearchAgent. Search functionality will be disabled.")
    SearchAgent = None

try:
    from PLANA.llm.plugins.bio_manager import BioManager
except ImportError:
    logging.error("Could not import BioManager. Bio functionality will be disabled.")
    BioManager = None

try:
    from PLANA.llm.plugins.memory_manager import MemoryManager
except ImportError:
    logging.error("Could not import MemoryManager. Memory functionality will be disabled.")
    MemoryManager = None

try:
    import aiofiles
except ImportError:
    aiofiles = None
    logging.warning("aiofiles library not found. Channel model settings will be saved synchronously. "
                    "Install with: pip install aiofiles")

logger = logging.getLogger(__name__)

# Constants
SUPPORTED_IMAGE_EXTENSIONS = ('.png', '.jpeg', '.jpg', '.gif', '.webp')
IMAGE_URL_PATTERN = re.compile(
    r'https?://[^\s]+\.(?:' + '|'.join(ext.lstrip('.') for ext in SUPPORTED_IMAGE_EXTENSIONS) + r')(?:\?[^\s]*)?',
    re.IGNORECASE
)
DISCORD_MESSAGE_MAX_LENGTH = 1990


class LLMCog(commands.Cog, name="LLM"):
    """A cog for interacting with Large Language Models, with tool support."""

    def __init__(self, bot: commands.Bot):
        self.bot = bot
        if not hasattr(self.bot, 'config') or not self.bot.config:
            raise commands.ExtensionFailed(self.qualified_name, "Bot config not loaded.")
        self.config = self.bot.config
        self.llm_config = self.config.get('llm')
        if not isinstance(self.llm_config, dict):
            raise commands.ExtensionFailed(self.qualified_name, "The 'llm' section in config is missing or invalid.")
        self.http_session = aiohttp.ClientSession()
        self.bot.cfg = self.llm_config
        self.conversation_threads: Dict[int, List[Dict[str, Any]]] = {}
        self.message_to_thread: Dict[int, int] = {}
        self.llm_clients: Dict[str, openai.AsyncOpenAI] = {}

        self.exception_handler = LLMExceptionHandler(self.llm_config)

        self.channel_settings_path = "data/channel_llm_models.json"
        self.channel_models: Dict[str, str] = self._load_json_data(self.channel_settings_path)
        logger.info(
            f"Loaded {len(self.channel_models)} channel-specific model settings from '{self.channel_settings_path}'.")

        self.jst = timezone(timedelta(hours=+9))

        # „Éó„É©„Ç∞„Ç§„É≥„ÅÆÂàùÊúüÂåñ
        self.search_agent = self._initialize_search_agent()
        self.bio_manager = self._initialize_bio_manager()
        self.memory_manager = self._initialize_memory_manager()

        default_model_string = self.llm_config.get('model')
        if default_model_string:
            main_llm_client = self._initialize_llm_client(default_model_string)
            if main_llm_client:
                self.llm_clients[default_model_string] = main_llm_client
                logger.info(f"Default LLM client '{default_model_string}' initialized and cached.")
            else:
                logger.error("Failed to initialize main LLM client. Core functionality may be disabled.")
        else:
            logger.error("Default LLM model is not configured in config.yaml.")

    async def cog_unload(self):
        await self.http_session.close()
        logger.info("LLMCog's aiohttp session has been closed.")

    def _load_json_data(self, path: str) -> Dict[str, Any]:
        try:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return {str(k): v for k, v in data.items()}
        except (IOError, json.JSONDecodeError) as e:
            logger.error(f"Failed to load JSON file '{path}': {e}")
        return {}

    async def _save_json_data(self, data: Dict[str, Any], path: str) -> None:
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            if aiofiles:
                async with aiofiles.open(path, 'w', encoding='utf-8') as f:
                    await f.write(json.dumps(data, indent=4, ensure_ascii=False))
            else:
                with open(path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=4, ensure_ascii=False)
        except IOError as e:
            logger.error(f"Failed to save JSON file '{path}': {e}")
            raise

    async def _save_channel_models(self) -> None:
        await self._save_json_data(self.channel_models, self.channel_settings_path)

    def _initialize_llm_client(self, model_string: Optional[str]) -> Optional[openai.AsyncOpenAI]:
        if not model_string or '/' not in model_string:
            logger.error(f"Invalid model format: '{model_string}'. Expected 'provider_name/model_name'.")
            return None
        try:
            provider_name, model_name = model_string.split('/', 1)
            provider_config = self.llm_config.get('providers', {}).get(provider_name)
            if not provider_config:
                logger.error(f"Configuration for LLM provider '{provider_name}' not found.")
                return None
            client = openai.AsyncOpenAI(base_url=provider_config.get('base_url'),
                                        api_key=provider_config.get('api_key') or "local-dummy-key")
            client.model_name_for_api_calls = model_name
            logger.info(f"Initialized LLM client for provider '{provider_name}' with model '{model_name}'.")
            return client
        except Exception as e:
            logger.error(f"Error initializing LLM client for '{model_string}': {e}", exc_info=True)
            return None

    async def _get_llm_client_for_channel(self, channel_id: int) -> Optional[openai.AsyncOpenAI]:
        channel_id_str = str(channel_id)
        model_string = self.channel_models.get(channel_id_str) or self.llm_config.get('model')
        if not model_string:
            logger.error("No default model is configured.")
            return None
        if model_string in self.llm_clients:
            return self.llm_clients[model_string]
        logger.info(f"Initializing a new LLM client for model '{model_string}' for channel {channel_id}")
        client = self._initialize_llm_client(model_string)
        if client:
            self.llm_clients[model_string] = client
        return client

    def _initialize_search_agent(self) -> Optional[SearchAgent]:
        if 'search' not in self.llm_config.get('active_tools', []) or not SearchAgent:
            return None
        search_config = self.llm_config.get('search_agent', {})
        if not search_config.get('api_key'):
            logger.error("SearchAgent config (api_key) is missing. Search will be disabled.")
            return None
        try:
            return SearchAgent(self.bot)
        except Exception as e:
            logger.error(f"Failed to initialize SearchAgent: {e}", exc_info=True)
            return None

    def _initialize_bio_manager(self) -> Optional[BioManager]:
        if not BioManager:
            return None
        try:
            return BioManager(self.bot)
        except Exception as e:
            logger.error(f"Failed to initialize BioManager: {e}", exc_info=True)
            return None

    def _initialize_memory_manager(self) -> Optional[MemoryManager]:
        if not MemoryManager:
            return None
        try:
            return MemoryManager(self.bot)
        except Exception as e:
            logger.error(f"Failed to initialize MemoryManager: {e}", exc_info=True)
            return None

    def get_tools_definition(self) -> Optional[List[Dict[str, Any]]]:
        definitions = []
        active_tools = self.llm_config.get('active_tools', [])

        if 'search' in active_tools and self.search_agent:
            definitions.append(self.search_agent.tool_spec)
        if 'user_bio' in active_tools and self.bio_manager:
            definitions.append(self.bio_manager.tool_spec)
        if 'memory' in active_tools and self.memory_manager:
            definitions.append(self.memory_manager.tool_spec)

        return definitions or None

    async def _get_conversation_thread_id(self, message: discord.Message) -> int:
        if message.id in self.message_to_thread:
            return self.message_to_thread[message.id]
        current_msg = message
        visited_ids = set()
        while current_msg.reference and current_msg.reference.message_id:
            if current_msg.id in visited_ids: break
            visited_ids.add(current_msg.id)
            try:
                parent_msg = current_msg.reference.resolved or await message.channel.fetch_message(
                    current_msg.reference.message_id)
                if parent_msg.author != self.bot.user: break
                current_msg = parent_msg
            except (discord.NotFound, discord.HTTPException):
                break
        thread_id = current_msg.id
        self.message_to_thread[message.id] = thread_id
        return thread_id

    async def _collect_conversation_history(self, message: discord.Message) -> List[Dict[str, Any]]:
        history = []
        current_msg = message
        visited_ids = set()
        while current_msg.reference and current_msg.reference.message_id:
            if current_msg.reference.message_id in visited_ids: break
            visited_ids.add(current_msg.reference.message_id)
            try:
                parent_msg = current_msg.reference.resolved or await message.channel.fetch_message(
                    current_msg.reference.message_id)
                if parent_msg.author != self.bot.user:
                    image_contents, text_content = await self._prepare_multimodal_content(parent_msg)
                    text_content = text_content.replace(f'<@!{self.bot.user.id}>', '').replace(f'<@{self.bot.user.id}>',
                                                                                               '').strip()
                    if text_content or image_contents:
                        user_content_parts = []
                        if text_content:
                            timestamp = parent_msg.created_at.astimezone(self.jst).strftime('[%H:%M]')
                            formatted_text = f"{timestamp} {text_content}"
                            user_content_parts.append({"type": "text", "text": formatted_text})

                        user_content_parts.extend(image_contents)
                        history.append({"role": "user", "content": user_content_parts})
                else:
                    thread_id = await self._get_conversation_thread_id(parent_msg)
                    if thread_id in self.conversation_threads:
                        for msg in self.conversation_threads[thread_id]:
                            if msg.get("role") == "assistant" and msg.get("message_id") == parent_msg.id:
                                history.append({"role": "assistant", "content": msg["content"]})
                                break
                current_msg = parent_msg
            except (discord.NotFound, discord.HTTPException):
                break
        history.reverse()
        max_history_entries = self.llm_config.get('max_messages', 10) * 2
        return history[-max_history_entries:] if len(history) > max_history_entries else history

    async def _process_image_url(self, url: str) -> Optional[Dict[str, Any]]:
        try:
            async with self.http_session.get(url) as response:
                if response.status == 200:
                    image_bytes = await response.read()
                    encoded_image = base64.b64encode(image_bytes).decode('utf-8')
                    mime_type = response.content_type
                    if not mime_type or not mime_type.startswith('image/'):
                        ext = url.split('.')[-1].lower().split('?')[0]
                        mime_type = f'image/{ext}' if ext in ('png', 'jpeg', 'gif', 'webp') else 'image/jpeg'
                    return {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{encoded_image}"}}
                else:
                    logger.warning(f"Failed to download image from {url} (Status: {response.status})")
                    return None
        except Exception as e:
            logger.error(f"Error processing image URL {url}: {e}", exc_info=True)
            return None

    async def _prepare_multimodal_content(self, message: discord.Message) -> Tuple[List[Dict[str, Any]], str]:
        image_inputs, processed_urls = [], set()
        messages_to_scan = [message]
        if message.reference and isinstance(message.reference.resolved, discord.Message):
            messages_to_scan.append(message.reference.resolved)
        source_urls = []
        for msg in messages_to_scan:
            source_urls.extend(url for url in IMAGE_URL_PATTERN.findall(msg.content) if url not in processed_urls)
            processed_urls.update(source_urls)
            source_urls.extend(att.url for att in msg.attachments if att.content_type and att.content_type.startswith(
                'image/') and att.url not in processed_urls)
            processed_urls.update(att.url for att in msg.attachments)
        max_images = self.llm_config.get('max_images', 1)
        for url in source_urls[:max_images]:
            if image_data := await self._process_image_url(url):
                image_inputs.append(image_data)
        if len(source_urls) > max_images:
            logger.info(f"Reached max image limit ({max_images}). Ignoring {len(source_urls) - max_images} images.")
            try:
                error_msg_template = self.llm_config.get('error_msg', {}).get('msg_max_image_size',
                                                                              "‚ö†Ô∏è Max images ({max_images}) reached.")
                await message.channel.send(error_msg_template.format(max_images=max_images), delete_after=10,
                                           silent=True)
            except discord.HTTPException:
                pass
        clean_text = IMAGE_URL_PATTERN.sub('', message.content).strip()
        return image_inputs, clean_text

    @commands.Cog.listener()
    async def on_message(self, message: discord.Message):
        if message.author.bot:
            return
        is_mentioned = self.bot.user.mentioned_in(message) and not message.mention_everyone
        is_reply_to_bot = (
                message.reference and
                isinstance(message.reference.resolved, discord.Message) and
                message.reference.resolved.author == self.bot.user
        )
        if not (is_mentioned or is_reply_to_bot):
            return

        try:
            llm_client = await self._get_llm_client_for_channel(message.channel.id)
            if not llm_client:
                error_msg = self.llm_config.get('error_msg', {}).get('general_error',
                                                                     "LLM client is not available for this channel.")
                await message.reply(error_msg, silent=True)
                return
        except Exception as e:
            logger.error(f"Failed to get LLM client for channel {message.channel.id}: {e}", exc_info=True)
            await message.reply(self.exception_handler.handle_exception(e), silent=True)
            return
        image_contents, text_content = await self._prepare_multimodal_content(message)
        text_content = text_content.replace(f'<@!{self.bot.user.id}>', '').replace(f'<@{self.bot.user.id}>', '').strip()
        if not text_content and not image_contents:
            error_key = 'empty_reply' if is_reply_to_bot and not is_mentioned else 'empty_mention_reply'
            default_msg = "‰Ωï„Åã„ÅäË©±„Åó„Åè„Å†„Åï„ÅÑ„ÄÇ" if error_key == 'empty_reply' else "„ÅØ„ÅÑ„ÄÅ‰Ωï„ÅãÂæ°Áî®„Åß„Åó„Çá„ÅÜ„ÅãÔºü"
            await message.reply(self.llm_config.get('error_msg', {}).get(error_key, default_msg), silent=True)
            return
        guild_log = f"guild='{message.guild.name}({message.guild.id})'" if message.guild else "guild='DM'"
        channel_log = f"channel='{message.channel.name}({message.channel.id})'" if hasattr(message.channel,
                                                                                           'name') and message.channel.name else f"channel(id)={message.channel.id}"
        author_log = f"author='{message.author.name}({message.author.id})'"
        log_context = f"{guild_log}, {channel_log}, {author_log}"

        model_in_use = llm_client.model_name_for_api_calls
        logger.info(
            f"üì® Received LLM request | {log_context} | model='{model_in_use}' | image_count={len(image_contents)} | is_reply={is_reply_to_bot}")
        logger.info(f"üîµ [INPUT] User text content:\n{text_content}")

        thread_id = await self._get_conversation_thread_id(message)

        if not self.bio_manager or not self.memory_manager:
            await message.reply("ÂøÖË¶Å„Å™„Éó„É©„Ç∞„Ç§„É≥„ÅåÂàùÊúüÂåñ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅ„ÄÅÂøúÁ≠î„Åß„Åç„Åæ„Åõ„Çì„ÄÇ", silent=True)
            return

        system_prompt = self.bio_manager.get_system_prompt(
            channel_id=message.channel.id,
            user_id=message.author.id,
            user_display_name=message.author.display_name
        )

        try:
            now = datetime.now(self.jst)
            current_date_str = now.strftime('%YÂπ¥%mÊúà%dÊó•')
            current_time_str = now.strftime('%H:%M')
            system_prompt = system_prompt.format(current_date=current_date_str, current_time=current_time_str)
        except (KeyError, ValueError) as e:
            logger.warning(
                f"Could not format system_prompt with date/time. It might be missing placeholders. Error: {e}")

        if formatted_memories := self.memory_manager.get_formatted_memories():
            system_prompt += f"\n\n{formatted_memories}"

        logger.info(f"üîµ [INPUT] System prompt prepared (length: {len(system_prompt)} chars)")

        messages_for_api: List[Dict[str, Any]] = [{"role": "system", "content": system_prompt}]

        conversation_history = await self._collect_conversation_history(message)
        messages_for_api.extend(conversation_history)

        user_content_parts = []
        if text_content:
            timestamp = message.created_at.astimezone(self.jst).strftime('[%H:%M]')
            formatted_text = f"{timestamp} {text_content}"
            user_content_parts.append({"type": "text", "text": formatted_text})

        user_content_parts.extend(image_contents)
        if image_contents:
            logger.info(f"üîµ [INPUT] Including {len(image_contents)} image(s) in request")

        user_message_for_api = {"role": "user", "content": user_content_parts}
        messages_for_api.append(user_message_for_api)

        logger.info(f"üîµ [INPUT] Total messages for API: {len(messages_for_api)} (system + history + user)")

        try:
            sent_message, llm_response = await self._handle_llm_streaming_response(
                message, messages_for_api, llm_client, log_context
            )

            if sent_message and llm_response:
                logger.info(f"üü¢ [OUTPUT] LLM final response (length: {len(llm_response)} chars):\n{llm_response}")
                logger.info(f"‚úÖ LLM stream finished | {log_context} | model='{model_in_use}'")

                if thread_id not in self.conversation_threads:
                    self.conversation_threads[thread_id] = []
                self.conversation_threads[thread_id].append(user_message_for_api)

                assistant_message = {"role": "assistant", "content": llm_response, "message_id": sent_message.id}
                self.conversation_threads[thread_id].append(assistant_message)
                self.message_to_thread[sent_message.id] = thread_id
                self._cleanup_old_threads()

        except Exception as e:
            await message.reply(self.exception_handler.handle_exception(e), silent=True)

    def _cleanup_old_threads(self):
        max_threads = 100
        if len(self.conversation_threads) > max_threads:
            threads_to_remove = list(self.conversation_threads.keys())[:len(self.conversation_threads) - max_threads]
            for thread_id in threads_to_remove:
                del self.conversation_threads[thread_id]
                self.message_to_thread = {k: v for k, v in self.message_to_thread.items() if v != thread_id}

    async def _handle_llm_streaming_response(
            self,
            message: discord.Message,
            initial_messages: List[Dict[str, Any]],
            client: openai.AsyncOpenAI,
            log_context: str
    ) -> Tuple[Optional[discord.Message], str]:
        sent_message = None
        full_response_text = ""
        last_update = 0.0
        last_displayed_length = 0
        chunk_count = 0
        update_interval = 0.5
        min_update_chars = 15
        retry_sleep_time = 2.0
        placeholder = ":incoming_envelope: Thinking... :incoming_envelope:"
        emoji_prefix = ":incoming_envelope: "
        emoji_suffix = " :incoming_envelope:"
        logger.info(f"üîµ [STREAMING] Starting LLM stream | {log_context}")

        try:
            sent_message = await message.reply(placeholder, silent=True)
        except discord.HTTPException:
            sent_message = await message.channel.send(placeholder, silent=True)

        try:
            stream_generator = self._llm_stream_and_tool_handler(
                initial_messages, client, log_context, message.channel.id, message.author.id
            )

            async for content_chunk in stream_generator:
                chunk_count += 1
                full_response_text += content_chunk

                if chunk_count % 100 == 0:
                    logger.debug(
                        f"üü¢ [STREAMING] Received chunk #{chunk_count}, total length: {len(full_response_text)} chars")

                current_time = time.time()
                chars_accumulated = len(full_response_text) - last_displayed_length

                should_update = (
                        current_time - last_update > update_interval and
                        chars_accumulated >= min_update_chars
                )

                if should_update and full_response_text:
                    # „Çπ„Éà„É™„Éº„Éü„É≥„Ç∞‰∏≠„ÅØÁµµÊñáÂ≠ó„ÇíÂâçÂæå„Å´ËøΩÂä†
                    emoji_suffix = " :incoming_envelope:"
                    max_content_length = DISCORD_MESSAGE_MAX_LENGTH - len(emoji_prefix) - len(emoji_suffix)
                    display_text = emoji_prefix + full_response_text[:max_content_length] + emoji_suffix

                    if display_text != sent_message.content:
                        try:
                            await sent_message.edit(content=display_text)
                            last_update = current_time
                            last_displayed_length = len(full_response_text)
                            logger.debug(
                                f"üü¢ [STREAMING] Updated Discord message (displayed: {len(display_text)} chars)")
                        except discord.NotFound:
                            logger.warning(
                                f"‚ö†Ô∏è Message deleted during stream (ID: {sent_message.id}). Aborting.")
                            return None, ""
                        except discord.HTTPException as e:
                            if e.status == 429:
                                retry_after = (e.retry_after or 1.0) + 0.5
                                logger.warning(
                                    f"‚ö†Ô∏è Rate limited on message edit (ID: {sent_message.id}). "
                                    f"Waiting {retry_after:.2f}s"
                                )
                                await asyncio.sleep(retry_after)
                                last_update = time.time()
                            else:
                                logger.warning(
                                    f"‚ö†Ô∏è Failed to edit message (ID: {sent_message.id}): "
                                    f"{e.status} - {getattr(e, 'text', str(e))}"
                                )
                                await asyncio.sleep(retry_sleep_time)

            logger.info(
                f"üü¢ [STREAMING] Stream completed | Total chunks: {chunk_count} | Final length: {len(full_response_text)} chars")

            if full_response_text:
                # „Çπ„Éà„É™„Éº„Éü„É≥„Ç∞ÂÆå‰∫ÜÂæå„ÅØÁµµÊñáÂ≠ó„ÇíÂâäÈô§„Åó„Å¶ÊúÄÁµÇ„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„ÅøË°®Á§∫
                final_text = full_response_text[:DISCORD_MESSAGE_MAX_LENGTH]
                if final_text != sent_message.content:
                    try:
                        await sent_message.edit(content=final_text)
                        logger.info(f"üü¢ [STREAMING] Final message updated successfully (emoji removed)")
                    except discord.HTTPException as e:
                        logger.error(
                            f"‚ùå Failed to update final message (ID: {sent_message.id}): {e}"
                        )
            else:
                error_msg = self.llm_config.get('error_msg', {}).get(
                    'general_error', "AI„Åã„ÇâÂøúÁ≠î„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ"
                )
                logger.warning(f"‚ö†Ô∏è Empty response from LLM")
                await sent_message.edit(content=error_msg)
                return None, ""

            return sent_message, full_response_text

        except Exception as e:
            logger.error(f"‚ùå Error during LLM streaming response: {e}", exc_info=True)
            error_msg = self.exception_handler.handle_exception(e)
            if sent_message:
                try:
                    await sent_message.edit(content=error_msg)
                except discord.HTTPException:
                    pass
            else:
                await message.reply(error_msg, silent=True)
            return None, ""

    async def _llm_stream_and_tool_handler(
            self,
            messages: List[Dict[str, Any]],
            client: openai.AsyncOpenAI,
            log_context: str,
            channel_id: int,
            user_id: int
    ) -> AsyncGenerator[str, None]:
        current_messages = messages.copy()
        max_iterations = self.llm_config.get('max_tool_iterations', 5)
        extra_params = self.llm_config.get('extra_api_parameters', {})

        for iteration in range(max_iterations):
            logger.info(
                f"üîµ [API CALL] Starting LLM API call (iteration {iteration + 1}/{max_iterations}) | {log_context}")

            tools_def = self.get_tools_definition()
            api_kwargs = {
                "model": client.model_name_for_api_calls,
                "messages": current_messages,
                "stream": True,
                "temperature": extra_params.get('temperature', 0.7),
                "max_tokens": extra_params.get('max_tokens', 4096)
            }
            if tools_def:
                api_kwargs["tools"] = tools_def
                api_kwargs["tool_choice"] = "auto"
                logger.info(f"üîß [TOOLS] Available tools: {[t['function']['name'] for t in tools_def]}")

            try:
                stream = await client.chat.completions.create(**api_kwargs)
                logger.info(f"üîµ [API CALL] Stream connection established")
            except Exception as e:
                logger.error(f"‚ùå Error calling LLM API in stream handler: {e}", exc_info=True)
                raise

            tool_calls_buffer = []
            assistant_response_content = ""
            async for chunk in stream:
                delta = chunk.choices[0].delta
                if delta and delta.content:
                    assistant_response_content += delta.content
                    yield delta.content

                if delta and delta.tool_calls:
                    for tool_call_chunk in delta.tool_calls:
                        if len(tool_calls_buffer) <= tool_call_chunk.index:
                            tool_calls_buffer.append(
                                {"id": "", "type": "function", "function": {"name": "", "arguments": ""}})

                        buffer = tool_calls_buffer[tool_call_chunk.index]
                        if tool_call_chunk.id:
                            buffer["id"] = tool_call_chunk.id
                        if tool_call_chunk.function:
                            if tool_call_chunk.function.name:
                                buffer["function"]["name"] = tool_call_chunk.function.name
                            if tool_call_chunk.function.arguments:
                                buffer["function"]["arguments"] += tool_call_chunk.function.arguments

            assistant_message = {"role": "assistant", "content": assistant_response_content or None}
            if tool_calls_buffer:
                assistant_message["tool_calls"] = tool_calls_buffer

            current_messages.append(assistant_message)

            if not tool_calls_buffer:
                logger.info(f"üü¢ [OUTPUT] No tool calls, returning final response")
                return

            logger.info(f"üîß [TOOLS] Processing {len(tool_calls_buffer)} tool call(s) in iteration {iteration + 1}")
            for tc in tool_calls_buffer:
                logger.info(
                    f"üîß [TOOLS] Tool call: {tc['function']['name']} with args: {tc['function']['arguments'][:200]}")

            tool_calls_obj = [
                SimpleNamespace(
                    id=tc['id'],
                    function=SimpleNamespace(name=tc['function']['name'], arguments=tc['function']['arguments'])
                ) for tc in tool_calls_buffer
            ]
            await self._process_tool_calls(tool_calls_obj, current_messages, log_context, channel_id, user_id)

        logger.warning(f"‚ö†Ô∏è Tool processing exceeded max iterations ({max_iterations})")
        yield self.llm_config.get('error_msg', {}).get('tool_loop_timeout', "Tool processing exceeded max iterations.")

    async def _process_tool_calls(self, tool_calls: List[Any], messages: List[Dict[str, Any]],
                                  log_context: str, channel_id: int, user_id: int) -> None:
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            error_content = None
            tool_response_content = ""

            try:
                function_args = json.loads(tool_call.function.arguments)
                logger.info(f"üîß [TOOL EXEC] Executing {function_name} | {log_context}")
                logger.info(f"üîß [TOOL EXEC] Arguments: {json.dumps(function_args, ensure_ascii=False, indent=2)}")

                if self.search_agent and function_name == self.search_agent.name:
                    query_text = function_args.get('query', 'N/A')
                    logger.info(f"üîç [SEARCH] Query: '{query_text}'")
                    tool_response_content = await self.search_agent.run(arguments=function_args, bot=self.bot,
                                                                        channel_id=channel_id)
                    logger.info(
                        f"üîç [SEARCH] Result (length: {len(str(tool_response_content))} chars):\n{str(tool_response_content)[:1000]}")

                elif self.bio_manager and function_name == self.bio_manager.name:
                    logger.info(f"üë§ [BIO] Executing bio manager tool")
                    tool_response_content = await self.bio_manager.run_tool(arguments=function_args, user_id=user_id)
                    logger.info(f"üë§ [BIO] Result:\n{tool_response_content}")

                elif self.memory_manager and function_name == self.memory_manager.name:
                    logger.info(f"üß† [MEMORY] Executing memory manager tool")
                    tool_response_content = await self.memory_manager.run_tool(arguments=function_args)
                    logger.info(f"üß† [MEMORY] Result:\n{tool_response_content}")

                else:
                    logger.warning(f"‚ö†Ô∏è Unsupported tool called: {function_name} | {log_context}")
                    error_content = f"Error: Tool '{function_name}' is not available."

            except json.JSONDecodeError as e:
                logger.error(f"‚ùå Error decoding tool arguments for {function_name}: {e}", exc_info=True)
                error_content = f"Error: Invalid JSON arguments - {str(e)}"
            except SearchAPIRateLimitError as e:
                logger.warning(f"‚ö†Ô∏è SearchAgent rate limit hit: {e}")
                error_content = "[Google Search Error]\nGoogleÊ§úÁ¥¢API„ÅÆÂà©Áî®Âà∂Èôê„Å´ÈÅî„Åó„Åæ„Åó„Åü„ÄÇÊôÇÈñì„ÇíÁΩÆ„ÅÑ„Å¶„Åã„ÇâÂÜçË©¶Ë°å„Åô„Çã„Çà„ÅÜ„Å´„É¶„Éº„Ç∂„Éº„Å´‰ºù„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
            except SearchAPIServerError as e:
                logger.error(f"‚ùå SearchAgent server error: {e}")
                error_content = "[Google Search Error]\nÊ§úÁ¥¢„Çµ„Éº„Éì„Çπ„Åß‰∏ÄÊôÇÁöÑ„Å™„Çµ„Éº„Éê„Éº„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü„ÄÇÊôÇÈñì„ÇíÁΩÆ„ÅÑ„Å¶„Åã„ÇâÂÜçË©¶Ë°å„Åô„Çã„Çà„ÅÜ„Å´„É¶„Éº„Ç∂„Éº„Å´‰ºù„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
            except SearchAgentError as e:
                logger.error(f"‚ùå Error during SearchAgent execution for {function_name}: {e}", exc_info=True)
                error_content = f"[Google Search Error]\nÊ§úÁ¥¢„ÅÆÂÆüË°å‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}"
            except Exception as e:
                logger.error(f"‚ùå Unexpected error during tool call for {function_name}: {e}", exc_info=True)
                error_content = f"[Tool Error]\n‰∫àÊúü„Åó„Å™„ÅÑ„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}"

            final_content = error_content if error_content else tool_response_content
            logger.info(f"üîß [TOOL RESULT] Sending tool response back to LLM (length: {len(final_content)} chars)")

            messages.append({
                "tool_call_id": tool_call.id,
                "role": "tool",
                "name": function_name,
                "content": final_content
            })

    @app_commands.command(
        name="set-ai-bio",
        description="„Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„ÅÆAI„ÅÆÊÄßÊ†º„ÇÑÂΩπÂâ≤(bio)„ÇíË®≠ÂÆö„Åó„Åæ„Åô„ÄÇ/ Set the AI's personality/role (bio) for this channel."
    )
    @app_commands.describe(
        bio="AI„Å´Ë®≠ÂÆö„Åó„Åü„ÅÑÊÄßÊ†º„ÇÑÂΩπÂâ≤„ÇíË®òËø∞„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ(‰æã: „ÅÇ„Å™„Åü„ÅØÁå´„Åß„Åô„ÄÇË™ûÂ∞æ„Å´„Äå„Å´„ÇÉ„Çì„Äç„Çí„Å§„Åë„Å¶Ë©±„Åó„Åæ„Åô„ÄÇ)"
    )
    async def set_ai_bio_slash(self, interaction: discord.Interaction, bio: str):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("‚ùå BioManager„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)
            return

        if len(bio) > 1024:
            await interaction.followup.send("‚ö†Ô∏è AI„ÅÆbio„ÅåÈï∑„Åô„Åé„Åæ„Åô„ÄÇ1024ÊñáÂ≠ó‰ª•ÂÜÖ„ÅßË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ", ephemeral=False)
            return

        try:
            await self.bio_manager.set_channel_bio(interaction.channel_id, bio)
            logger.info(f"AI bio for channel {interaction.channel_id} set by {interaction.user.name}")
            embed = discord.Embed(
                title="‚úÖ AI„ÅÆbio„ÇíË®≠ÂÆö„Åó„Åæ„Åó„Åü",
                description=f"„Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„Åß„ÅÆAI„ÅÆÂΩπÂâ≤„Åå‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´Ë®≠ÂÆö„Åï„Çå„Åæ„Åó„Åü„ÄÇ\n\n**Êñ∞„Åó„ÅÑAI„ÅÆbio:**\n```\n{bio}\n```",
                color=discord.Color.green()
            )
            await interaction.followup.send(embed=embed, ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save channel AI bio settings: {e}", exc_info=True)
            await interaction.followup.send("‚ùå AI„ÅÆbioË®≠ÂÆö„ÅÆ‰øùÂ≠ò„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ", ephemeral=False)

    @app_commands.command(
        name="show-ai-bio",
        description="„Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„ÅÆAI„Å´ÁèæÂú®Ë®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Çãbio„ÇíË°®Á§∫„Åó„Åæ„Åô„ÄÇ/ Show the AI's current bio for this channel."
    )
    async def show_ai_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("‚ùå BioManager„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)
            return

        current_bio = self.bio_manager.get_channel_bio(interaction.channel_id)
        if current_bio:
            title = "ÁèæÂú®„ÅÆAI„ÅÆbio"
            description = f"„Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„Åß„ÅØ„ÄÅAI„Å´‰ª•‰∏ã„ÅÆÂΩπÂâ≤„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n**AI„ÅÆbio:**\n```\n{current_bio}\n```"
            color = discord.Color.blue()
        else:
            default_prompt = self.llm_config.get('system_prompt', "Ë®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ")
            try:
                now = datetime.now(self.jst)
                current_date_str = now.strftime('%YÂπ¥%mÊúà%dÊó•')
                current_time_str = now.strftime('%H:%M')
                formatted_prompt = default_prompt.format(current_date=current_date_str, current_time=current_time_str)
            except (KeyError, ValueError):
                formatted_prompt = default_prompt

            title = "ÁèæÂú®„ÅÆAI„ÅÆbio"
            description = f"„Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„Å´„ÅØÂ∞ÇÁî®„ÅÆAI bio„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ\n„Çµ„Éº„Éê„Éº„ÅÆ„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„Åå‰ΩøÁî®„Åï„Çå„Åæ„Åô„ÄÇ\n\n**„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö:**\n```\n{formatted_prompt}\n```"
            color = discord.Color.greyple()
        embed = discord.Embed(title=title, description=description, color=color)
        await interaction.followup.send(embed=embed, ephemeral=False)

    @app_commands.command(
        name="reset-ai-bio",
        description="„Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„ÅÆAI„ÅÆbio„Çí„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„Å´Êàª„Åó„Åæ„Åô„ÄÇ/ Reset the AI's bio to default for this channel."
    )
    async def reset_ai_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("‚ùå BioManager„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)
            return

        try:
            if await self.bio_manager.reset_channel_bio(interaction.channel_id):
                logger.info(f"AI bio for channel {interaction.channel_id} reset by {interaction.user.name}")
                default_prompt = self.llm_config.get('system_prompt', 'Êú™Ë®≠ÂÆö')
                try:
                    now = datetime.now(self.jst)
                    current_date_str = now.strftime('%YÂπ¥%mÊúà%dÊó•')
                    current_time_str = now.strftime('%H:%M')
                    formatted_prompt = default_prompt.format(current_date=current_date_str,
                                                             current_time=current_time_str)
                except (KeyError, ValueError):
                    formatted_prompt = default_prompt

                display_prompt = (formatted_prompt[:100] + '...') if len(
                    formatted_prompt) > 103 else formatted_prompt

                await interaction.followup.send(
                    f"‚úÖ „Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„ÅÆAI„ÅÆbio„Çí„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„Å´Êàª„Åó„Åæ„Åó„Åü„ÄÇ\n> ÁèæÂú®„ÅÆ„Éá„Éï„Ç©„É´„Éà: `{display_prompt}`",
                    ephemeral=False
                )
            else:
                await interaction.followup.send("‚ÑπÔ∏è „Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„Å´„ÅØÂ∞ÇÁî®„ÅÆAI bio„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ",
                                                ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save channel AI bio settings after reset: {e}", exc_info=True)
            await interaction.followup.send("‚ùå AI„ÅÆbioË®≠ÂÆö„ÅÆ‰øùÂ≠ò„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ", ephemeral=False)

    @app_commands.command(
        name="set-user-bio",
        description="AI„Å´„ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„ÇíË®òÊÜ∂„Åï„Åõ„Åæ„Åô„ÄÇ/ Save your information for the AI to remember."
    )
    @app_commands.describe(
        bio="AI„Å´Ë¶ö„Åà„Å¶„Åª„Åó„ÅÑ„ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„ÇíË®òËø∞„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ(‰æã: ÁßÅ„ÅÆÂêçÂâç„ÅØÁî∞‰∏≠„Åß„Åô„ÄÇË∂£Âë≥„ÅØË™≠Êõ∏„Åß„Åô„ÄÇ)",
        mode="‰øùÂ≠ò„É¢„Éº„Éâ„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ'‰∏äÊõ∏„Åç'„Åæ„Åü„ÅØ'ËøΩË®ò'„ÅåÂèØËÉΩ„Åß„Åô„ÄÇ"
    )
    @app_commands.choices(mode=[
        app_commands.Choice(name="‰∏äÊõ∏„Åç (Overwrite)", value="overwrite"),
        app_commands.Choice(name="ËøΩË®ò (Append)", value="append"),
    ])
    async def set_user_bio_slash(self, interaction: discord.Interaction, bio: str, mode: app_commands.Choice[str]):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("‚ùå BioManager„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)
            return

        if len(bio) > 1024:
            await interaction.followup.send("‚ö†Ô∏è „É¶„Éº„Ç∂„ÉºÊÉÖÂ†±(bio)„ÅåÈï∑„Åô„Åé„Åæ„Åô„ÄÇ1024ÊñáÂ≠ó‰ª•ÂÜÖ„ÅßË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
                                            ephemeral=False)
            return

        try:
            await self.bio_manager.set_user_bio(interaction.user.id, bio, mode=mode.value)
            logger.info(
                f"User bio for {interaction.user.name} ({interaction.user.id}) was set with mode '{mode.value}'.")

            updated_bio = self.bio_manager.get_user_bio(interaction.user.id)

            embed = discord.Embed(
                title=f"‚úÖ „ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„ÇíË®òÊÜ∂„Åó„Åæ„Åó„Åü ({mode.name})",
                description=f"AI„ÅØ„ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„Çí‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´Ë®òÊÜ∂„Åó„Åæ„Åó„Åü„ÄÇ\n\n**„ÅÇ„Å™„Åü„ÅÆbio:**\n```\n{updated_bio}\n```",
                color=discord.Color.green()
            )
            await interaction.followup.send(embed=embed, ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save user bio settings: {e}", exc_info=True)
            await interaction.followup.send("‚ùå „ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„ÅÆ‰øùÂ≠ò„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ", ephemeral=False)

    @app_commands.command(
        name="show-user-bio",
        description="AI„ÅåË®òÊÜ∂„Åó„Å¶„ÅÑ„Çã„ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„ÇíË°®Á§∫„Åó„Åæ„Åô„ÄÇ/ Show the information the AI has stored about you."
    )
    async def show_user_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("‚ùå BioManager„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)
            return

        current_bio = self.bio_manager.get_user_bio(interaction.user.id)
        if current_bio:
            embed = discord.Embed(
                title=f"üí° {interaction.user.display_name}„Åï„Çì„ÅÆÊÉÖÂ†±",
                description=f"**bio:**\n```\n{current_bio}\n```",
                color=discord.Color.blue()
            )
        else:
            embed = discord.Embed(
                title=f"üí° {interaction.user.display_name}„Åï„Çì„ÅÆÊÉÖÂ†±",
                description="ÁèæÂú®„ÄÅ„ÅÇ„Å™„Åü„Å´Èñ¢„Åô„ÇãÊÉÖÂ†±„ÅØ‰Ωï„ÇÇË®òÊÜ∂„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ\n`/set-user-bio` „Ç≥„Éû„É≥„Éâ„Åã„ÄÅ‰ºöË©±„ÅÆ‰∏≠„ÅßAI„Å´Ë®òÊÜ∂„ÇíÈ†º„ÇÄ„Åì„Å®„ÅßË®≠ÂÆö„Åß„Åç„Åæ„Åô„ÄÇ",
                color=discord.Color.greyple()
            )
        await interaction.followup.send(embed=embed, ephemeral=False)

    @app_commands.command(
        name="reset-user-bio",
        description="AI„ÅåË®òÊÜ∂„Åó„Å¶„ÅÑ„Çã„ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„Çí„Åô„Åπ„Å¶ÂâäÈô§„Åó„Åæ„Åô„ÄÇ/ Delete all information the AI has stored about you."
    )
    async def reset_user_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("‚ùå BioManager„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)
            return

        try:
            if await self.bio_manager.reset_user_bio(interaction.user.id):
                logger.info(f"User bio for {interaction.user.name} ({interaction.user.id}) was reset.")
                await interaction.followup.send(
                    f"‚úÖ {interaction.user.display_name}„Åï„Çì„Å´Èñ¢„Åô„ÇãÊÉÖÂ†±„Çí„Åô„Åπ„Å¶ÂâäÈô§„Åó„Åæ„Åó„Åü„ÄÇ", ephemeral=False)
            else:
                await interaction.followup.send("‚ÑπÔ∏è „ÅÇ„Å™„Åü„Å´Èñ¢„Åô„ÇãÊÉÖÂ†±„ÅØ‰Ωï„ÇÇË®òÊÜ∂„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save user bio settings after reset: {e}", exc_info=True)
            await interaction.followup.send("‚ùå „ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„ÅÆÂâäÈô§„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ", ephemeral=False)

    @app_commands.command(
        name="memory-save",
        description="„Ç∞„É≠„Éº„Éê„É´ÂÖ±Êúâ„É°„É¢„É™„Å´ÊÉÖÂ†±„Çí‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ/ Save information to the global shared memory."
    )
    @app_commands.describe(
        key="ÊÉÖÂ†±„ÅÆ„Ç≠„ÉºÔºàÈ†ÖÁõÆÂêçÔºâ ‰æã: 'ÈñãÁô∫ËÄÖ„Åã„Çâ„ÅÆ„ÅäÁü•„Çâ„Åõ'",
        value="ÊÉÖÂ†±„ÅÆÂÜÖÂÆπ ‰æã: 'Ê¨°Âõû„ÅÆ„É°„É≥„ÉÜ„Éä„É≥„Çπ„ÅØ...'"
    )
    async def memory_save_slash(self, interaction: discord.Interaction, key: str, value: str):
        await interaction.response.defer(ephemeral=False)
        if not self.memory_manager:
            await interaction.followup.send("‚ùå MemoryManager„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)
            return

        try:
            await self.memory_manager.save_memory(key, value)
            embed = discord.Embed(
                title="‚úÖ „Ç∞„É≠„Éº„Éê„É´ÂÖ±Êúâ„É°„É¢„É™„Å´‰øùÂ≠ò„Åó„Åæ„Åó„Åü",
                color=discord.Color.green()
            )
            embed.add_field(name="„Ç≠„Éº", value=f"```{key}```", inline=False)
            embed.add_field(name="ÂÄ§", value=f"```{value}```", inline=False)
            await interaction.followup.send(embed=embed, ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save global memory via command: {e}", exc_info=True)
            await interaction.followup.send("‚ùå „Ç∞„É≠„Éº„Éê„É´ÂÖ±Êúâ„É°„É¢„É™„Å∏„ÅÆ‰øùÂ≠ò„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ", ephemeral=False)

    @app_commands.command(
        name="memory-list",
        description="„Ç∞„É≠„Éº„Éê„É´ÂÖ±Êúâ„É°„É¢„É™„ÅÆÊÉÖÂ†±„Çí‰∏ÄË¶ßË°®Á§∫„Åó„Åæ„Åô„ÄÇ/ List all global shared memories."
    )
    async def memory_list_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.memory_manager:
            await interaction.followup.send("‚ùå MemoryManager„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)
            return

        memories = self.memory_manager.list_memories()
        if not memories:
            await interaction.followup.send("‚ÑπÔ∏è „Ç∞„É≠„Éº„Éê„É´ÂÖ±Êúâ„É°„É¢„É™„Å´„ÅØ‰Ωï„ÇÇ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)
            return

        embed = discord.Embed(
            title="üåê „Ç∞„É≠„Éº„Éê„É´ÂÖ±Êúâ„É°„É¢„É™",
            color=discord.Color.blue()
        )
        description = ""
        for key, value in memories.items():
            field_text = f"**{key}**: {value}\n"
            if len(description) + len(field_text) > 4000:
                description += "\n... (Ë°®Á§∫Âà∂Èôê„ÅÆ„Åü„ÇÅ‰∏ÄÈÉ®ÁúÅÁï•)"
                break
            description += field_text

        embed.description = description
        await interaction.followup.send(embed=embed, ephemeral=False)

    async def memory_key_autocomplete(self, interaction: discord.Interaction, current: str) -> List[
        app_commands.Choice[str]]:
        if not self.memory_manager:
            return []
        keys = self.memory_manager.list_memories().keys()
        return [
                   app_commands.Choice(name=key, value=key)
                   for key in keys if current.lower() in key.lower()
               ][:25]

    @app_commands.command(
        name="memory-delete",
        description="„Ç∞„É≠„Éº„Éê„É´ÂÖ±Êúâ„É°„É¢„É™„Åã„ÇâÊÉÖÂ†±„ÇíÂâäÈô§„Åó„Åæ„Åô„ÄÇ/ Delete a global shared memory."
    )
    @app_commands.describe(key="ÂâäÈô§„Åó„Åü„ÅÑÊÉÖÂ†±„ÅÆ„Ç≠„Éº")
    @app_commands.autocomplete(key=memory_key_autocomplete)
    async def memory_delete_slash(self, interaction: discord.Interaction, key: str):
        await interaction.response.defer(ephemeral=False)
        if not self.memory_manager:
            await interaction.followup.send("‚ùå MemoryManager„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)
            return

        try:
            if await self.memory_manager.delete_memory(key):
                await interaction.followup.send(f"‚úÖ „Ç∞„É≠„Éº„Éê„É´ÂÖ±Êúâ„É°„É¢„É™„Åã„Çâ„Ç≠„Éº '{key}' „ÇíÂâäÈô§„Åó„Åæ„Åó„Åü„ÄÇ",
                                                ephemeral=False)
            else:
                await interaction.followup.send(f"‚ö†Ô∏è „Ç≠„Éº '{key}' „ÅØ„Ç∞„É≠„Éº„Éê„É´ÂÖ±Êúâ„É°„É¢„É™„Å´Â≠òÂú®„Åó„Åæ„Åõ„Çì„ÄÇ",
                                                ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to delete global memory via command: {e}", exc_info=True)
            await interaction.followup.send("‚ùå „Ç∞„É≠„Éº„Éê„É´ÂÖ±Êúâ„É°„É¢„É™„Åã„Çâ„ÅÆÂâäÈô§„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ", ephemeral=False)

    async def model_autocomplete(self, interaction: discord.Interaction, current: str) -> List[
        app_commands.Choice[str]]:
        available_models = self.llm_config.get('available_models', [])
        return [
                   app_commands.Choice(name=model, value=model)
                   for model in available_models if current.lower() in model.lower()
               ][:25]

    @app_commands.command(
        name="switch-models",
        description="„Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„Åß‰ΩøÁî®„Åô„ÇãAI„É¢„Éá„É´„ÇíÂàá„ÇäÊõø„Åà„Åæ„Åô„ÄÇ/ Switches the AI model used for this channel."
    )
    @app_commands.describe(
        model="‰ΩøÁî®„Åó„Åü„ÅÑ„É¢„Éá„É´„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
    )
    @app_commands.autocomplete(model=model_autocomplete)
    async def switch_model_slash(self, interaction: discord.Interaction, model: str):
        await interaction.response.defer(ephemeral=False)
        available_models = self.llm_config.get('available_models', [])
        if model not in available_models:
            await interaction.followup.send(f"‚ö†Ô∏è ÊåáÂÆö„Åï„Çå„Åü„É¢„Éá„É´ '{model}' „ÅØÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ")
            return

        channel_id_str = str(interaction.channel_id)
        self.channel_models[channel_id_str] = model

        try:
            await self._save_channel_models()
            await self._get_llm_client_for_channel(interaction.channel_id)
            await interaction.followup.send(f"‚úÖ „Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„ÅÆAI„É¢„Éá„É´„Åå `{model}` „Å´Âàá„ÇäÊõø„Åà„Çâ„Çå„Åæ„Åó„Åü„ÄÇ",
                                            ephemeral=False)
            logger.info(f"Model for channel {interaction.channel_id} switched to '{model}' by {interaction.user.name}")
        except Exception as e:
            logger.error(f"Failed to save channel model settings: {e}", exc_info=True)
            await interaction.followup.send("‚ùå Ë®≠ÂÆö„ÅÆ‰øùÂ≠ò„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ")

    @app_commands.command(
        name="switch-models-default-server",
        description="„Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„ÅÆAI„É¢„Éá„É´„Çí„Çµ„Éº„Éê„Éº„ÅÆ„Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö„Å´Êàª„Åó„Åæ„Åô„ÄÇ/ Resets the AI model for this channel to the server default."
    )
    async def reset_model_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        channel_id_str = str(interaction.channel_id)

        if channel_id_str in self.channel_models:
            del self.channel_models[channel_id_str]
            try:
                await self._save_channel_models()
                default_model = self.llm_config.get('model', 'Êú™Ë®≠ÂÆö')
                await interaction.followup.send(
                    f"‚úÖ „Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„ÅÆAI„É¢„Éá„É´„Çí„Éá„Éï„Ç©„É´„Éà (`{default_model}`) „Å´Êàª„Åó„Åæ„Åó„Åü„ÄÇ", ephemeral=False)
                logger.info(f"Model for channel {interaction.channel_id} reset to default by {interaction.user.name}")
            except Exception as e:
                logger.error(f"Failed to save channel model settings after reset: {e}", exc_info=True)
                await interaction.followup.send("‚ùå Ë®≠ÂÆö„ÅÆ‰øùÂ≠ò„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ")
        else:
            await interaction.followup.send("‚ÑπÔ∏è „Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„Å´„ÅØÂ∞ÇÁî®„ÅÆ„É¢„Éá„É´„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ", ephemeral=False)

    @switch_model_slash.error
    async def switch_model_error(self, interaction: discord.Interaction, error: app_commands.AppCommandError):
        logger.error(f"Error in /switch-model command: {error}", exc_info=True)
        if not interaction.response.is_done():
            await interaction.response.send_message(f"‰∫àÊúü„Åõ„Å¨„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {error}", ephemeral=False)
        else:
            await interaction.followup.send(f"‰∫àÊúü„Åõ„Å¨„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {error}", ephemeral=False)

    @app_commands.command(name="llm_help",
                          description="LLM (AIÂØæË©±) Ê©üËÉΩ„ÅÆ„Éò„É´„Éó„Å®Âà©Áî®„Ç¨„Ç§„Éâ„É©„Ç§„É≥„ÇíË°®Á§∫„Åó„Åæ„Åô„ÄÇ/ Displays help and usage guidelines for LLM (AI Chat) features.")
    async def llm_help_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        bot_user = self.bot.user or interaction.client.user
        bot_name = bot_user.name if bot_user else "ÂΩìBot"
        embed = discord.Embed(title=f"üí° {bot_name} AIÂØæË©±Ê©üËÉΩ„Éò„É´„ÉóÔºÜ„Ç¨„Ç§„Éâ„É©„Ç§„É≥",
                              description=f"{bot_name}„ÅÆAIÂØæË©±Ê©üËÉΩ„Å´„Å§„ÅÑ„Å¶„ÅÆË™¨Êòé„Å®Âà©Áî®Ë¶èÁ¥Ñ„Åß„Åô„ÄÇ",
                              color=discord.Color.purple())
        if bot_user and bot_user.avatar: embed.set_thumbnail(url=bot_user.avatar.url)
        embed.add_field(
            name="Âü∫Êú¨ÁöÑ„Å™‰Ωø„ÅÑÊñπ",
            value=(
                f"‚Ä¢ Bot„Å´„É°„É≥„Ç∑„Éß„É≥ (`@{bot_name}`) „Åó„Å¶Ë©±„Åó„Åã„Åë„Çã„Å®„ÄÅAI„ÅåÂøúÁ≠î„Åó„Åæ„Åô„ÄÇ\n"
                f"‚Ä¢ **Bot„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„Å´Ëøî‰ø°„Åô„Çã„Åì„Å®„Åß„ÇÇ‰ºöË©±„ÇíÁ∂ö„Åë„Çâ„Çå„Åæ„ÅôÔºà„É°„É≥„Ç∑„Éß„É≥‰∏çË¶ÅÔºâ„ÄÇ**\n"
                f"‚Ä¢ „ÄåÁßÅ„ÅÆÂêçÂâç„ÅØ„Äá„Äá„Åß„Åô„ÄÇË¶ö„Åà„Å¶„Åä„ÅÑ„Å¶„Äç„ÅÆ„Çà„ÅÜ„Å´Ë©±„Åó„Åã„Åë„Çã„Å®„ÄÅAI„Åå„ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„ÇíË®òÊÜ∂„Åó„Çà„ÅÜ„Å®„Åó„Åæ„Åô„ÄÇ\n"
                f"‚Ä¢ ÁîªÂÉè„Å®‰∏ÄÁ∑í„Å´Ë©±„Åó„Åã„Åë„Çã„Å®„ÄÅAI„ÅåÁîªÂÉè„ÅÆÂÜÖÂÆπ„ÇÇÁêÜËß£„Åó„Çà„ÅÜ„Å®„Åó„Åæ„Åô„ÄÇ"
            ),
            inline=False
        )
        embed.add_field(
            name="‰æøÂà©„Å™„Ç≥„Éû„É≥„Éâ",
            value=(
                "**„ÄêAI„ÅÆË®≠ÂÆö („ÉÅ„É£„É≥„Éç„É´„Åî„Å®)„Äë**\n"
                "‚Ä¢ `/switch-models`: „Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„Åß‰Ωø„ÅÜAI„É¢„Éá„É´„ÇíÂ§âÊõ¥„Åó„Åæ„Åô„ÄÇ\n"
                "‚Ä¢ `/set-ai-bio`: „Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´Â∞ÇÁî®„ÅÆAI„ÅÆÊÄßÊ†º„ÇÑÂΩπÂâ≤„ÇíË®≠ÂÆö„Åó„Åæ„Åô„ÄÇ\n"
                "‚Ä¢ `/show-ai-bio`: ÁèæÂú®„ÅÆAI„ÅÆbioË®≠ÂÆö„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ\n"
                "‚Ä¢ `/reset-ai-bio`: AI„ÅÆbioË®≠ÂÆö„Çí„Éá„Éï„Ç©„É´„Éà„Å´Êàª„Åó„Åæ„Åô„ÄÇ\n"
                "**„Äê„ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„Äë**\n"
                "‚Ä¢ `/set-user-bio`: AI„Å´Ë¶ö„Åà„Å¶„Åª„Åó„ÅÑ„ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„ÇíË®≠ÂÆö„Åó„Åæ„Åô„ÄÇ\n"
                "‚Ä¢ `/show-user-bio`: AI„ÅåË®òÊÜ∂„Åó„Å¶„ÅÑ„Çã„ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ\n"
                "‚Ä¢ `/reset-user-bio`: „ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±„ÇíAI„ÅÆË®òÊÜ∂„Åã„ÇâÂâäÈô§„Åó„Åæ„Åô„ÄÇ\n"
                "**„Äê„Ç∞„É≠„Éº„Éê„É´„É°„É¢„É™„Äë**\n"
                "‚Ä¢ `/memory-save`: ÂÖ®„Çµ„Éº„Éê„ÉºÂÖ±ÈÄö„ÅÆ„É°„É¢„É™„Å´ÊÉÖÂ†±„Çí‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ\n"
                "‚Ä¢ `/memory-list`: „Ç∞„É≠„Éº„Éê„É´„É°„É¢„É™„ÅÆÊÉÖÂ†±„Çí‰∏ÄË¶ßË°®Á§∫„Åó„Åæ„Åô„ÄÇ\n"
                "‚Ä¢ `/memory-delete`: „Ç∞„É≠„Éº„Éê„É´„É°„É¢„É™„Åã„ÇâÊÉÖÂ†±„ÇíÂâäÈô§„Åó„Åæ„Åô„ÄÇ\n"
                "**„Äê„Åù„ÅÆ‰ªñ„Äë**\n"
                "‚Ä¢ `/clear_history`: ‰ºöË©±Â±•Ê≠¥„Çí„É™„Çª„ÉÉ„Éà„Åó„Åæ„Åô„ÄÇ"
            ),
            inline=False
        )
        channel_model_str = self.channel_models.get(str(interaction.channel_id))
        model_display = f"`{channel_model_str}` („Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´Â∞ÇÁî®)" if channel_model_str else f"`{self.llm_config.get('model', 'Êú™Ë®≠ÂÆö')}` („Éá„Éï„Ç©„É´„Éà)"

        ai_bio_display = "N/A"
        user_bio_display = "N/A"
        if self.bio_manager:
            ai_bio_display = "‚úÖ (Â∞ÇÁî®Ë®≠ÂÆö„ÅÇ„Çä)" if self.bio_manager.get_channel_bio(interaction.channel_id) else "„Éá„Éï„Ç©„É´„Éà"
            user_bio_display = "‚úÖ (Ë®òÊÜ∂„ÅÇ„Çä)" if self.bio_manager.get_user_bio(interaction.user.id) else "„Å™„Åó"

        active_tools = self.llm_config.get('active_tools', [])
        tools_info = "‚Ä¢ „Å™„Åó" if not active_tools else "‚Ä¢ " + ", ".join(active_tools)
        embed.add_field(name="ÁèæÂú®„ÅÆAIË®≠ÂÆö",
                        value=f"‚Ä¢ **‰ΩøÁî®„É¢„Éá„É´:** {model_display}\n"
                              f"‚Ä¢ **AI„ÅÆÂΩπÂâ≤(„ÉÅ„É£„É≥„Éç„É´):** {ai_bio_display} (Ë©≥Á¥∞„ÅØ `/show-ai-bio`)\n"
                              f"‚Ä¢ **„ÅÇ„Å™„Åü„ÅÆÊÉÖÂ†±:** {user_bio_display} (Ë©≥Á¥∞„ÅØ `/show-user-bio`)\n"
                              f"‚Ä¢ **‰ºöË©±Â±•Ê≠¥„ÅÆÊúÄÂ§ß‰øùÊåÅÊï∞:** {self.llm_config.get('max_messages', 'Êú™Ë®≠ÂÆö')} „Éö„Ç¢\n"
                              f"‚Ä¢ **‰∏ÄÂ∫¶„Å´Âá¶ÁêÜ„Åß„Åç„ÇãÊúÄÂ§ßÁîªÂÉèÊûöÊï∞:** {self.llm_config.get('max_images', 'Êú™Ë®≠ÂÆö')} Êûö\n"
                              f"‚Ä¢ **Âà©Áî®ÂèØËÉΩ„Å™„ÉÑ„Éº„É´:** {tools_info}",
                        inline=False)
        embed.add_field(name="--- üìú AIÂà©Áî®„Ç¨„Ç§„Éâ„É©„Ç§„É≥ ---",
                        value="AIÊ©üËÉΩ„ÇíÂÆâÂÖ®„Å´„ÅîÂà©Áî®„ÅÑ„Åü„Å†„Åè„Åü„ÇÅ„ÄÅ‰ª•‰∏ã„ÅÆÂÜÖÂÆπ„ÇíÂøÖ„Åö„ÅîÁ¢∫Ë™ç„Åè„Å†„Åï„ÅÑ„ÄÇ", inline=False)
        embed.add_field(name="‚ö†Ô∏è 1. „Éá„Éº„ÇøÂÖ•ÂäõÊôÇ„ÅÆÊ≥®ÊÑè", value=(
            "AI„Å´Ë®òÊÜ∂„Åï„Åõ„ÇãÊÉÖÂ†±„Å´„ÅØ„ÄÅÊ∞èÂêç„ÄÅÈÄ£Áµ°ÂÖà„ÄÅ„Éë„Çπ„ÉØ„Éº„Éâ„Å™„Å©„ÅÆ**ÂÄã‰∫∫ÊÉÖÂ†±„ÇÑÁßòÂØÜÊÉÖÂ†±„ÇíÁµ∂ÂØæ„Å´Âê´„ÇÅ„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ**"),
                        inline=False)
        embed.add_field(name="‚úÖ 2. ÁîüÊàêÁâ©Âà©Áî®ÊôÇ„ÅÆÊ≥®ÊÑè", value=(
            "AI„ÅÆÂøúÁ≠î„Å´„ÅØËôöÂÅΩ„ÇÑÂÅèË¶ã„ÅåÂê´„Åæ„Çå„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ**ÂøÖ„Åö„Éï„Ç°„ÇØ„Éà„ÉÅ„Çß„ÉÉ„ÇØ„ÇíË°å„ÅÑ„ÄÅËá™Â∑±„ÅÆË≤¨‰ªª„ÅßÂà©Áî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ**"),
                        inline=False)
        embed.set_footer(text="„Ç¨„Ç§„Éâ„É©„Ç§„É≥„ÅØ‰∫àÂëä„Å™„ÅèÂ§âÊõ¥„Åï„Çå„ÇãÂ†¥Âêà„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ")
        await interaction.followup.send(embed=embed, ephemeral=False)

    @app_commands.command(name="llm_help_en",
                          description="Displays help and usage guidelines for LLM (AI Chat) features.")
    async def llm_help_en_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        bot_user = self.bot.user or interaction.client.user
        bot_name = bot_user.name if bot_user else "This Bot"
        embed = discord.Embed(title=f"üí° {bot_name} AI Chat Help & Guidelines",
                              description=f"Explanation and terms of use for the AI chat features of {bot_name}.",
                              color=discord.Color.purple())
        if bot_user and bot_user.avatar: embed.set_thumbnail(url=bot_user.avatar.url)
        embed.add_field(
            name="Basic Usage",
            value=(
                f"‚Ä¢ Mention the bot (`@{bot_name}`) to get a response from the AI.\n"
                f"‚Ä¢ **You can also continue the conversation by replying to the bot's messages (no mention needed).**\n"
                f"‚Ä¢ If you ask the AI to remember something (e.g., 'My name is John, please remember it'), it will try to store that information.\n"
                f"‚Ä¢ Attach images or paste image URLs with your message, and the AI will try to understand them."
            ),
            inline=False
        )
        embed.add_field(
            name="Useful Commands",
            value=(
                "**[AI Settings (Per Channel)]**\n"
                "‚Ä¢ `/switch-models`: Change the AI model used in this channel.\n"
                "‚Ä¢ `/set-ai-bio`: Set a custom personality/role for the AI in this channel.\n"
                "‚Ä¢ `/show-ai-bio`: Check the current AI bio setting.\n"
                "‚Ä¢ `/reset-ai-bio`: Reset the AI bio to the default.\n"
                "**[Your Information]**\n"
                "‚Ä¢ `/set-user-bio`: Set information about you for the AI to remember.\n"
                "‚Ä¢ `/show-user-bio`: Check the information the AI has stored about you.\n"
                "‚Ä¢ `/reset-user-bio`: Delete your information from the AI's memory.\n"
                "**[Global Memory]**\n"
                "‚Ä¢ `/memory-save`: Save information to the global shared memory.\n"
                "‚Ä¢ `/memory-list`: List all information in the global memory.\n"
                "‚Ä¢ `/memory-delete`: Delete information from the global memory.\n"
                "**[Other]**\n"
                "‚Ä¢ `/clear_history`: Reset the conversation history."
            ),
            inline=False
        )
        channel_model_str = self.channel_models.get(str(interaction.channel_id))
        model_display = f"`{channel_model_str}` (Channel-specific)" if channel_model_str else f"`{self.llm_config.get('model', 'Not set')}` (Default)"

        ai_bio_display = "N/A"
        user_bio_display = "N/A"
        if self.bio_manager:
            ai_bio_display = "‚úÖ (Custom)" if self.bio_manager.get_channel_bio(interaction.channel_id) else "Default"
            user_bio_display = "‚úÖ (Stored)" if self.bio_manager.get_user_bio(interaction.user.id) else "None"

        active_tools = self.llm_config.get('active_tools', [])
        tools_info = "‚Ä¢ None" if not active_tools else "‚Ä¢ " + ", ".join(active_tools)
        embed.add_field(name="Current AI Settings",
                        value=f"‚Ä¢ **Model in Use:** {model_display}\n"
                              f"‚Ä¢ **AI Role (Channel):** {ai_bio_display} (see `/show-ai-bio`)\n"
                              f"‚Ä¢ **Your Info:** {user_bio_display} (see `/show-user-bio`)\n"
                              f"‚Ä¢ **Max Conversation History:** {self.llm_config.get('max_messages', 'Not set')} pairs\n"
                              f"‚Ä¢ **Max Images Processed at Once:** {self.llm_config.get('max_images', 'Not set')} image(s)\n"
                              f"‚Ä¢ **Available Tools:** {tools_info}",
                        inline=False)
        embed.add_field(name="--- üìú AI Usage Guidelines ---",
                        value="Please review the following to ensure safe use of the AI features.", inline=False)
        embed.add_field(name="‚ö†Ô∏è 1. Precautions for Data Input", value=(
            "**NEVER include personal or confidential information** such as your name, contact details, or passwords in the information you ask the AI to remember."),
                        inline=False)
        embed.add_field(name="‚úÖ 2. Precautions for Using Generated Output", value=(
            "The AI's responses may contain inaccuracies or biases. **Always fact-check and use them at your own risk.**"),
                        inline=False)
        embed.set_footer(text="These guidelines are subject to change without notice.")
        await interaction.followup.send(embed=embed, ephemeral=False)

    @app_commands.command(
        name="clear_history",
        description="ÁèæÂú®„ÅÆ‰ºöË©±„Çπ„É¨„ÉÉ„Éâ„ÅÆÂ±•Ê≠¥„Çí„ÇØ„É™„Ç¢„Åó„Åæ„Åô„ÄÇ/ Clears the history of the current conversation thread."
    )
    async def clear_history_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        cleared_count = 0
        threads_to_clear = set()
        try:
            async for msg in interaction.channel.history(limit=200):
                if msg.id in self.message_to_thread:
                    threads_to_clear.add(self.message_to_thread[msg.id])
        except (discord.Forbidden, discord.HTTPException):
            await interaction.followup.send("‚ö†Ô∏è „ÉÅ„É£„É≥„Éç„É´„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏Â±•Ê≠¥„ÇíË™≠„ÅøÂèñ„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ")
            return
        for thread_id in threads_to_clear:
            if thread_id in self.conversation_threads:
                del self.conversation_threads[thread_id]
                self.message_to_thread = {k: v for k, v in self.message_to_thread.items() if v != thread_id}
                cleared_count += 1
        if cleared_count > 0:
            await interaction.followup.send(
                f"‚úÖ „Åì„ÅÆ„ÉÅ„É£„É≥„Éç„É´„Å´Èñ¢ÈÄ£„Åô„Çã {cleared_count} ÂÄã„ÅÆ‰ºöË©±„Çπ„É¨„ÉÉ„Éâ„ÅÆÂ±•Ê≠¥„Çí„ÇØ„É™„Ç¢„Åó„Åæ„Åó„Åü„ÄÇ")
        else:
            await interaction.followup.send("‚ÑπÔ∏è „ÇØ„É™„Ç¢ÂØæË±°„ÅÆ‰ºöË©±Â±•Ê≠¥„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ")


async def setup(bot: commands.Bot):
    """Sets up the LLMCog."""
    try:
        await bot.add_cog(LLMCog(bot))
        logger.info("LLMCog loaded successfully.")
    except Exception as e:
        logger.critical(f"Failed to set up LLMCog: {e}", exc_info=True)
        raise