from __future__ import annotations

import asyncio
import base64
import io
import json
import logging
import os
import re
import time
from datetime import datetime, timezone, timedelta
from types import SimpleNamespace
from typing import List, Dict, Any, Tuple, Optional, AsyncGenerator

import aiohttp
import discord
import openai
from discord import app_commands
from discord.ext import commands

from PLANA.llm.error.errors import (
    LLMExceptionHandler,
    SearchAgentError,
    SearchAPIRateLimitError,
    SearchAPIServerError
)

try:
    from PLANA.llm.plugins.search_agent import SearchAgent
except ImportError:
    logging.error("Could not import SearchAgent. Search functionality will be disabled.")
    SearchAgent = None

try:
    from PLANA.llm.plugins.bio_manager import BioManager
except ImportError:
    logging.error("Could not import BioManager. Bio functionality will be disabled.")
    BioManager = None

try:
    from PLANA.llm.plugins.memory_manager import MemoryManager
except ImportError:
    logging.error("Could not import MemoryManager. Memory functionality will be disabled.")
    MemoryManager = None

try:
    import aiofiles
except ImportError:
    aiofiles = None
    logging.warning("aiofiles library not found. Channel model settings will be saved synchronously. "
                    "Install with: pip install aiofiles")

logger = logging.getLogger(__name__)

# Constants
SUPPORTED_IMAGE_EXTENSIONS = ('.png', '.jpeg', '.jpg', '.gif', '.webp')
IMAGE_URL_PATTERN = re.compile(
    r'https?://[^\s]+\.(?:' + '|'.join(ext.lstrip('.') for ext in SUPPORTED_IMAGE_EXTENSIONS) + r')(?:\?[^\s]*)?',
    re.IGNORECASE
)
DISCORD_MESSAGE_MAX_LENGTH = 1990


class LLMCog(commands.Cog, name="LLM"):
    """A cog for interacting with Large Language Models, with tool support."""

    def __init__(self, bot: commands.Bot):
        self.bot = bot
        if not hasattr(self.bot, 'config') or not self.bot.config:
            raise commands.ExtensionFailed(self.qualified_name, "Bot config not loaded.")
        self.config = self.bot.config
        self.llm_config = self.config.get('llm')
        if not isinstance(self.llm_config, dict):
            raise commands.ExtensionFailed(self.qualified_name, "The 'llm' section in config is missing or invalid.")
        self.http_session = aiohttp.ClientSession()
        self.bot.cfg = self.llm_config
        self.conversation_threads: Dict[int, List[Dict[str, Any]]] = {}
        self.message_to_thread: Dict[int, int] = {}
        self.llm_clients: Dict[str, openai.AsyncOpenAI] = {}

        self.exception_handler = LLMExceptionHandler(self.llm_config)

        self.channel_settings_path = "data/channel_llm_models.json"
        self.channel_models: Dict[str, str] = self._load_json_data(self.channel_settings_path)
        logger.info(
            f"Loaded {len(self.channel_models)} channel-specific model settings from '{self.channel_settings_path}'.")

        self.jst = timezone(timedelta(hours=+9))

        # ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®åˆæœŸåŒ–
        self.search_agent = self._initialize_search_agent()
        self.bio_manager = self._initialize_bio_manager()
        self.memory_manager = self._initialize_memory_manager()

        default_model_string = self.llm_config.get('model')
        if default_model_string:
            main_llm_client = self._initialize_llm_client(default_model_string)
            if main_llm_client:
                self.llm_clients[default_model_string] = main_llm_client
                logger.info(f"Default LLM client '{default_model_string}' initialized and cached.")
            else:
                logger.error("Failed to initialize main LLM client. Core functionality may be disabled.")
        else:
            logger.error("Default LLM model is not configured in config.yaml.")

    async def cog_unload(self):
        await self.http_session.close()
        logger.info("LLMCog's aiohttp session has been closed.")

    def _load_json_data(self, path: str) -> Dict[str, Any]:
        try:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return {str(k): v for k, v in data.items()}
        except (IOError, json.JSONDecodeError) as e:
            logger.error(f"Failed to load JSON file '{path}': {e}")
        return {}

    async def _save_json_data(self, data: Dict[str, Any], path: str) -> None:
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            if aiofiles:
                async with aiofiles.open(path, 'w', encoding='utf-8') as f:
                    await f.write(json.dumps(data, indent=4, ensure_ascii=False))
            else:
                with open(path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=4, ensure_ascii=False)
        except IOError as e:
            logger.error(f"Failed to save JSON file '{path}': {e}")
            raise

    async def _save_channel_models(self) -> None:
        await self._save_json_data(self.channel_models, self.channel_settings_path)

    def _initialize_llm_client(self, model_string: Optional[str]) -> Optional[openai.AsyncOpenAI]:
        if not model_string or '/' not in model_string:
            logger.error(f"Invalid model format: '{model_string}'. Expected 'provider_name/model_name'.")
            return None
        try:
            provider_name, model_name = model_string.split('/', 1)
            provider_config = self.llm_config.get('providers', {}).get(provider_name)
            if not provider_config:
                logger.error(f"Configuration for LLM provider '{provider_name}' not found.")
                return None
            client = openai.AsyncOpenAI(base_url=provider_config.get('base_url'),
                                        api_key=provider_config.get('api_key') or "local-dummy-key")
            client.model_name_for_api_calls = model_name
            logger.info(f"Initialized LLM client for provider '{provider_name}' with model '{model_name}'.")
            return client
        except Exception as e:
            logger.error(f"Error initializing LLM client for '{model_string}': {e}", exc_info=True)
            return None

    async def _get_llm_client_for_channel(self, channel_id: int) -> Optional[openai.AsyncOpenAI]:
        channel_id_str = str(channel_id)
        model_string = self.channel_models.get(channel_id_str) or self.llm_config.get('model')
        if not model_string:
            logger.error("No default model is configured.")
            return None
        if model_string in self.llm_clients:
            return self.llm_clients[model_string]
        logger.info(f"Initializing a new LLM client for model '{model_string}' for channel {channel_id}")
        client = self._initialize_llm_client(model_string)
        if client:
            self.llm_clients[model_string] = client
        return client

    def _initialize_search_agent(self) -> Optional[SearchAgent]:
        if 'search' not in self.llm_config.get('active_tools', []) or not SearchAgent:
            return None
        search_config = self.llm_config.get('search_agent', {})
        if not search_config.get('api_key'):
            logger.error("SearchAgent config (api_key) is missing. Search will be disabled.")
            return None
        try:
            return SearchAgent(self.bot)
        except Exception as e:
            logger.error(f"Failed to initialize SearchAgent: {e}", exc_info=True)
            return None

    def _initialize_bio_manager(self) -> Optional[BioManager]:
        if not BioManager:
            return None
        try:
            return BioManager(self.bot)
        except Exception as e:
            logger.error(f"Failed to initialize BioManager: {e}", exc_info=True)
            return None

    def _initialize_memory_manager(self) -> Optional[MemoryManager]:
        if not MemoryManager:
            return None
        try:
            return MemoryManager(self.bot)
        except Exception as e:
            logger.error(f"Failed to initialize MemoryManager: {e}", exc_info=True)
            return None

    def get_tools_definition(self) -> Optional[List[Dict[str, Any]]]:
        definitions = []
        active_tools = self.llm_config.get('active_tools', [])

        if 'search' in active_tools and self.search_agent:
            definitions.append(self.search_agent.tool_spec)
        if 'user_bio' in active_tools and self.bio_manager:
            definitions.append(self.bio_manager.tool_spec)
        if 'memory' in active_tools and self.memory_manager:
            definitions.append(self.memory_manager.tool_spec)

        return definitions or None

    async def _get_conversation_thread_id(self, message: discord.Message) -> int:
        if message.id in self.message_to_thread:
            return self.message_to_thread[message.id]
        current_msg = message
        visited_ids = set()
        while current_msg.reference and current_msg.reference.message_id:
            if current_msg.id in visited_ids: break
            visited_ids.add(current_msg.id)
            try:
                parent_msg = current_msg.reference.resolved or await message.channel.fetch_message(
                    current_msg.reference.message_id)
                if parent_msg.author != self.bot.user: break
                current_msg = parent_msg
            except (discord.NotFound, discord.HTTPException):
                break
        thread_id = current_msg.id
        self.message_to_thread[message.id] = thread_id
        return thread_id

    async def _collect_conversation_history(self, message: discord.Message) -> List[Dict[str, Any]]:
        history = []
        current_msg = message
        visited_ids = set()
        while current_msg.reference and current_msg.reference.message_id:
            if current_msg.reference.message_id in visited_ids: break
            visited_ids.add(current_msg.reference.message_id)
            try:
                parent_msg = current_msg.reference.resolved or await message.channel.fetch_message(
                    current_msg.reference.message_id)
                if parent_msg.author != self.bot.user:
                    image_contents, text_content = await self._prepare_multimodal_content(parent_msg)
                    text_content = text_content.replace(f'<@!{self.bot.user.id}>', '').replace(f'<@{self.bot.user.id}>',
                                                                                               '').strip()
                    if text_content or image_contents:
                        user_content_parts = []
                        if text_content:
                            timestamp = parent_msg.created_at.astimezone(self.jst).strftime('[%H:%M]')
                            formatted_text = f"{timestamp} {text_content}"
                            user_content_parts.append({"type": "text", "text": formatted_text})

                        user_content_parts.extend(image_contents)
                        history.append({"role": "user", "content": user_content_parts})
                else:
                    thread_id = await self._get_conversation_thread_id(parent_msg)
                    if thread_id in self.conversation_threads:
                        for msg in self.conversation_threads[thread_id]:
                            if msg.get("role") == "assistant" and msg.get("message_id") == parent_msg.id:
                                history.append({"role": "assistant", "content": msg["content"]})
                                break
                current_msg = parent_msg
            except (discord.NotFound, discord.HTTPException):
                break
        history.reverse()
        max_history_entries = self.llm_config.get('max_messages', 10) * 2
        return history[-max_history_entries:] if len(history) > max_history_entries else history

    async def _process_image_url(self, url: str) -> Optional[Dict[str, Any]]:
        try:
            async with self.http_session.get(url) as response:
                if response.status == 200:
                    image_bytes = await response.read()
                    encoded_image = base64.b64encode(image_bytes).decode('utf-8')
                    mime_type = response.content_type
                    if not mime_type or not mime_type.startswith('image/'):
                        ext = url.split('.')[-1].lower().split('?')[0]
                        mime_type = f'image/{ext}' if ext in ('png', 'jpeg', 'gif', 'webp') else 'image/jpeg'
                    return {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{encoded_image}"}}
                else:
                    logger.warning(f"Failed to download image from {url} (Status: {response.status})")
                    return None
        except Exception as e:
            logger.error(f"Error processing image URL {url}: {e}", exc_info=True)
            return None

    async def _prepare_multimodal_content(self, message: discord.Message) -> Tuple[List[Dict[str, Any]], str]:
        image_inputs, processed_urls = [], set()
        messages_to_scan = [message]
        logger.info(f"ğŸ”µ [IMAGE] Starting image scan for message ID: {message.id}")

        # å¼•ç”¨ãƒªãƒ—ãƒ©ã‚¤å…ƒã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚ç”»åƒå–å¾—å¯¾è±¡ã«è¿½åŠ 
        if message.reference and message.reference.message_id:
            try:
                # ã¾ãšã‚­ãƒ£ãƒƒã‚·ãƒ¥ (resolved) ã‚’ç¢ºèª
                referenced_msg = message.reference.resolved
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã‘ã‚Œã°APIã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
                if not referenced_msg:
                    logger.info(
                        f"ğŸ”µ [IMAGE] Referenced message not in cache. Fetching ID: {message.reference.message_id}")
                    referenced_msg = await message.channel.fetch_message(message.reference.message_id)

                if referenced_msg:
                    messages_to_scan.append(referenced_msg)
                    logger.info(f"ğŸ”µ [IMAGE] Added referenced message to scan (ID: {referenced_msg.id})")
            except discord.Forbidden:
                # æ¨©é™ä¸è¶³ã¯è‡´å‘½çš„ãªã®ã§ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦è¨˜éŒ²
                logger.error(
                    f"âŒ [IMAGE] Lacking 'Read Message History' permission in channel '{message.channel.name}' ({message.channel.id}) "
                    f"to fetch referenced message. Please check bot permissions."
                )
            except (discord.NotFound, discord.HTTPException) as e:
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€ãã®ä»–ã®HTTPã‚¨ãƒ©ãƒ¼ã¯è­¦å‘Šã¨ã—ã¦è¨˜éŒ²
                logger.warning(
                    f"âš ï¸ [IMAGE] Could not fetch referenced message (ID: {message.reference.message_id}): {e}")

        # åé›†ãƒ­ã‚¸ãƒƒã‚¯
        source_urls = []
        for msg in messages_to_scan:
            logger.info(f"ğŸ”µ [IMAGE] Scanning message ID: {msg.id} by {msg.author.name}")

            # 1. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æœ¬æ–‡ã®URLã‚’æ¤œç´¢
            for url in IMAGE_URL_PATTERN.findall(msg.content):
                if url not in processed_urls:
                    source_urls.append(url)
                    processed_urls.add(url)

            # 2. æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            for attachment in msg.attachments:
                if attachment.content_type and attachment.content_type.startswith(
                        'image/') and attachment.url not in processed_urls:
                    source_urls.append(attachment.url)
                    processed_urls.add(attachment.url)

            # 3. åŸ‹ã‚è¾¼ã¿(Embed)å†…ã®ç”»åƒã‚’æ¤œç´¢
            for embed in msg.embeds:
                # embed.image (å¤§ãã„ç”»åƒ)
                if embed.image and embed.image.url and embed.image.url not in processed_urls:
                    source_urls.append(embed.image.url)
                    processed_urls.add(embed.image.url)
                # embed.thumbnail (å°ã•ã„ç”»åƒ)
                if embed.thumbnail and embed.thumbnail.url and embed.thumbnail.url not in processed_urls:
                    source_urls.append(embed.thumbnail.url)
                    processed_urls.add(embed.thumbnail.url)

        if source_urls:
            logger.info(f"ğŸ”µ [IMAGE] Found {len(source_urls)} unique image URL(s): {source_urls}")

        max_images = self.llm_config.get('max_images', 1)
        for url in source_urls[:max_images]:
            if image_data := await self._process_image_url(url):
                image_inputs.append(image_data)

        if len(source_urls) > max_images:
            logger.info(f"Reached max image limit ({max_images}). Ignoring {len(source_urls) - max_images} images.")
            try:
                error_msg_template = self.llm_config.get('error_msg', {}).get('msg_max_image_size',
                                                                              "âš ï¸ Max images ({max_images}) reached.")
                await message.channel.send(error_msg_template.format(max_images=max_images), delete_after=10,
                                           silent=True)
            except discord.HTTPException:
                pass

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’è¿”ã™
        clean_text = IMAGE_URL_PATTERN.sub('', message.content).strip()
        return image_inputs, clean_text

    @commands.Cog.listener()
    async def on_message(self, message: discord.Message):
        if message.author.bot:
            return
        is_mentioned = self.bot.user.mentioned_in(message) and not message.mention_everyone
        is_reply_to_bot = (
                message.reference and
                isinstance(message.reference.resolved, discord.Message) and
                message.reference.resolved.author == self.bot.user
        )
        if not (is_mentioned or is_reply_to_bot):
            return

        try:
            llm_client = await self._get_llm_client_for_channel(message.channel.id)
            if not llm_client:
                error_msg = self.llm_config.get('error_msg', {}).get('general_error',
                                                                     "LLM client is not available for this channel.")
                await message.reply(error_msg, silent=True)
                return
        except Exception as e:
            logger.error(f"Failed to get LLM client for channel {message.channel.id}: {e}", exc_info=True)
            await message.reply(self.exception_handler.handle_exception(e), silent=True)
            return
        image_contents, text_content = await self._prepare_multimodal_content(message)
        text_content = text_content.replace(f'<@!{self.bot.user.id}>', '').replace(f'<@{self.bot.user.id}>', '').strip()
        if not text_content and not image_contents:
            error_key = 'empty_reply' if is_reply_to_bot and not is_mentioned else 'empty_mention_reply'
            default_msg = "ä½•ã‹ãŠè©±ã—ãã ã•ã„ã€‚" if error_key == 'empty_reply' else "ã¯ã„ã€ä½•ã‹å¾¡ç”¨ã§ã—ã‚‡ã†ã‹ï¼Ÿ"
            await message.reply(self.llm_config.get('error_msg', {}).get(error_key, default_msg), silent=True)
            return
        guild_log = f"guild='{message.guild.name}({message.guild.id})'" if message.guild else "guild='DM'"
        channel_log = f"channel='{message.channel.name}({message.channel.id})'" if hasattr(message.channel,
                                                                                           'name') and message.channel.name else f"channel(id)={message.channel.id}"
        author_log = f"author='{message.author.name}({message.author.id})'"
        log_context = f"{guild_log}, {channel_log}, {author_log}"

        model_in_use = llm_client.model_name_for_api_calls
        logger.info(
            f"ğŸ“¨ Received LLM request | {log_context} | model='{model_in_use}' | image_count={len(image_contents)} | is_reply={is_reply_to_bot}")
        logger.info(f"ğŸ”µ [INPUT] User text content:\n{text_content}")

        thread_id = await self._get_conversation_thread_id(message)

        if not self.bio_manager or not self.memory_manager:
            await message.reply("å¿…è¦ãªãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€å¿œç­”ã§ãã¾ã›ã‚“ã€‚", silent=True)
            return

        system_prompt = self.bio_manager.get_system_prompt(
            channel_id=message.channel.id,
            user_id=message.author.id,
            user_display_name=message.author.display_name
        )

        try:
            now = datetime.now(self.jst)
            current_date_str = now.strftime('%Yå¹´%mæœˆ%dæ—¥')
            current_time_str = now.strftime('%H:%M')
            system_prompt = system_prompt.format(current_date=current_date_str, current_time=current_time_str)
        except (KeyError, ValueError) as e:
            logger.warning(
                f"Could not format system_prompt with date/time. It might be missing placeholders. Error: {e}")

        if formatted_memories := self.memory_manager.get_formatted_memories():
            system_prompt += f"\n\n{formatted_memories}"

        logger.info(f"ğŸ”µ [INPUT] System prompt prepared (length: {len(system_prompt)} chars)")

        messages_for_api: List[Dict[str, Any]] = [{"role": "system", "content": system_prompt}]

        conversation_history = await self._collect_conversation_history(message)
        messages_for_api.extend(conversation_history)

        user_content_parts = []
        if text_content:
            timestamp = message.created_at.astimezone(self.jst).strftime('[%H:%M]')
            formatted_text = f"{timestamp} {text_content}"
            user_content_parts.append({"type": "text", "text": formatted_text})

        user_content_parts.extend(image_contents)
        if image_contents:
            logger.info(f"ğŸ”µ [INPUT] Including {len(image_contents)} image(s) in request")

        user_message_for_api = {"role": "user", "content": user_content_parts}
        messages_for_api.append(user_message_for_api)

        logger.info(f"ğŸ”µ [INPUT] Total messages for API: {len(messages_for_api)} (system + history + user)")

        try:
            sent_message, llm_response = await self._handle_llm_streaming_response(
                message, messages_for_api, llm_client, log_context
            )

            if sent_message and llm_response:
                logger.info(f"ğŸŸ¢ [OUTPUT] LLM final response (length: {len(llm_response)} chars):\n{llm_response}")
                logger.info(f"âœ… LLM stream finished | {log_context} | model='{model_in_use}'")

                if thread_id not in self.conversation_threads:
                    self.conversation_threads[thread_id] = []
                self.conversation_threads[thread_id].append(user_message_for_api)

                assistant_message = {"role": "assistant", "content": llm_response, "message_id": sent_message.id}
                self.conversation_threads[thread_id].append(assistant_message)
                self.message_to_thread[sent_message.id] = thread_id
                self._cleanup_old_threads()

        except Exception as e:
            await message.reply(self.exception_handler.handle_exception(e), silent=True)

    def _cleanup_old_threads(self):
        max_threads = 100
        if len(self.conversation_threads) > max_threads:
            threads_to_remove = list(self.conversation_threads.keys())[:len(self.conversation_threads) - max_threads]
            for thread_id in threads_to_remove:
                del self.conversation_threads[thread_id]
                self.message_to_thread = {k: v for k, v in self.message_to_thread.items() if v != thread_id}

    async def _handle_llm_streaming_response(
            self,
            message: discord.Message,
            initial_messages: List[Dict[str, Any]],
            client: openai.AsyncOpenAI,
            log_context: str
    ) -> Tuple[Optional[discord.Message], str]:
        sent_message = None
        full_response_text = ""
        last_update = 0.0
        last_displayed_length = 0
        chunk_count = 0
        update_interval = 0.5
        min_update_chars = 15
        retry_sleep_time = 2.0
        placeholder = ":incoming_envelope: Thinking... :incoming_envelope:"
        emoji_prefix = ":incoming_envelope: "
        emoji_suffix = " :incoming_envelope:"
        logger.info(f"ğŸ”µ [STREAMING] Starting LLM stream | {log_context}")

        try:
            sent_message = await message.reply(placeholder, silent=True)
        except discord.HTTPException:
            sent_message = await message.channel.send(placeholder, silent=True)

        try:
            stream_generator = self._llm_stream_and_tool_handler(
                initial_messages, client, log_context, message.channel.id, message.author.id
            )

            async for content_chunk in stream_generator:
                chunk_count += 1
                full_response_text += content_chunk

                if chunk_count % 100 == 0:
                    logger.debug(
                        f"ğŸŸ¢ [STREAMING] Received chunk #{chunk_count}, total length: {len(full_response_text)} chars")

                current_time = time.time()
                chars_accumulated = len(full_response_text) - last_displayed_length

                should_update = (
                        current_time - last_update > update_interval and
                        chars_accumulated >= min_update_chars
                )

                if should_update and full_response_text:
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã¯çµµæ–‡å­—ã‚’å‰å¾Œã«è¿½åŠ 
                    max_content_length = DISCORD_MESSAGE_MAX_LENGTH - len(emoji_prefix) - len(emoji_suffix)
                    display_text = emoji_prefix + full_response_text[:max_content_length] + emoji_suffix

                    if display_text != sent_message.content:
                        try:
                            await sent_message.edit(content=display_text)
                            last_update = current_time
                            last_displayed_length = len(full_response_text)
                            logger.debug(
                                f"ğŸŸ¢ [STREAMING] Updated Discord message (displayed: {len(display_text)} chars)")
                        except discord.NotFound:
                            logger.warning(
                                f"âš ï¸ Message deleted during stream (ID: {sent_message.id}). Aborting.")
                            return None, ""
                        except discord.HTTPException as e:
                            if e.status == 429:
                                retry_after = (e.retry_after or 1.0) + 0.5
                                logger.warning(
                                    f"âš ï¸ Rate limited on message edit (ID: {sent_message.id}). "
                                    f"Waiting {retry_after:.2f}s"
                                )
                                await asyncio.sleep(retry_after)
                                last_update = time.time()
                            else:
                                logger.warning(
                                    f"âš ï¸ Failed to edit message (ID: {sent_message.id}): "
                                    f"{e.status} - {getattr(e, 'text', str(e))}"
                                )
                                await asyncio.sleep(retry_sleep_time)

            logger.info(
                f"ğŸŸ¢ [STREAMING] Stream completed | Total chunks: {chunk_count} | Final length: {len(full_response_text)} chars")

            if full_response_text:
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†å¾Œã¯çµµæ–‡å­—ã‚’å‰Šé™¤ã—ã¦æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆã®ã¿è¡¨ç¤º
                final_text = full_response_text[:DISCORD_MESSAGE_MAX_LENGTH]
                if final_text != sent_message.content:
                    try:
                        await sent_message.edit(content=final_text)
                        logger.info(f"ğŸŸ¢ [STREAMING] Final message updated successfully (emoji removed)")
                    except discord.HTTPException as e:
                        logger.error(
                            f"âŒ Failed to update final message (ID: {sent_message.id}): {e}"
                        )
            else:
                error_msg = self.llm_config.get('error_msg', {}).get(
                    'general_error', "AIã‹ã‚‰å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                )
                logger.warning(f"âš ï¸ Empty response from LLM")
                await sent_message.edit(content=error_msg)
                return None, ""

            return sent_message, full_response_text

        except Exception as e:
            logger.error(f"âŒ Error during LLM streaming response: {e}", exc_info=True)
            error_msg = self.exception_handler.handle_exception(e)
            if sent_message:
                try:
                    await sent_message.edit(content=error_msg)
                except discord.HTTPException:
                    pass
            else:
                await message.reply(error_msg, silent=True)
            return None, ""

    async def _llm_stream_and_tool_handler(
            self,
            messages: List[Dict[str, Any]],
            client: openai.AsyncOpenAI,
            log_context: str,
            channel_id: int,
            user_id: int
    ) -> AsyncGenerator[str, None]:
        current_messages = messages.copy()
        max_iterations = self.llm_config.get('max_tool_iterations', 5)
        extra_params = self.llm_config.get('extra_api_parameters', {})

        for iteration in range(max_iterations):
            logger.info(
                f"ğŸ”µ [API CALL] Starting LLM API call (iteration {iteration + 1}/{max_iterations}) | {log_context}")

            tools_def = self.get_tools_definition()
            api_kwargs = {
                "model": client.model_name_for_api_calls,
                "messages": current_messages,
                "stream": True,
                "temperature": extra_params.get('temperature', 0.7),
                "max_tokens": extra_params.get('max_tokens', 4096)
            }
            if tools_def:
                api_kwargs["tools"] = tools_def
                api_kwargs["tool_choice"] = "auto"
                logger.info(f"ğŸ”§ [TOOLS] Available tools: {[t['function']['name'] for t in tools_def]}")

            try:
                stream = await client.chat.completions.create(**api_kwargs)
                logger.info(f"ğŸ”µ [API CALL] Stream connection established")
            except Exception as e:
                logger.error(f"âŒ Error calling LLM API in stream handler: {e}", exc_info=True)
                raise

            tool_calls_buffer = []
            assistant_response_content = ""
            async for chunk in stream:
                delta = chunk.choices[0].delta
                if delta and delta.content:
                    assistant_response_content += delta.content
                    yield delta.content

                if delta and delta.tool_calls:
                    for tool_call_chunk in delta.tool_calls:
                        # indexãŒNoneã®å ´åˆã‚’è€ƒæ…®ã—ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                        chunk_index = tool_call_chunk.index if tool_call_chunk.index is not None else 0

                        if len(tool_calls_buffer) <= chunk_index:
                            tool_calls_buffer.append(
                                {"id": "", "type": "function", "function": {"name": "", "arguments": ""}})

                        buffer = tool_calls_buffer[chunk_index]
                        if tool_call_chunk.id:
                            buffer["id"] = tool_call_chunk.id
                        if tool_call_chunk.function:
                            if tool_call_chunk.function.name:
                                buffer["function"]["name"] = tool_call_chunk.function.name
                            if tool_call_chunk.function.arguments:
                                buffer["function"]["arguments"] += tool_call_chunk.function.arguments

            assistant_message = {"role": "assistant", "content": assistant_response_content or None}
            if tool_calls_buffer:
                assistant_message["tool_calls"] = tool_calls_buffer

            current_messages.append(assistant_message)

            if not tool_calls_buffer:
                logger.info(f"ğŸŸ¢ [OUTPUT] No tool calls, returning final response")
                return

            logger.info(f"ğŸ”§ [TOOLS] Processing {len(tool_calls_buffer)} tool call(s) in iteration {iteration + 1}")
            for tc in tool_calls_buffer:
                logger.info(
                    f"ğŸ”§ [TOOLS] Tool call: {tc['function']['name']} with args: {tc['function']['arguments'][:200]}")

            tool_calls_obj = [
                SimpleNamespace(
                    id=tc['id'],
                    function=SimpleNamespace(name=tc['function']['name'], arguments=tc['function']['arguments'])
                ) for tc in tool_calls_buffer
            ]
            await self._process_tool_calls(tool_calls_obj, current_messages, log_context, channel_id, user_id)

        logger.warning(f"âš ï¸ Tool processing exceeded max iterations ({max_iterations})")
        yield self.llm_config.get('error_msg', {}).get('tool_loop_timeout', "Tool processing exceeded max iterations.")

    async def _process_tool_calls(self, tool_calls: List[Any], messages: List[Dict[str, Any]],
                                  log_context: str, channel_id: int, user_id: int) -> None:
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            error_content = None
            tool_response_content = ""

            try:
                function_args = json.loads(tool_call.function.arguments)
                logger.info(f"ğŸ”§ [TOOL EXEC] Executing {function_name} | {log_context}")
                logger.info(f"ğŸ”§ [TOOL EXEC] Arguments: {json.dumps(function_args, ensure_ascii=False, indent=2)}")

                if self.search_agent and function_name == self.search_agent.name:
                    query_text = function_args.get('query', 'N/A')
                    logger.info(f"ğŸ” [SEARCH] Query: '{query_text}'")
                    tool_response_content = await self.search_agent.run(arguments=function_args, bot=self.bot,
                                                                        channel_id=channel_id)
                    logger.info(
                        f"ğŸ” [SEARCH] Result (length: {len(str(tool_response_content))} chars):\n{str(tool_response_content)[:1000]}")

                elif self.bio_manager and function_name == self.bio_manager.name:
                    logger.info(f"ğŸ‘¤ [BIO] Executing bio manager tool")
                    tool_response_content = await self.bio_manager.run_tool(arguments=function_args, user_id=user_id)
                    logger.info(f"ğŸ‘¤ [BIO] Result:\n{tool_response_content}")

                elif self.memory_manager and function_name == self.memory_manager.name:
                    logger.info(f"ğŸ§  [MEMORY] Executing memory manager tool")
                    tool_response_content = await self.memory_manager.run_tool(arguments=function_args)
                    logger.info(f"ğŸ§  [MEMORY] Result:\n{tool_response_content}")

                else:
                    logger.warning(f"âš ï¸ Unsupported tool called: {function_name} | {log_context}")
                    error_content = f"Error: Tool '{function_name}' is not available."

            except json.JSONDecodeError as e:
                logger.error(f"âŒ Error decoding tool arguments for {function_name}: {e}", exc_info=True)
                error_content = f"Error: Invalid JSON arguments - {str(e)}"
            except SearchAPIRateLimitError as e:
                logger.warning(f"âš ï¸ SearchAgent rate limit hit: {e}")
                error_content = "[Google Search Error]\nGoogleæ¤œç´¢APIã®åˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ç½®ã„ã¦ã‹ã‚‰å†è©¦è¡Œã™ã‚‹ã‚ˆã†ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ä¼ãˆã¦ãã ã•ã„ã€‚"
            except SearchAPIServerError as e:
                logger.error(f"âŒ SearchAgent server error: {e}")
                error_content = "[Google Search Error]\næ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã§ä¸€æ™‚çš„ãªã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ç½®ã„ã¦ã‹ã‚‰å†è©¦è¡Œã™ã‚‹ã‚ˆã†ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ä¼ãˆã¦ãã ã•ã„ã€‚"
            except SearchAgentError as e:
                logger.error(f"âŒ Error during SearchAgent execution for {function_name}: {e}", exc_info=True)
                error_content = f"[Google Search Error]\næ¤œç´¢ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            except Exception as e:
                logger.error(f"âŒ Unexpected error during tool call for {function_name}: {e}", exc_info=True)
                error_content = f"[Tool Error]\näºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

            final_content = error_content if error_content else tool_response_content
            logger.info(f"ğŸ”§ [TOOL RESULT] Sending tool response back to LLM (length: {len(final_content)} chars)")

            messages.append({
                "tool_call_id": tool_call.id,
                "role": "tool",
                "name": function_name,
                "content": final_content
            })

    @app_commands.command(
        name="set-ai-bio",
        description="ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIã®æ€§æ ¼ã‚„å½¹å‰²(bio)ã‚’è¨­å®šã—ã¾ã™ã€‚/ Set the AI's personality/role (bio) for this channel."
    )
    @app_commands.describe(
        bio="AIã«è¨­å®šã—ãŸã„æ€§æ ¼ã‚„å½¹å‰²ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚(ä¾‹: ã‚ãªãŸã¯çŒ«ã§ã™ã€‚èªå°¾ã«ã€Œã«ã‚ƒã‚“ã€ã‚’ã¤ã‘ã¦è©±ã—ã¾ã™ã€‚)"
    )
    async def set_ai_bio_slash(self, interaction: discord.Interaction, bio: str):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("âŒ BioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚", ephemeral=False)
            return

        if len(bio) > 1024:
            await interaction.followup.send("âš ï¸ AIã®bioãŒé•·ã™ãã¾ã™ã€‚1024æ–‡å­—ä»¥å†…ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚", ephemeral=False)
            return

        try:
            await self.bio_manager.set_channel_bio(interaction.channel_id, bio)
            logger.info(f"AI bio for channel {interaction.channel_id} set by {interaction.user.name}")
            embed = discord.Embed(
                title="âœ… AIã®bioã‚’è¨­å®šã—ã¾ã—ãŸ",
                description=f"ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ã®AIã®å½¹å‰²ãŒä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã•ã‚Œã¾ã—ãŸã€‚\n\n**æ–°ã—ã„AIã®bio:**\n```\n{bio}\n```",
                color=discord.Color.green()
            )
            await interaction.followup.send(embed=embed, ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save channel AI bio settings: {e}", exc_info=True)
            await interaction.followup.send("âŒ AIã®bioè¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", ephemeral=False)

    @app_commands.command(
        name="show-ai-bio",
        description="ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIã«ç¾åœ¨è¨­å®šã•ã‚Œã¦ã„ã‚‹bioã‚’è¡¨ç¤ºã—ã¾ã™ã€‚/ Show the AI's current bio for this channel."
    )
    async def show_ai_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("âŒ BioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚", ephemeral=False)
            return

        current_bio = self.bio_manager.get_channel_bio(interaction.channel_id)
        if current_bio:
            title = "ç¾åœ¨ã®AIã®bio"
            description = f"ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ã¯ã€AIã«ä»¥ä¸‹ã®å½¹å‰²ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n**AIã®bio:**\n```\n{current_bio}\n```"
            color = discord.Color.blue()
        else:
            default_prompt = self.llm_config.get('system_prompt', "è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            try:
                now = datetime.now(self.jst)
                current_date_str = now.strftime('%Yå¹´%mæœˆ%dæ—¥')
                current_time_str = now.strftime('%H:%M')
                formatted_prompt = default_prompt.format(current_date=current_date_str, current_time=current_time_str)
            except (KeyError, ValueError):
                formatted_prompt = default_prompt

            title = "ç¾åœ¨ã®AIã®bio"
            description = f"ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«ã¯å°‚ç”¨ã®AI bioãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\nã‚µãƒ¼ãƒãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n\n**ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š:**\n```\n{formatted_prompt}\n```"
            color = discord.Color.greyple()
        embed = discord.Embed(title=title, description=description, color=color)
        await interaction.followup.send(embed=embed, ephemeral=False)

    @app_commands.command(
        name="reset-ai-bio",
        description="ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIã®bioã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã«æˆ»ã—ã¾ã™ã€‚/ Reset the AI's bio to default for this channel."
    )
    async def reset_ai_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("âŒ BioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚", ephemeral=False)
            return

        try:
            if await self.bio_manager.reset_channel_bio(interaction.channel_id):
                logger.info(f"AI bio for channel {interaction.channel_id} reset by {interaction.user.name}")
                default_prompt = self.llm_config.get('system_prompt', 'æœªè¨­å®š')
                try:
                    now = datetime.now(self.jst)
                    current_date_str = now.strftime('%Yå¹´%mæœˆ%dæ—¥')
                    current_time_str = now.strftime('%H:%M')
                    formatted_prompt = default_prompt.format(current_date=current_date_str,
                                                             current_time=current_time_str)
                except (KeyError, ValueError):
                    formatted_prompt = default_prompt

                display_prompt = (formatted_prompt[:100] + '...') if len(
                    formatted_prompt) > 103 else formatted_prompt

                await interaction.followup.send(
                    f"âœ… ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIã®bioã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã«æˆ»ã—ã¾ã—ãŸã€‚\n> ç¾åœ¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `{display_prompt}`",
                    ephemeral=False
                )
            else:
                await interaction.followup.send("â„¹ï¸ ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«ã¯å°‚ç”¨ã®AI bioãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                                                ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save channel AI bio settings after reset: {e}", exc_info=True)
            await interaction.followup.send("âŒ AIã®bioè¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", ephemeral=False)

    @app_commands.command(
        name="set-user-bio",
        description="AIã«ã‚ãªãŸã®æƒ…å ±ã‚’è¨˜æ†¶ã•ã›ã¾ã™ã€‚/ Save your information for the AI to remember."
    )
    @app_commands.describe(
        bio="AIã«è¦šãˆã¦ã»ã—ã„ã‚ãªãŸã®æƒ…å ±ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚(ä¾‹: ç§ã®åå‰ã¯ç”°ä¸­ã§ã™ã€‚è¶£å‘³ã¯èª­æ›¸ã§ã™ã€‚)",
        mode="ä¿å­˜ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚'ä¸Šæ›¸ã'ã¾ãŸã¯'è¿½è¨˜'ãŒå¯èƒ½ã§ã™ã€‚"
    )
    @app_commands.choices(mode=[
        app_commands.Choice(name="ä¸Šæ›¸ã (Overwrite)", value="overwrite"),
        app_commands.Choice(name="è¿½è¨˜ (Append)", value="append"),
    ])
    async def set_user_bio_slash(self, interaction: discord.Interaction, bio: str, mode: app_commands.Choice[str]):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("âŒ BioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚", ephemeral=False)
            return

        if len(bio) > 1024:
            await interaction.followup.send("âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±(bio)ãŒé•·ã™ãã¾ã™ã€‚1024æ–‡å­—ä»¥å†…ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚",
                                            ephemeral=False)
            return

        try:
            await self.bio_manager.set_user_bio(interaction.user.id, bio, mode=mode.value)
            logger.info(
                f"User bio for {interaction.user.name} ({interaction.user.id}) was set with mode '{mode.value}'.")

            updated_bio = self.bio_manager.get_user_bio(interaction.user.id)

            embed = discord.Embed(
                title=f"âœ… ã‚ãªãŸã®æƒ…å ±ã‚’è¨˜æ†¶ã—ã¾ã—ãŸ ({mode.name})",
                description=f"AIã¯ã‚ãªãŸã®æƒ…å ±ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¨˜æ†¶ã—ã¾ã—ãŸã€‚\n\n**ã‚ãªãŸã®bio:**\n```\n{updated_bio}\n```",
                color=discord.Color.green()
            )
            await interaction.followup.send(embed=embed, ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save user bio settings: {e}", exc_info=True)
            await interaction.followup.send("âŒ ã‚ãªãŸã®æƒ…å ±ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", ephemeral=False)

    @app_commands.command(
        name="show-user-bio",
        description="AIãŒè¨˜æ†¶ã—ã¦ã„ã‚‹ã‚ãªãŸã®æƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚/ Show the information the AI has stored about you."
    )
    async def show_user_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("âŒ BioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚", ephemeral=False)
            return

        current_bio = self.bio_manager.get_user_bio(interaction.user.id)
        if current_bio:
            embed = discord.Embed(
                title=f"ğŸ’¡ {interaction.user.display_name}ã•ã‚“ã®æƒ…å ±",
                description=f"**bio:**\n```\n{current_bio}\n```",
                color=discord.Color.blue()
            )
        else:
            embed = discord.Embed(
                title=f"ğŸ’¡ {interaction.user.display_name}ã•ã‚“ã®æƒ…å ±",
                description="ç¾åœ¨ã€ã‚ãªãŸã«é–¢ã™ã‚‹æƒ…å ±ã¯ä½•ã‚‚è¨˜æ†¶ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n`/set-user-bio` ã‚³ãƒãƒ³ãƒ‰ã‹ã€ä¼šè©±ã®ä¸­ã§AIã«è¨˜æ†¶ã‚’é ¼ã‚€ã“ã¨ã§è¨­å®šã§ãã¾ã™ã€‚",
                color=discord.Color.greyple()
            )
        await interaction.followup.send(embed=embed, ephemeral=False)

    @app_commands.command(
        name="reset-user-bio",
        description="AIãŒè¨˜æ†¶ã—ã¦ã„ã‚‹ã‚ãªãŸã®æƒ…å ±ã‚’ã™ã¹ã¦å‰Šé™¤ã—ã¾ã™ã€‚/ Delete all information the AI has stored about you."
    )
    async def reset_user_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            await interaction.followup.send("âŒ BioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚", ephemeral=False)
            return

        try:
            if await self.bio_manager.reset_user_bio(interaction.user.id):
                logger.info(f"User bio for {interaction.user.name} ({interaction.user.id}) was reset.")
                await interaction.followup.send(
                    f"âœ… {interaction.user.display_name}ã•ã‚“ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ã™ã¹ã¦å‰Šé™¤ã—ã¾ã—ãŸã€‚", ephemeral=False)
            else:
                await interaction.followup.send("â„¹ï¸ ã‚ãªãŸã«é–¢ã™ã‚‹æƒ…å ±ã¯ä½•ã‚‚è¨˜æ†¶ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save user bio settings after reset: {e}", exc_info=True)
            await interaction.followup.send("âŒ ã‚ãªãŸã®æƒ…å ±ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", ephemeral=False)

    @app_commands.command(
        name="memory-save",
        description="ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã«æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã™ã€‚/ Save information to the global shared memory."
    )
    @app_commands.describe(
        key="æƒ…å ±ã®ã‚­ãƒ¼ï¼ˆé …ç›®åï¼‰ ä¾‹: 'é–‹ç™ºè€…ã‹ã‚‰ã®ãŠçŸ¥ã‚‰ã›'",
        value="æƒ…å ±ã®å†…å®¹ ä¾‹: 'æ¬¡å›ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã¯...'"
    )
    async def memory_save_slash(self, interaction: discord.Interaction, key: str, value: str):
        await interaction.response.defer(ephemeral=False)
        if not self.memory_manager:
            await interaction.followup.send("âŒ MemoryManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚", ephemeral=False)
            return

        try:
            await self.memory_manager.save_memory(key, value)
            embed = discord.Embed(
                title="âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã—ã¾ã—ãŸ",
                color=discord.Color.green()
            )
            embed.add_field(name="ã‚­ãƒ¼", value=f"```{key}```", inline=False)
            embed.add_field(name="å€¤", value=f"```{value}```", inline=False)
            await interaction.followup.send(embed=embed, ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save global memory via command: {e}", exc_info=True)
            await interaction.followup.send("âŒ ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", ephemeral=False)

    @app_commands.command(
        name="memory-list",
        description="ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã®æƒ…å ±ã‚’ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚/ List all global shared memories."
    )
    async def memory_list_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.memory_manager:
            await interaction.followup.send("âŒ MemoryManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚", ephemeral=False)
            return

        memories = self.memory_manager.list_memories()
        if not memories:
            await interaction.followup.send("â„¹ï¸ ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã«ã¯ä½•ã‚‚ä¿å­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", ephemeral=False)
            return

        embed = discord.Embed(
            title="ğŸŒ ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒª",
            color=discord.Color.blue()
        )
        description = ""
        for key, value in memories.items():
            field_text = f"**{key}**: {value}\n"
            if len(description) + len(field_text) > 4000:
                description += "\n... (è¡¨ç¤ºåˆ¶é™ã®ãŸã‚ä¸€éƒ¨çœç•¥)"
                break
            description += field_text

        embed.description = description
        await interaction.followup.send(embed=embed, ephemeral=False)

    async def memory_key_autocomplete(self, interaction: discord.Interaction, current: str) -> List[
        app_commands.Choice[str]]:
        if not self.memory_manager:
            return []
        keys = self.memory_manager.list_memories().keys()
        return [
                   app_commands.Choice(name=key, value=key)
                   for key in keys if current.lower() in key.lower()
               ][:25]

    @app_commands.command(
        name="memory-delete",
        description="ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã‹ã‚‰æƒ…å ±ã‚’å‰Šé™¤ã—ã¾ã™ã€‚/ Delete a global shared memory."
    )
    @app_commands.describe(key="å‰Šé™¤ã—ãŸã„æƒ…å ±ã®ã‚­ãƒ¼")
    @app_commands.autocomplete(key=memory_key_autocomplete)
    async def memory_delete_slash(self, interaction: discord.Interaction, key: str):
        await interaction.response.defer(ephemeral=False)
        if not self.memory_manager:
            await interaction.followup.send("âŒ MemoryManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚", ephemeral=False)
            return

        try:
            if await self.memory_manager.delete_memory(key):
                await interaction.followup.send(f"âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã‹ã‚‰ã‚­ãƒ¼ '{key}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚",
                                                ephemeral=False)
            else:
                await interaction.followup.send(f"âš ï¸ ã‚­ãƒ¼ '{key}' ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚",
                                                ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to delete global memory via command: {e}", exc_info=True)
            await interaction.followup.send("âŒ ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã‹ã‚‰ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", ephemeral=False)

    async def model_autocomplete(self, interaction: discord.Interaction, current: str) -> List[
        app_commands.Choice[str]]:
        available_models = self.llm_config.get('available_models', [])
        return [
                   app_commands.Choice(name=model, value=model)
                   for model in available_models if current.lower() in model.lower()
               ][:25]

    @app_commands.command(
        name="switch-models",
        description="ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚/ Switches the AI model used for this channel."
    )
    @app_commands.describe(
        model="ä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
    )
    @app_commands.autocomplete(model=model_autocomplete)
    async def switch_model_slash(self, interaction: discord.Interaction, model: str):
        await interaction.response.defer(ephemeral=False)
        available_models = self.llm_config.get('available_models', [])
        if model not in available_models:
            await interaction.followup.send(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ« '{model}' ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
            return

        channel_id_str = str(interaction.channel_id)
        self.channel_models[channel_id_str] = model

        try:
            await self._save_channel_models()
            await self._get_llm_client_for_channel(interaction.channel_id)
            await interaction.followup.send(f"âœ… ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ãŒ `{model}` ã«åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¾ã—ãŸã€‚",
                                            ephemeral=False)
            logger.info(f"Model for channel {interaction.channel_id} switched to '{model}' by {interaction.user.name}")
        except Exception as e:
            logger.error(f"Failed to save channel model settings: {e}", exc_info=True)
            await interaction.followup.send("âŒ è¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    @app_commands.command(
        name="switch-models-default-server",
        description="ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒ¼ãƒãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã«æˆ»ã—ã¾ã™ã€‚/ Resets the AI model for this channel to the server default."
    )
    async def reset_model_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        channel_id_str = str(interaction.channel_id)

        if channel_id_str in self.channel_models:
            del self.channel_models[channel_id_str]
            try:
                await self._save_channel_models()
                default_model = self.llm_config.get('model', 'æœªè¨­å®š')
                await interaction.followup.send(
                    f"âœ… ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (`{default_model}`) ã«æˆ»ã—ã¾ã—ãŸã€‚", ephemeral=False)
                logger.info(f"Model for channel {interaction.channel_id} reset to default by {interaction.user.name}")
            except Exception as e:
                logger.error(f"Failed to save channel model settings after reset: {e}", exc_info=True)
                await interaction.followup.send("âŒ è¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        else:
            await interaction.followup.send("â„¹ï¸ ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«ã¯å°‚ç”¨ã®ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", ephemeral=False)

    @switch_model_slash.error
    async def switch_model_error(self, interaction: discord.Interaction, error: app_commands.AppCommandError):
        logger.error(f"Error in /switch-model command: {error}", exc_info=True)
        if not interaction.response.is_done():
            await interaction.response.send_message(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}", ephemeral=False)
        else:
            await interaction.followup.send(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}", ephemeral=False)

    @app_commands.command(name="llm_help",
                          description="LLM (AIå¯¾è©±) æ©Ÿèƒ½ã®ãƒ˜ãƒ«ãƒ—ã¨åˆ©ç”¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚/ Displays help and usage guidelines for LLM (AI Chat) features.")
    async def llm_help_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        bot_user = self.bot.user or interaction.client.user
        bot_name = bot_user.name if bot_user else "å½“Bot"
        embed = discord.Embed(title=f"ğŸ’¡ {bot_name} AIå¯¾è©±æ©Ÿèƒ½ãƒ˜ãƒ«ãƒ—ï¼†ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
                              description=f"{bot_name}ã®AIå¯¾è©±æ©Ÿèƒ½ã«ã¤ã„ã¦ã®èª¬æ˜ã¨åˆ©ç”¨è¦ç´„ã§ã™ã€‚",
                              color=discord.Color.purple())
        if bot_user and bot_user.avatar: embed.set_thumbnail(url=bot_user.avatar.url)
        embed.add_field(
            name="åŸºæœ¬çš„ãªä½¿ã„æ–¹",
            value=(
                f"â€¢ Botã«ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ (`@{bot_name}`) ã—ã¦è©±ã—ã‹ã‘ã‚‹ã¨ã€AIãŒå¿œç­”ã—ã¾ã™ã€‚\n"
                f"â€¢ **Botã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿”ä¿¡ã™ã‚‹ã“ã¨ã§ã‚‚ä¼šè©±ã‚’ç¶šã‘ã‚‰ã‚Œã¾ã™ï¼ˆãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ä¸è¦ï¼‰ã€‚**\n"
                f"â€¢ ã€Œç§ã®åå‰ã¯ã€‡ã€‡ã§ã™ã€‚è¦šãˆã¦ãŠã„ã¦ã€ã®ã‚ˆã†ã«è©±ã—ã‹ã‘ã‚‹ã¨ã€AIãŒã‚ãªãŸã®æƒ…å ±ã‚’è¨˜æ†¶ã—ã‚ˆã†ã¨ã—ã¾ã™ã€‚\n"
                f"â€¢ ç”»åƒã¨ä¸€ç·’ã«è©±ã—ã‹ã‘ã‚‹ã¨ã€AIãŒç”»åƒã®å†…å®¹ã‚‚ç†è§£ã—ã‚ˆã†ã¨ã—ã¾ã™ã€‚"
            ),
            inline=False
        )
        embed.add_field(
            name="ä¾¿åˆ©ãªã‚³ãƒãƒ³ãƒ‰",
            value=(
                "**ã€AIã®è¨­å®š (ãƒãƒ£ãƒ³ãƒãƒ«ã”ã¨)ã€‘**\n"
                "â€¢ `/switch-models`: ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ä½¿ã†AIãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã—ã¾ã™ã€‚\n"
                "â€¢ `/set-ai-bio`: ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«å°‚ç”¨ã®AIã®æ€§æ ¼ã‚„å½¹å‰²ã‚’è¨­å®šã—ã¾ã™ã€‚\n"
                "â€¢ `/show-ai-bio`: ç¾åœ¨ã®AIã®bioè¨­å®šã‚’ç¢ºèªã—ã¾ã™ã€‚\n"
                "â€¢ `/reset-ai-bio`: AIã®bioè¨­å®šã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã—ã¾ã™ã€‚\n"
                "**ã€ã‚ãªãŸã®æƒ…å ±ã€‘**\n"
                "â€¢ `/set-user-bio`: AIã«è¦šãˆã¦ã»ã—ã„ã‚ãªãŸã®æƒ…å ±ã‚’è¨­å®šã—ã¾ã™ã€‚\n"
                "â€¢ `/show-user-bio`: AIãŒè¨˜æ†¶ã—ã¦ã„ã‚‹ã‚ãªãŸã®æƒ…å ±ã‚’ç¢ºèªã—ã¾ã™ã€‚\n"
                "â€¢ `/reset-user-bio`: ã‚ãªãŸã®æƒ…å ±ã‚’AIã®è¨˜æ†¶ã‹ã‚‰å‰Šé™¤ã—ã¾ã™ã€‚\n"
                "**ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã€‘**\n"
                "â€¢ `/memory-save`: å…¨ã‚µãƒ¼ãƒãƒ¼å…±é€šã®ãƒ¡ãƒ¢ãƒªã«æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã™ã€‚\n"
                "â€¢ `/memory-list`: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã®æƒ…å ±ã‚’ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚\n"
                "â€¢ `/memory-delete`: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã‹ã‚‰æƒ…å ±ã‚’å‰Šé™¤ã—ã¾ã™ã€‚\n"
                "**ã€ãã®ä»–ã€‘**\n"
                "â€¢ `/clear_history`: ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã™ã€‚"
            ),
            inline=False
        )
        channel_model_str = self.channel_models.get(str(interaction.channel_id))
        model_display = f"`{channel_model_str}` (ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«å°‚ç”¨)" if channel_model_str else f"`{self.llm_config.get('model', 'æœªè¨­å®š')}` (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)"

        ai_bio_display = "N/A"
        user_bio_display = "N/A"
        if self.bio_manager:
            ai_bio_display = "âœ… (å°‚ç”¨è¨­å®šã‚ã‚Š)" if self.bio_manager.get_channel_bio(interaction.channel_id) else "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ"
            user_bio_display = "âœ… (è¨˜æ†¶ã‚ã‚Š)" if self.bio_manager.get_user_bio(interaction.user.id) else "ãªã—"

        active_tools = self.llm_config.get('active_tools', [])
        tools_info = "â€¢ ãªã—" if not active_tools else "â€¢ " + ", ".join(active_tools)
        embed.add_field(name="ç¾åœ¨ã®AIè¨­å®š",
                        value=f"â€¢ **ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:** {model_display}\n"
                              f"â€¢ **AIã®å½¹å‰²(ãƒãƒ£ãƒ³ãƒãƒ«):** {ai_bio_display} (è©³ç´°ã¯ `/show-ai-bio`)\n"
                              f"â€¢ **ã‚ãªãŸã®æƒ…å ±:** {user_bio_display} (è©³ç´°ã¯ `/show-user-bio`)\n"
                              f"â€¢ **ä¼šè©±å±¥æ­´ã®æœ€å¤§ä¿æŒæ•°:** {self.llm_config.get('max_messages', 'æœªè¨­å®š')} ãƒšã‚¢\n"
                              f"â€¢ **ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹æœ€å¤§ç”»åƒæšæ•°:** {self.llm_config.get('max_images', 'æœªè¨­å®š')} æš\n"
                              f"â€¢ **åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:** {tools_info}",
                        inline=False)
        embed.add_field(name="--- ğŸ“œ AIåˆ©ç”¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ ---",
                        value="AIæ©Ÿèƒ½ã‚’å®‰å…¨ã«ã”åˆ©ç”¨ã„ãŸã ããŸã‚ã€ä»¥ä¸‹ã®å†…å®¹ã‚’å¿…ãšã”ç¢ºèªãã ã•ã„ã€‚", inline=False)
        embed.add_field(name="âš ï¸ 1. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ™‚ã®æ³¨æ„", value=(
            "AIã«è¨˜æ†¶ã•ã›ã‚‹æƒ…å ±ã«ã¯ã€æ°åã€é€£çµ¡å…ˆã€ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãªã©ã®**å€‹äººæƒ…å ±ã‚„ç§˜å¯†æƒ…å ±ã‚’çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚**"),
                        inline=False)
        embed.add_field(name="âœ… 2. ç”Ÿæˆç‰©åˆ©ç”¨æ™‚ã®æ³¨æ„", value=(
            "AIã®å¿œç­”ã«ã¯è™šå½ã‚„åè¦‹ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚**å¿…ãšãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã€è‡ªå·±ã®è²¬ä»»ã§åˆ©ç”¨ã—ã¦ãã ã•ã„ã€‚**"),
                        inline=False)
        embed.set_footer(text="ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¯äºˆå‘Šãªãå¤‰æ›´ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")
        await interaction.followup.send(embed=embed, ephemeral=False)

    @app_commands.command(name="llm_help_en",
                          description="Displays help and usage guidelines for LLM (AI Chat) features.")
    async def llm_help_en_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        bot_user = self.bot.user or interaction.client.user
        bot_name = bot_user.name if bot_user else "This Bot"
        embed = discord.Embed(title=f"ğŸ’¡ {bot_name} AI Chat Help & Guidelines",
                              description=f"Explanation and terms of use for the AI chat features of {bot_name}.",
                              color=discord.Color.purple())
        if bot_user and bot_user.avatar: embed.set_thumbnail(url=bot_user.avatar.url)
        embed.add_field(
            name="Basic Usage",
            value=(
                f"â€¢ Mention the bot (`@{bot_name}`) to get a response from the AI.\n"
                f"â€¢ **You can also continue the conversation by replying to the bot's messages (no mention needed).**\n"
                f"â€¢ If you ask the AI to remember something (e.g., 'My name is John, please remember it'), it will try to store that information.\n"
                f"â€¢ Attach images or paste image URLs with your message, and the AI will try to understand them."
            ),
            inline=False
        )
        embed.add_field(
            name="Useful Commands",
            value=(
                "**[AI Settings (Per Channel)]**\n"
                "â€¢ `/switch-models`: Change the AI model used in this channel.\n"
                "â€¢ `/set-ai-bio`: Set a custom personality/role for the AI in this channel.\n"
                "â€¢ `/show-ai-bio`: Check the current AI bio setting.\n"
                "â€¢ `/reset-ai-bio`: Reset the AI bio to the default.\n"
                "**[Your Information]**\n"
                "â€¢ `/set-user-bio`: Set information about you for the AI to remember.\n"
                "â€¢ `/show-user-bio`: Check the information the AI has stored about you.\n"
                "â€¢ `/reset-user-bio`: Delete your information from the AI's memory.\n"
                "**[Global Memory]**\n"
                "â€¢ `/memory-save`: Save information to the global shared memory.\n"
                "â€¢ `/memory-list`: List all information in the global memory.\n"
                "â€¢ `/memory-delete`: Delete information from the global memory.\n"
                "**[Other]**\n"
                "â€¢ `/clear_history`: Reset the conversation history."
            ),
            inline=False
        )
        channel_model_str = self.channel_models.get(str(interaction.channel_id))
        model_display = f"`{channel_model_str}` (Channel-specific)" if channel_model_str else f"`{self.llm_config.get('model', 'Not set')}` (Default)"

        ai_bio_display = "N/A"
        user_bio_display = "N/A"
        if self.bio_manager:
            ai_bio_display = "âœ… (Custom)" if self.bio_manager.get_channel_bio(interaction.channel_id) else "Default"
            user_bio_display = "âœ… (Stored)" if self.bio_manager.get_user_bio(interaction.user.id) else "None"

        active_tools = self.llm_config.get('active_tools', [])
        tools_info = "â€¢ None" if not active_tools else "â€¢ " + ", ".join(active_tools)
        embed.add_field(name="Current AI Settings",
                        value=f"â€¢ **Model in Use:** {model_display}\n"
                              f"â€¢ **AI Role (Channel):** {ai_bio_display} (see `/show-ai-bio`)\n"
                              f"â€¢ **Your Info:** {user_bio_display} (see `/show-user-bio`)\n"
                              f"â€¢ **Max Conversation History:** {self.llm_config.get('max_messages', 'Not set')} pairs\n"
                              f"â€¢ **Max Images Processed at Once:** {self.llm_config.get('max_images', 'Not set')} image(s)\n"
                              f"â€¢ **Available Tools:** {tools_info}",
                        inline=False)
        embed.add_field(name="--- ğŸ“œ AI Usage Guidelines ---",
                        value="Please review the following to ensure safe use of the AI features.", inline=False)
        embed.add_field(name="âš ï¸ 1. Precautions for Data Input", value=(
            "**NEVER include personal or confidential information** such as your name, contact details, or passwords in the information you ask the AI to remember."),
                        inline=False)
        embed.add_field(name="âœ… 2. Precautions for Using Generated Output", value=(
            "The AI's responses may contain inaccuracies or biases. **Always fact-check and use them at your own risk.**"),
                        inline=False)
        embed.set_footer(text="These guidelines are subject to change without notice.")
        await interaction.followup.send(embed=embed, ephemeral=False)

    @app_commands.command(
        name="clear_history",
        description="ç¾åœ¨ã®ä¼šè©±ã‚¹ãƒ¬ãƒƒãƒ‰ã®å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚/ Clears the history of the current conversation thread."
    )
    async def clear_history_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        cleared_count = 0
        threads_to_clear = set()
        try:
            async for msg in interaction.channel.history(limit=200):
                if msg.id in self.message_to_thread:
                    threads_to_clear.add(self.message_to_thread[msg.id])
        except (discord.Forbidden, discord.HTTPException):
            await interaction.followup.send("âš ï¸ ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        for thread_id in threads_to_clear:
            if thread_id in self.conversation_threads:
                del self.conversation_threads[thread_id]
                self.message_to_thread = {k: v for k, v in self.message_to_thread.items() if v != thread_id}
                cleared_count += 1
        if cleared_count > 0:
            await interaction.followup.send(
                f"âœ… ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«é–¢é€£ã™ã‚‹ {cleared_count} å€‹ã®ä¼šè©±ã‚¹ãƒ¬ãƒƒãƒ‰ã®å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
        else:
            await interaction.followup.send("â„¹ï¸ ã‚¯ãƒªã‚¢å¯¾è±¡ã®ä¼šè©±å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


async def setup(bot: commands.Bot):
    """Sets up the LLMCog."""
    try:
        await bot.add_cog(LLMCog(bot))
        logger.info("LLMCog loaded successfully.")
    except Exception as e:
        logger.critical(f"Failed to set up LLMCog: {e}", exc_info=True)
        raise