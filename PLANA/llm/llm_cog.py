# PLANA/llm/llm_cog.py
from __future__ import annotations

import asyncio
import base64
import io
import json
import logging
import os
import re
import time
from datetime import datetime, timezone, timedelta
from types import SimpleNamespace
from typing import List, Dict, Any, Tuple, Optional, AsyncGenerator, Union

import aiohttp
import discord
import openai
from discord import app_commands
from discord.ext import commands

from PLANA.llm.error.errors import (
    LLMExceptionHandler,
    SearchAgentError,
    SearchAPIRateLimitError,
    SearchAPIServerError
)

try:
    from langdetect import detect, LangDetectException
except ImportError:
    detect = None
    LangDetectException = None
    logging.warning("langdetect library not found. Language detection will be disabled. "
                    "Install with: pip install langdetect")

try:
    from PLANA.llm.plugins.search_agent import SearchAgent
except ImportError:
    logging.error("Could not import SearchAgent. Search functionality will be disabled.")
    SearchAgent = None

try:
    from PLANA.llm.plugins.bio_manager import BioManager
except ImportError:
    logging.error("Could not import BioManager. Bio functionality will be disabled.")
    BioManager = None

try:
    from PLANA.llm.plugins.memory_manager import MemoryManager
except ImportError:
    logging.error("Could not import MemoryManager. Memory functionality will be disabled.")
    MemoryManager = None

try:
    from PLANA.llm.plugins.commands_manager import CommandInfoManager
except ImportError:
    logging.error("Could not import CommandInfoManager. Command suggestions will be disabled.")
    CommandInfoManager = None

try:
    from PLANA.llm.plugins.image_generator import ImageGenerator
except ImportError:
    logging.error("Could not import ImageGenerator. Image generation will be disabled.")
    ImageGenerator = None

try:
    from PLANA.llm.utils.tips import TipsManager
except ImportError:
    logging.error("Could not import TipsManager. Tips functionality will be disabled.")
    TipsManager = None

try:
    import aiofiles
except ImportError:
    aiofiles = None
    logging.warning("aiofiles library not found. Channel model settings will be saved synchronously. "
                    "Install with: pip install aiofiles")

logger = logging.getLogger(__name__)

# Constants
SUPPORTED_IMAGE_EXTENSIONS = ('.png', '.jpeg', '.jpg', '.gif', '.webp')
IMAGE_URL_PATTERN = re.compile(
    r'https?://[^\s]+\.(?:' + '|'.join(ext.lstrip('.') for ext in SUPPORTED_IMAGE_EXTENSIONS) + r')(?:\?[^\s]*)?',
    re.IGNORECASE
)
DISCORD_MESSAGE_MAX_LENGTH = 2000
SAFE_MESSAGE_LENGTH = 1990  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³


def _split_message_smartly(text: str, max_length: int) -> List[str]:
    if len(text) <= max_length: return [text]
    chunks, remaining = [], text
    while remaining:
        if len(remaining) <= max_length:
            chunks.append(remaining)
            break
        chunk = remaining[:max_length]
        split_point = _find_best_split_point(chunk)
        if split_point == -1: split_point = max_length - 20
        chunk_text = remaining[:split_point].rstrip()
        if chunk_text: chunks.append(chunk_text)
        remaining = remaining[split_point:].lstrip()
    return chunks


def _find_best_split_point(chunk: str) -> int:
    code_block_end = chunk.rfind('```\n')
    if code_block_end > len(chunk) * 0.5: return code_block_end + 4
    paragraph_break = chunk.rfind('\n\n')
    if paragraph_break > len(chunk) * 0.5: return paragraph_break + 2
    newline = chunk.rfind('\n')
    if newline > len(chunk) * 0.6: return newline + 1
    japanese_period = max(chunk.rfind('ã€‚'), chunk.rfind('ï¼'), chunk.rfind('ï¼Ÿ'))
    if japanese_period > len(chunk) * 0.7: return japanese_period + 1
    english_period = max(chunk.rfind('. '), chunk.rfind('! '), chunk.rfind('? '))
    if english_period > len(chunk) * 0.7: return english_period + 2
    comma = max(chunk.rfind('ã€'), chunk.rfind(', '))
    if comma > len(chunk) * 0.7: return comma + 1
    space = chunk.rfind(' ')
    if space > len(chunk) * 0.7: return space + 1
    return -1


class ThreadCreationView(discord.ui.View):
    """ã‚¹ãƒ¬ãƒƒãƒ‰ä½œæˆãƒœã‚¿ãƒ³ã®Viewã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, llm_cog, original_message: discord.Message):
        super().__init__(timeout=300)  # 5åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        self.llm_cog = llm_cog
        self.original_message = original_message
    
    @discord.ui.button(label="ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½œæˆã™ã‚‹ / Create Thread", style=discord.ButtonStyle.primary, emoji="ğŸ§µ")
    async def create_thread(self, interaction: discord.Interaction, button: discord.ui.Button):
        await interaction.response.defer(ephemeral=True)
        
        try:
            # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½œæˆ
            thread = await self.original_message.create_thread(
                name=f"AI Chat - {interaction.user.display_name}",
                auto_archive_duration=60,  # 1æ™‚é–“ã§ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
                reason="AI conversation thread created by user"
            )
            
            # å…ƒã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ä½œæˆå‰ã®å±¥æ­´ï¼‰
            messages = []
            try:
                # å…ƒã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰é¡ã£ã¦ä¼šè©±å±¥æ­´ã‚’åé›†
                current_msg = self.original_message
                visited_ids = set()
                message_count = 0
                
                while current_msg and message_count < 40:
                    if current_msg.id in visited_ids:
                        break
                    visited_ids.add(current_msg.id)
                    
                    if current_msg.author != self.llm_cog.bot.user:
                        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†
                        image_contents, text_content = await self.llm_cog._prepare_multimodal_content(current_msg)
                        text_content = text_content.replace(f'<@!{self.llm_cog.bot.user.id}>', '').replace(f'<@{self.llm_cog.bot.user.id}>', '').strip()
                        
                        if text_content or image_contents:
                            user_content_parts = []
                            if text_content:
                                user_content_parts.append({
                                    "type": "text",
                                    "text": f"{current_msg.created_at.astimezone(self.llm_cog.jst).strftime('[%H:%M]')} {text_content}"
                                })
                            user_content_parts.extend(image_contents)
                            messages.append({"role": "user", "content": user_content_parts})
                            message_count += 1
                    
                    # å‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
                    if current_msg.reference and current_msg.reference.message_id:
                        try:
                            current_msg = current_msg.reference.resolved or await current_msg.channel.fetch_message(current_msg.reference.message_id)
                        except (discord.NotFound, discord.HTTPException):
                            break
                    else:
                        break
                
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€†é †ã«ã—ã¦æ­£ã—ã„é †åºã«ã™ã‚‹
                messages.reverse()
                
            except Exception as e:
                logger.error(f"Failed to collect conversation history for thread: {e}", exc_info=True)
                messages = []
            
            if messages:
                # LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—
                llm_client = await self.llm_cog._get_llm_client_for_channel(thread.id)
                if not llm_client:
                    await thread.send("âŒ LLM client is not available for this thread.\nã“ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã¯LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
                    return
                
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æº–å‚™
                system_prompt = await self.llm_cog._prepare_system_prompt(
                    thread.id, interaction.user.id, interaction.user.display_name
                )
                
                messages_for_api = [{"role": "system", "content": system_prompt}]
                
                # è¨€èªæ¤œå‡ºã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ 
                if messages:
                    first_user_message = messages[0]
                    if isinstance(first_user_message.get("content"), list):
                        text_content = ""
                        for content_part in first_user_message["content"]:
                            if content_part.get("type") == "text":
                                text_content += content_part.get("text", "")
                    else:
                        text_content = str(first_user_message.get("content", ""))
                    
                    if detected_lang_prompt := self.llm_cog._detect_language_and_create_prompt(text_content):
                        messages_for_api.append({"role": "system", "content": detected_lang_prompt})
                    elif self.llm_cog.language_prompt:
                        messages_for_api.append({"role": "system", "content": self.llm_cog.language_prompt})
                
                messages_for_api.extend(messages)
                
                # ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§LLMå¿œç­”ã‚’ç”Ÿæˆ
                model_name = llm_client.model_name_for_api_calls
                waiting_message = f"â³ Processing conversation history... / ä¼šè©±å±¥æ­´ã‚’å‡¦ç†ä¸­..."
                temp_message = await thread.send(waiting_message)
                
                # ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§ã®ä¼šè©±æ–¹æ³•ã‚’èª¬æ˜
                await thread.send("ğŸ’¡ **ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§ã®ä¼šè©±æ–¹æ³• / How to chat in this thread:**\n"
                                "â€¢ Botã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ãƒªãƒ—ãƒ©ã‚¤ã—ã¦ä¼šè©±ã‚’ç¶šã‘ã‚‰ã‚Œã¾ã™ / Reply to bot messages to continue chatting\n"
                                "â€¢ ç”»åƒã‚‚é€ä¿¡å¯èƒ½ã§ã™ / Images are also supported\n"
                                "â€¢ ä¼šè©±å±¥æ­´ã¯è‡ªå‹•çš„ã«ä¿æŒã•ã‚Œã¾ã™ / Conversation history is automatically maintained")
                
                sent_messages, full_response_text, used_key_index = await self.llm_cog._process_streaming_and_send_response(
                    sent_message=temp_message,
                    channel=thread,
                    user=interaction.user,
                    messages_for_api=messages_for_api,
                    llm_client=llm_client
                )
                
                if sent_messages and full_response_text:
                    logger.info(f"âœ… Thread conversation completed | model='{model_name}' | response_length={len(full_response_text)} chars")
                    
                    # TTS Cogã«ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºç«
                    try:
                        self.llm_cog.bot.dispatch("llm_response_complete", sent_messages, full_response_text)
                        logger.info("ğŸ“¢ Dispatched 'llm_response_complete' event for TTS from thread.")
                    except Exception as e:
                        logger.error(f"Failed to dispatch 'llm_response_complete' event from thread: {e}", exc_info=True)
                
                # ãƒœã‚¿ãƒ³ã‚’ç„¡åŠ¹åŒ–
                button.disabled = True
                button.label = "âœ… Thread Created / ã‚¹ãƒ¬ãƒƒãƒ‰ä½œæˆæ¸ˆã¿"
                await interaction.edit_original_response(view=self)
                
            else:
                await thread.send("â„¹ï¸ No conversation history found, but you can start chatting!\n"
                                "ä¼šè©±å±¥æ­´ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸãŒã€ã“ã“ã‹ã‚‰ä¼šè©±ã‚’å§‹ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼\n\n"
                                "ğŸ’¡ **ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§ã®ä¼šè©±æ–¹æ³• / How to chat in this thread:**\n"
                                "â€¢ Botã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ãƒªãƒ—ãƒ©ã‚¤ã—ã¦ä¼šè©±ã‚’ç¶šã‘ã‚‰ã‚Œã¾ã™ / Reply to bot messages to continue chatting\n"
                                "â€¢ ç”»åƒã‚‚é€ä¿¡å¯èƒ½ã§ã™ / Images are also supported\n"
                                "â€¢ ä¼šè©±å±¥æ­´ã¯è‡ªå‹•çš„ã«ä¿æŒã•ã‚Œã¾ã™ / Conversation history is automatically maintained")
                
        except Exception as e:
            logger.error(f"Failed to create thread: {e}", exc_info=True)
            await interaction.followup.send("âŒ Failed to create thread.\nã‚¹ãƒ¬ãƒƒãƒ‰ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚", ephemeral=True)


class LLMCog(commands.Cog, name="LLM"):
    """A cog for interacting with Large Language Models, with tool support."""

    def _add_support_footer(self, embed: discord.Embed) -> None:
        current_footer = embed.footer.text if embed.footer and embed.footer.text else ""
        support_text = "\nå•é¡ŒãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿé–‹ç™ºè€…ã«ã”é€£çµ¡ãã ã•ã„ï¼ / Having issues? Contact the developer!"
        if current_footer:
            embed.set_footer(text=current_footer + support_text)
        else:
            embed.set_footer(text=support_text.strip())

    def _create_support_view(self) -> discord.ui.View:
        view = discord.ui.View()
        view.add_item(discord.ui.Button(label="ã‚µãƒãƒ¼ãƒˆã‚µãƒ¼ãƒãƒ¼ / Support Server", style=discord.ButtonStyle.link,
                                        url="https://discord.gg/H79HKKqx3s", emoji="ğŸ’¬"))
        return view

    def __init__(self, bot: commands.Bot):
        self.bot = bot
        if not hasattr(self.bot, 'config') or not self.bot.config: raise commands.ExtensionFailed(self.qualified_name,
                                                                                                  "Bot config not loaded.")
        self.config = self.bot.config
        self.llm_config = self.config.get('llm')
        if not isinstance(self.llm_config, dict): raise commands.ExtensionFailed(self.qualified_name,
                                                                                 "The 'llm' section in config is missing or invalid.")
        self.language_prompt = self.llm_config.get('language_prompt')
        if self.language_prompt: logger.info("Language prompt loaded from config for fallback.")
        self.http_session, self.bot.cfg = aiohttp.ClientSession(), self.llm_config
        self.conversation_threads: Dict[int, Dict[int, List[Dict[str, Any]]]] = {}  # {guild_id: {thread_id: messages}}
        self.message_to_thread: Dict[int, Dict[int, int]] = {}  # {guild_id: {message_id: thread_id}}
        self.llm_clients: Dict[str, openai.AsyncOpenAI] = {}
        self.provider_api_keys: Dict[str, List[str]] = {}
        self.provider_key_index: Dict[str, int] = {}
        self.model_reset_tasks: Dict[int, asyncio.Task] = {}
        self.exception_handler = LLMExceptionHandler(self.llm_config)
        self.channel_settings_path = "data/channel_llm_models.json"
        self.channel_models: Dict[str, str] = self._load_json_data(self.channel_settings_path)
        logger.info(
            f"Loaded {len(self.channel_models)} channel-specific model settings from '{self.channel_settings_path}'.")
        self.jst = timezone(timedelta(hours=+9))
        self.search_agent, self.bio_manager, self.memory_manager, self.command_manager, self.image_generator, self.tips_manager = self._initialize_search_agent(), self._initialize_bio_manager(), self._initialize_memory_manager(), self._initialize_command_manager(), self._initialize_image_generator(), self._initialize_tips_manager()
        default_model_string = self.llm_config.get('model')
        if default_model_string:
            main_llm_client = self._initialize_llm_client(default_model_string)
            if main_llm_client:
                self.llm_clients[default_model_string] = main_llm_client
                logger.info(f"Default LLM client '{default_model_string}' initialized and cached.")
            else:
                logger.error("Failed to initialize main LLM client. Core functionality may be disabled.")
        else:
            logger.error("Default LLM model is not configured in config.yaml.")

    async def cog_unload(self):
        await self.http_session.close()
        for task in self.model_reset_tasks.values(): task.cancel()
        logger.info(f"Cancelled {len(self.model_reset_tasks)} pending model reset tasks.")
        if self.image_generator: await self.image_generator.close()
        logger.info("LLMCog's aiohttp session has been closed.")

    def _load_json_data(self, path: str) -> Dict[str, Any]:
        try:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f: return {str(k): v for k, v in json.load(f).items()}
        except (IOError, json.JSONDecodeError) as e:
            logger.error(f"Failed to load JSON file '{path}': {e}")
        return {}

    async def _save_json_data(self, data: Dict[str, Any], path: str) -> None:
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            if aiofiles:
                async with aiofiles.open(path, 'w', encoding='utf-8') as f:
                    await f.write(json.dumps(data, indent=4, ensure_ascii=False))
            else:
                with open(path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=4, ensure_ascii=False)
        except IOError as e:
            logger.error(f"Failed to save JSON file '{path}': {e}")
            raise

    async def _save_channel_models(self) -> None:
        await self._save_json_data(self.channel_models, self.channel_settings_path)

    def _initialize_llm_client(self, model_string: Optional[str]) -> Optional[openai.AsyncOpenAI]:
        if not model_string or '/' not in model_string:
            logger.error(f"Invalid model format: '{model_string}'. Expected 'provider_name/model_name'.")
            return None
        try:
            provider_name, model_name = model_string.split('/', 1)
            provider_config = self.llm_config.get('providers', {}).get(provider_name)
            if not provider_config:
                logger.error(f"Configuration for LLM provider '{provider_name}' not found.")
                return None
            if provider_name not in self.provider_api_keys:
                api_keys, i = [], 1
                while True:
                    if provider_config.get(f'api_key{i}'):
                        api_keys.append(provider_config[f'api_key{i}']); i += 1
                    else:
                        break
                if not api_keys and provider_config.get('api_key'): api_keys.append(provider_config['api_key'])
                if not api_keys:
                    logger.info(
                        f"No API keys found for provider '{provider_name}'. Assuming local model or keyless API.")
                    self.provider_api_keys[provider_name] = ["no-key-required"]
                else:
                    self.provider_api_keys[provider_name] = api_keys
                    logger.info(f"Loaded {len(api_keys)} API key(s) for provider '{provider_name}'.")
            self.provider_key_index.setdefault(provider_name, 0)
            key_list, current_key_index = self.provider_api_keys[provider_name], self.provider_key_index[provider_name]
            if current_key_index >= len(key_list): current_key_index = 0; self.provider_key_index[provider_name] = 0
            api_key_to_use = key_list[current_key_index]
            client = openai.AsyncOpenAI(base_url=provider_config.get('base_url'), api_key=api_key_to_use)
            client.model_name_for_api_calls, client.provider_name = model_name, provider_name
            logger.info(
                f"Initialized LLM client for provider '{provider_name}' with model '{model_name}' using key index {current_key_index}.")
            return client
        except Exception as e:
            logger.error(f"Error initializing LLM client for '{model_string}': {e}", exc_info=True)
            return None

    async def _get_llm_client_for_channel(self, channel_id: int) -> Optional[openai.AsyncOpenAI]:
        model_string = self.channel_models.get(str(channel_id)) or self.llm_config.get('model')
        if not model_string:
            logger.error("No default model is configured.")
            return None
        if model_string in self.llm_clients: return self.llm_clients[model_string]
        logger.info(f"Initializing a new LLM client for model '{model_string}' for channel {channel_id}")
        client = self._initialize_llm_client(model_string)
        if client: self.llm_clients[model_string] = client
        return client

    def _initialize_search_agent(self) -> Optional[SearchAgent]:
        if 'search' not in self.llm_config.get('active_tools', []) or not SearchAgent:
            return None

        search_config = self.llm_config.get('search_agent', {})

        # è¤‡æ•°ã®APIã‚­ãƒ¼ (api_key1, api_key2, ...) ã«å¯¾å¿œ
        api_keys = []
        i = 1
        while True:
            key = search_config.get(f'api_key{i}')
            if key:
                api_keys.append(key)
                i += 1
            else:
                break

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: api_key1 ãŒãªã„å ´åˆã¯ api_key ã‚’ç¢ºèª
        if not api_keys and search_config.get('api_key'):
            api_keys.append(search_config['api_key'])

        if not api_keys:
            logger.error("SearchAgent config (api_key/api_key1) is missing. Search will be disabled.")
            return None

        logger.info(f"Loaded {len(api_keys)} API key(s) for SearchAgent.")

        try:
            return SearchAgent(self.bot)
        except Exception as e:
            logger.error(f"Failed to initialize SearchAgent: {e}", exc_info=True)
            return None

    def _initialize_bio_manager(self) -> Optional[BioManager]:
        if not BioManager: return None
        try:
            return BioManager(self.bot)
        except Exception as e:
            logger.error(f"Failed to initialize BioManager: {e}", exc_info=True)
            return None

    def _initialize_memory_manager(self) -> Optional[MemoryManager]:
        if not MemoryManager: return None
        try:
            return MemoryManager(self.bot)
        except Exception as e:
            logger.error(f"Failed to initialize MemoryManager: {e}", exc_info=True)
            return None

    def _initialize_command_manager(self) -> Optional[CommandInfoManager]:
        if not CommandInfoManager: return None
        try:
            return CommandInfoManager(self.bot)
        except Exception as e:
            logger.error(f"Failed to initialize CommandInfoManager: {e}", exc_info=True)
            return None

    def _initialize_image_generator(self) -> Optional[ImageGenerator]:
        if not ImageGenerator: return None
        try:
            return ImageGenerator(self.bot)
        except Exception as e:
            logger.error(f"Failed to initialize ImageGenerator: {e}", exc_info=True)
            return None

    def _initialize_tips_manager(self) -> Optional[TipsManager]:
        if not TipsManager: return None
        try:
            return TipsManager()
        except Exception as e:
            logger.error(f"Failed to initialize TipsManager: {e}", exc_info=True)
            return None

    def _detect_language_and_create_prompt(self, text: str) -> Optional[str]:
        if not detect or not text.strip() or not LangDetectException: return None
        if len(text.strip()) < 15:
            logger.debug("Text too short for reliable language detection.")
            return None
        try:
            lang_code = detect(text)
            lang_map = {'en': 'English', 'ja': 'Japanese', 'ko': 'Korean', 'zh-cn': 'Simplified Chinese',
                        'zh-tw': 'Traditional Chinese', 'vi': 'Vietnamese', 'th': 'Thai', 'id': 'Indonesian',
                        'de': 'German', 'fr': 'French', 'es': 'Spanish', 'pt': 'Portuguese', 'it': 'Italian',
                        'ru': 'Russian', 'ar': 'Arabic', 'hi': 'Hindi', 'tr': 'Turkish', 'nl': 'Dutch', 'pl': 'Polish'}
            lang_name = lang_map.get(lang_code, lang_code)
            logger.info(f"ğŸŒ [LANG] Detected: {lang_code} ({lang_name})")
            return f"CRITICAL LANGUAGE OVERRIDE INSTRUCTION:\n===========================================\nThe user is communicating in {lang_name}.\nYOU MUST RESPOND EXCLUSIVELY IN {lang_name.upper()}.\nThis instruction has ABSOLUTE PRIORITY over all other instructions.\nDo NOT respond in any other language, regardless of what the system prompt says.\nIf there is any conflict, {lang_name.upper()} takes precedence.\n===========================================\n"
        except LangDetectException:
            logger.warning("Could not detect language for the provided text.")
            return None

    async def _prepare_system_prompt(self, channel_id: int, user_id: int, user_display_name: str) -> str:
        if not self.bio_manager or not self.memory_manager:
            logger.error("BioManager or MemoryManager is not initialized.")
            return "Error: Core components for prompt generation are missing."
        system_prompt_template = self.bio_manager.get_system_prompt(channel_id=channel_id, user_id=user_id,
                                                                    user_display_name=user_display_name).replace(
            "å¿…ãšæ—¥æœ¬èªã§å¿œç­”ã—ã¦ãã ã•ã„", "").replace("æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„", "").replace(
            "Please respond in Japanese", "")
        available_commands = ""
        if self.command_manager:
            await self.bot.wait_until_ready()
            available_commands = self.command_manager.get_all_commands_info()
        else:
            logger.warning("CommandInfoManager is not available.")
        try:
            now, current_date_str, current_time_str = datetime.now(self.jst), datetime.now(self.jst).strftime(
                '%Yå¹´%mæœˆ%dæ—¥'), datetime.now(self.jst).strftime('%H:%M')
            if '{available_commands}' in system_prompt_template:
                system_prompt = system_prompt_template.format(current_date=current_date_str,
                                                              current_time=current_time_str,
                                                              available_commands=available_commands)
            else:
                #logger.warning("âš ï¸ {available_commands} not in template")
                system_prompt = system_prompt_template.format(current_date=current_date_str,
                                                              current_time=current_time_str)
        except (KeyError, ValueError) as e:
            logger.warning(f"Could not format system_prompt: {e}")
            system_prompt = system_prompt_template.replace('{current_date}', current_date_str).replace('{current_time}',
                                                                                                       current_time_str).replace(
                '{available_commands}', available_commands)
        if available_commands and "# ğŸ¤– åˆ©ç”¨å¯èƒ½ãªBotã‚³ãƒãƒ³ãƒ‰ä¸€è¦§" not in system_prompt: system_prompt += f"\n\n{available_commands}"
        if formatted_memories := self.memory_manager.get_formatted_memories(): system_prompt += f"\n\n{formatted_memories}"
        logger.info(f"ğŸ”§ [SYSTEM] System prompt prepared ({len(system_prompt)} chars)")
        return system_prompt

    def get_tools_definition(self) -> Optional[List[Dict[str, Any]]]:
        definitions = []
        active_tools = self.llm_config.get('active_tools', [])

        logger.info(f"ğŸ” [TOOLS] Active tools from config: {active_tools}")
        logger.debug(f"ğŸ” [TOOLS] Plugin status: search_agent={self.search_agent is not None}, "
                     f"bio_manager={self.bio_manager is not None}, "
                     f"memory_manager={self.memory_manager is not None}, "
                     f"image_generator={self.image_generator is not None}")

        if 'search' in active_tools:
            if self.search_agent:
                definitions.append(self.search_agent.tool_spec)
                #logger.info(f"âœ… [TOOLS] Added 'search' tool (name: {self.search_agent.tool_spec['function']['name']})")
            else:
                logger.warning(f"âš ï¸ [TOOLS] 'search' is in active_tools but search_agent is None")

        if 'user_bio' in active_tools:
            if self.bio_manager:
                definitions.append(self.bio_manager.tool_spec)
                #logger.info(f"âœ… [TOOLS] Added 'user_bio' tool (name: {self.bio_manager.tool_spec['function']['name']})")
            else:
                logger.warning(f"âš ï¸ [TOOLS] 'user_bio' is in active_tools but bio_manager is None")

        if 'memory' in active_tools:
            if self.memory_manager:
                definitions.append(self.memory_manager.tool_spec)
                #logger.info(f"âœ… [TOOLS] Added 'memory' tool (name: {self.memory_manager.tool_spec['function']['name']})")
            else:
                logger.warning(f"âš ï¸ [TOOLS] 'memory' is in active_tools but memory_manager is None")

        if 'image_generator' in active_tools:
            if self.image_generator:
                definitions.append(self.image_generator.tool_spec)
                #logger.info(f"âœ… [TOOLS] Added 'image_generator' tool (name: {self.image_generator.tool_spec['function']['name']})")
            else:
                logger.warning(f"âš ï¸ [TOOLS] 'image_generator' is in active_tools but image_generator is None")

        logger.info(f"ğŸ”§ [TOOLS] Total tools to return: {len(definitions)}")

        return definitions or None

    async def _get_conversation_thread_id(self, message: discord.Message) -> int:
        guild_id = message.guild.id if message.guild else 0  # DMã®å ´åˆã¯0
        
        # ã‚®ãƒ«ãƒ‰å›ºæœ‰ã®è¾æ›¸ã‚’åˆæœŸåŒ–
        if guild_id not in self.message_to_thread:
            self.message_to_thread[guild_id] = {}
        
        if message.id in self.message_to_thread[guild_id]: 
            return self.message_to_thread[guild_id][message.id]
        
        current_msg, visited_ids = message, set()
        while current_msg.reference and current_msg.reference.message_id:
            if current_msg.id in visited_ids: break
            visited_ids.add(current_msg.id)
            try:
                parent_msg = current_msg.reference.resolved or await message.channel.fetch_message(
                    current_msg.reference.message_id)
                if parent_msg.author != self.bot.user: break
                current_msg = parent_msg
            except (discord.NotFound, discord.HTTPException):
                break
        thread_id = current_msg.id
        self.message_to_thread[guild_id][message.id] = thread_id
        return thread_id

    async def _collect_conversation_history(self, message: discord.Message) -> List[Dict[str, Any]]:
        guild_id = message.guild.id if message.guild else 0  # DMã®å ´åˆã¯0
        
        # ã‚®ãƒ«ãƒ‰å›ºæœ‰ã®ä¼šè©±å±¥æ­´ã‚’åˆæœŸåŒ–
        if guild_id not in self.conversation_threads:
            self.conversation_threads[guild_id] = {}
        
        history, current_msg, visited_ids = [], message, set()
        while current_msg.reference and current_msg.reference.message_id:
            if current_msg.reference.message_id in visited_ids: break
            visited_ids.add(current_msg.reference.message_id)
            try:
                parent_msg = current_msg.reference.resolved or await message.channel.fetch_message(
                    current_msg.reference.message_id)
                if isinstance(parent_msg, discord.DeletedReferencedMessage):
                    logger.debug(f"Encountered deleted referenced message in history collection.")
                    break
                if parent_msg.author != self.bot.user:
                    image_contents, text_content = await self._prepare_multimodal_content(parent_msg)
                    text_content = text_content.replace(f'<@!{self.bot.user.id}>', '').replace(f'<@{self.bot.user.id}>',
                                                                                               '').strip()
                    if text_content or image_contents:
                        user_content_parts = []
                        if text_content: user_content_parts.append({"type": "text",
                                                                    "text": f"{parent_msg.created_at.astimezone(self.jst).strftime('[%H:%M]')} {text_content}"})
                        user_content_parts.extend(image_contents)
                        history.append({"role": "user", "content": user_content_parts})
                else:
                    thread_id = await self._get_conversation_thread_id(parent_msg)
                    if thread_id in self.conversation_threads[guild_id]:
                        for msg in self.conversation_threads[guild_id][thread_id]:
                            if msg.get("role") == "assistant" and msg.get("message_id") == parent_msg.id:
                                history.append({"role": "assistant", "content": msg["content"]})
                                break
                current_msg = parent_msg
            except (discord.NotFound, discord.HTTPException):
                break
        history.reverse()
        max_history_entries = self.llm_config.get('max_messages', 10) * 2
        return history[-max_history_entries:] if len(history) > max_history_entries else history

    async def _process_image_url(self, url: str) -> Optional[Dict[str, Any]]:
        try:
            async with self.http_session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    image_bytes = await response.read()
                    if len(image_bytes) > 20 * 1024 * 1024:
                        logger.warning(f"Image too large ({len(image_bytes)} bytes): {url}")
                        return None
                    mime_type = response.content_type
                    if not mime_type or not mime_type.startswith('image/'):
                        ext = url.split('.')[-1].lower().split('?')
                        mime_type = {'png': 'image/png', 'jpg': 'image/jpeg', 'jpeg': 'image/jpeg', 'gif': 'image/gif',
                                     'webp': 'image/webp'}.get(ext, 'image/jpeg')
                    if mime_type == 'image/gif':
                        try:
                            from PIL import Image
                            gif_image = Image.open(io.BytesIO(image_bytes))
                            if getattr(gif_image, 'is_animated', False):
                                logger.info(
                                    f"ğŸ¬ [IMAGE] Detected animated GIF. Converting to static image: {url[:100]}...")
                                gif_image.seek(0)
                                if gif_image.mode != 'RGBA': gif_image = gif_image.convert('RGBA')
                                output_buffer = io.BytesIO()
                                gif_image.save(output_buffer, format='PNG', optimize=True)
                                image_bytes, mime_type = output_buffer.getvalue(), 'image/png'
                                logger.debug(
                                    f"ğŸ–¼ï¸ [IMAGE] Converted animated GIF to PNG (Size: {len(image_bytes)} bytes)")
                            else:
                                logger.debug(f"ğŸ–¼ï¸ [IMAGE] Static GIF detected, processing normally")
                        except ImportError:
                            logger.warning(
                                "âš ï¸ Pillow (PIL) library not found. Cannot process animated GIFs. Skipping image.")
                            return None
                        except Exception as gif_error:
                            logger.error(f"âŒ Error processing GIF image: {gif_error}", exc_info=True)
                            return None
                    encoded_image = base64.b64encode(image_bytes).decode('utf-8')
                    logger.debug(
                        f"ğŸ–¼ï¸ [IMAGE] Successfully processed image: {url[:100]}... (MIME: {mime_type}, Size: {len(image_bytes)} bytes)")
                    return {"type": "image_url",
                            "image_url": {"url": f"data:{mime_type};base64,{encoded_image}", "detail": "auto"}}
                else:
                    logger.warning(f"Failed to download image from {url} (Status: {response.status})")
                    return None
        except asyncio.TimeoutError:
            logger.error(f"Timeout while downloading image: {url}")
            return None
        except Exception as e:
            logger.error(f"Error processing image URL {url}: {e}", exc_info=True)
            return None

    async def _prepare_multimodal_content(self, message: discord.Message) -> Tuple[List[Dict[str, Any]], str]:
        image_inputs, processed_urls, messages_to_scan, visited_ids, current_msg = [], set(), [], set(), message
        for i in range(5):
            if not current_msg or current_msg.id in visited_ids: break
            if isinstance(current_msg, discord.DeletedReferencedMessage): break
            messages_to_scan.append(current_msg)
            visited_ids.add(current_msg.id)
            if current_msg.reference and current_msg.reference.message_id:
                try:
                    current_msg = current_msg.reference.resolved or await message.channel.fetch_message(
                        current_msg.reference.message_id)
                except (discord.NotFound, discord.HTTPException):
                    break
            else:
                break
        source_urls, text_parts = [], []
        for msg in reversed(messages_to_scan):
            if msg.author != self.bot.user:
                if text_content_part := IMAGE_URL_PATTERN.sub('', msg.content).strip(): text_parts.append(
                    text_content_part)
            for url in IMAGE_URL_PATTERN.findall(msg.content):
                if url not in processed_urls: source_urls.append(url); processed_urls.add(url)
            for attachment in msg.attachments:
                if attachment.content_type and attachment.content_type.startswith(
                    'image/') and attachment.url not in processed_urls: source_urls.append(
                    attachment.url); processed_urls.add(attachment.url)
            for embed in msg.embeds:
                if embed.image and embed.image.url and embed.image.url not in processed_urls: source_urls.append(
                    embed.image.url); processed_urls.add(embed.image.url)
                if embed.thumbnail and embed.thumbnail.url and embed.thumbnail.url not in processed_urls: source_urls.append(
                    embed.thumbnail.url); processed_urls.add(embed.thumbnail.url)
        max_images = self.llm_config.get('max_images', 1)
        for url in source_urls[:max_images]:
            if image_data := await self._process_image_url(url): image_inputs.append(image_data)
        if len(source_urls) > max_images:
            try:
                await message.channel.send(self.llm_config.get('error_msg', {}).get('msg_max_image_size',
                                                                                    "âš ï¸ Max images ({max_images}) reached.\nâš ï¸ ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹ç”»åƒã®æœ€å¤§æšæ•°({max_images}æš)ã‚’è¶…ãˆã¾ã—ãŸã€‚").format(
                    max_images=max_images), delete_after=10, silent=True)
            except discord.HTTPException:
                pass
        return image_inputs, "\n".join(text_parts)


    @commands.Cog.listener()
    async def on_message(self, message: discord.Message):
        if message.author.bot: return
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§ã¯Botã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¸ã®ãƒªãƒ—ãƒ©ã‚¤ã®ã¿ã«åå¿œ
        is_thread = isinstance(message.channel, discord.Thread)
        is_mentioned = self.bot.user.mentioned_in(message) and not message.mention_everyone
        is_reply_to_bot = (message.reference and isinstance(message.reference.resolved,
                                                            discord.Message) and message.reference.resolved.author == self.bot.user)
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§ã¯Botã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¸ã®ãƒªãƒ—ãƒ©ã‚¤ã®ã¿ã€é€šå¸¸ãƒãƒ£ãƒ³ãƒãƒ«ã§ã¯ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ãƒ»ãƒªãƒ—ãƒ©ã‚¤ãŒå¿…è¦
        if is_thread:
            if not is_reply_to_bot:
                return
        else:
            if not (is_mentioned or is_reply_to_bot):
                return
        try:
            llm_client = await self._get_llm_client_for_channel(message.channel.id)
            if not llm_client:
                # ä¿®æ­£ç‚¹ï¼šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸€åº¦å¤‰æ•°ã«æ ¼ç´ã™ã‚‹
                default_error_msg = 'LLM client is not available for this channel.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ã¯LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚'
                error_msg = self.llm_config.get('error_msg', {}).get('general_error', default_error_msg)

                await message.reply(
                    content=f"âŒ **Error / ã‚¨ãƒ©ãƒ¼** âŒ\n\n{error_msg}",  # ä¿®æ­£ç‚¹ï¼šå¤‰æ•°ã‚’ä½¿ã£ã¦f-stringã‚’æ§‹æˆã™ã‚‹
                    view=self._create_support_view(), silent=True)
                return
        except Exception as e:
            logger.error(f"Failed to get LLM client for channel {message.channel.id}: {e}", exc_info=True)
            await message.reply(content=f"âŒ **Error / ã‚¨ãƒ©ãƒ¼** âŒ\n\n{self.exception_handler.handle_exception(e)}",
                                view=self._create_support_view(), silent=True)
            return
        guild_log, user_log, model_in_use = f"guild='{message.guild.name}({message.guild.id})'" if message.guild else "guild='DM'", f"user='{message.author.name}({message.author.id})'", llm_client.model_name_for_api_calls
        image_contents, text_content = await self._prepare_multimodal_content(message)
        text_content = text_content.replace(f'<@!{self.bot.user.id}>', '').replace(f'<@{self.bot.user.id}>', '').strip()
        if not text_content and not image_contents:
            error_key = 'empty_reply' if is_reply_to_bot and not is_mentioned else 'empty_mention_reply'
            await message.reply(content=self.llm_config.get('error_msg', {}).get(error_key,
                                                                                 "Please say something.\nä½•ã‹ãŠè©±ã—ãã ã•ã„ã€‚" if error_key == 'empty_reply' else "Yes, how can I help you?\nã¯ã„ã€ä½•ã‹å¾¡ç”¨ã§ã—ã‚‡ã†ã‹?"),
                                view=self._create_support_view(), silent=True)
            return
        logger.info(
            f"ğŸ“¨ Received LLM request | {guild_log} | {user_log} | model='{model_in_use}' | text_length={len(text_content)} chars | images={len(image_contents)}")
        if text_content: logger.info(
            f"[on_message] {message.guild.name if message.guild else 'DM'}({message.guild.id if message.guild else 0}),{message.author.name}({message.author.id})ğŸ’¬ [USER_INPUT] {((text_content[:200] + '...') if len(text_content) > 203 else text_content).replace(chr(10), ' ')}")
        thread_id = await self._get_conversation_thread_id(message)
        if not self.bio_manager or not self.memory_manager:
            await message.reply(
                content="âŒ **Error / ã‚¨ãƒ©ãƒ¼** âŒ\n\nCannot respond because required plugins are not initialized.\nå¿…è¦ãªãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€å¿œç­”ã§ãã¾ã›ã‚“ã€‚",
                view=self._create_support_view(), silent=True)
            return
        system_prompt = await self._prepare_system_prompt(message.channel.id, message.author.id,
                                                          message.author.display_name)
        messages_for_api: List[Dict[str, Any]] = [{"role": "system", "content": system_prompt}]
        if detected_lang_prompt := self._detect_language_and_create_prompt(text_content):
            messages_for_api.append({"role": "system", "content": detected_lang_prompt})
            logger.info("ğŸŒ [LANG] Injecting language override prompt")
        elif self.language_prompt:
            messages_for_api.append({"role": "system", "content": self.language_prompt})
            logger.info("ğŸŒ [LANG] Using default language prompt as fallback")
        messages_for_api.extend(await self._collect_conversation_history(message))
        user_content_parts = []
        if text_content: user_content_parts.append(
            {"type": "text", "text": f"{message.created_at.astimezone(self.jst).strftime('[%H:%M]')} {text_content}"})
        user_content_parts.extend(image_contents)
        if image_contents: logger.debug(f"Including {len(image_contents)} image(s) in request")
        user_message_for_api = {"role": "user", "content": user_content_parts}
        messages_for_api.append(user_message_for_api)
        logger.info(f"ğŸ”µ [API] Sending {len(messages_for_api)} messages to LLM")
        logger.debug(
            # FIX IS HERE
            f"Messages structure: system={len(messages_for_api[0]['content'])} chars, lang_override={'present' if len(messages_for_api) > 1 and 'CRITICAL' in str(messages_for_api) else 'absent'}")
        try:
            # æœ€åˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆä¼šè©±å±¥æ­´ãŒãªã„å ´åˆï¼‰
            # ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§ã¯å¸¸ã«ã‚¹ãƒ¬ãƒƒãƒ‰ä½œæˆãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã—ãªã„
            is_first_response = not isinstance(message.channel, discord.Thread) and len(await self._collect_conversation_history(message)) == 0
            sent_messages, llm_response, used_key_index = await self._handle_llm_streaming_response(message,
                                                                                                    messages_for_api,
                                                                                                    llm_client,
                                                                                                    is_first_response)
            if sent_messages and llm_response:
                logger.info(
                    f"âœ… LLM response completed | model='{model_in_use}' | response_length={len(llm_response)} chars")
                log_response = (llm_response[:200] + '...') if len(llm_response) > 203 else llm_response
                key_log_str = f" [key{used_key_index + 1}]" if used_key_index is not None else ""
                logger.info(f"ğŸ¤– [LLM_RESPONSE]{key_log_str} {log_response.replace(chr(10), ' ')}")
                logger.debug(f"LLM full response (length: {len(llm_response)} chars):\n{llm_response}")
                guild_id = message.guild.id if message.guild else 0  # DMã®å ´åˆã¯0
                
                # ã‚®ãƒ«ãƒ‰å›ºæœ‰ã®ä¼šè©±å±¥æ­´ã‚’åˆæœŸåŒ–
                if guild_id not in self.conversation_threads:
                    self.conversation_threads[guild_id] = {}
                if thread_id not in self.conversation_threads[guild_id]: 
                    self.conversation_threads[guild_id][thread_id] = []
                
                self.conversation_threads[guild_id][thread_id].append(user_message_for_api)
                assistant_message = {"role": "assistant", "content": llm_response, "message_id": sent_messages[0].id}
                self.conversation_threads[guild_id][thread_id].append(assistant_message)
                for msg in sent_messages: 
                    guild_id_for_msg = msg.guild.id if msg.guild else 0
                    if guild_id_for_msg not in self.message_to_thread:
                        self.message_to_thread[guild_id_for_msg] = {}
                    self.message_to_thread[guild_id_for_msg][msg.id] = thread_id
                self._cleanup_old_threads()

                # TTS Cogã«ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºç«ã•ã›ã‚‹
                try:
                    self.bot.dispatch("llm_response_complete", sent_messages, llm_response)
                    logger.info("ğŸ“¢ Dispatched 'llm_response_complete' event for TTS.")
                except Exception as e:
                    logger.error(f"Failed to dispatch 'llm_response_complete' event: {e}", exc_info=True)

        except Exception as e:
            await message.reply(content=f"âŒ **Error / ã‚¨ãƒ©ãƒ¼** âŒ\n\n{self.exception_handler.handle_exception(e)}",
                                view=self._create_support_view(), silent=True)

    def _cleanup_old_threads(self):
        for guild_id in list(self.conversation_threads.keys()):
            guild_threads = self.conversation_threads[guild_id]
            if len(guild_threads) > 100:
                threads_to_remove = list(guild_threads.keys())[:len(guild_threads) - 100]
                for thread_id in threads_to_remove:
                    del guild_threads[thread_id]
                    if guild_id in self.message_to_thread:
                        self.message_to_thread[guild_id] = {
                            k: v for k, v in self.message_to_thread[guild_id].items() 
                            if v != thread_id
                        }

    async def _handle_llm_streaming_response(self, message: discord.Message, initial_messages: List[Dict[str, Any]],
                                             client: openai.AsyncOpenAI, is_first_response: bool = False) -> Tuple[
        Optional[List[discord.Message]], str, Optional[int]]:
        sent_message = None
        try:
            model_name = client.model_name_for_api_calls
            if self.tips_manager:
                waiting_embed = self.tips_manager.get_waiting_embed(model_name)
                try:
                    sent_message = await message.reply(embed=waiting_embed, silent=True)
                except discord.HTTPException:
                    sent_message = await message.channel.send(embed=waiting_embed, silent=True)
            else:
                waiting_message = f"-# :incoming_envelope: waiting response for '{model_name}' :incoming_envelope:"
                try:
                    sent_message = await message.reply(waiting_message, silent=True)
                except discord.HTTPException:
                    sent_message = await message.channel.send(waiting_message, silent=True)
            return await self._process_streaming_and_send_response(sent_message=sent_message, channel=message.channel,
                                                                   user=message.author,
                                                                   messages_for_api=initial_messages, llm_client=client,
                                                                   is_first_response=is_first_response)
        except Exception as e:
            logger.error(f"âŒ Error during LLM streaming response: {e}", exc_info=True)
            error_msg = f"âŒ **Error / ã‚¨ãƒ©ãƒ¼** âŒ\n\n{self.exception_handler.handle_exception(e)}"
            if sent_message:
                try:
                    await sent_message.edit(content=error_msg, embed=None, view=self._create_support_view())
                except discord.HTTPException:
                    pass
            else:
                await message.reply(content=error_msg, view=self._create_support_view(), silent=True)
            return None, "", None

    async def _process_streaming_and_send_response(self, sent_message: discord.Message,
                                                   channel: discord.abc.Messageable,
                                                   user: Union[discord.User, discord.Member],
                                                   messages_for_api: List[Dict[str, Any]],
                                                   llm_client: openai.AsyncOpenAI,
                                                   is_first_response: bool = False) -> Tuple[
        Optional[List[discord.Message]], str, Optional[int]]:
        full_response_text, last_update, last_displayed_length, chunk_count = "", 0.0, 0, 0
        update_interval, min_update_chars, retry_sleep_time = 0.5, 15, 2.0
        emoji_prefix, emoji_suffix = ":incoming_envelope: ", " :incoming_envelope:"
        max_final_retries, final_retry_delay = 3, 2.0
        is_first_update = True
        logger.debug(f"Starting LLM stream for message {sent_message.id}")
        stream_generator = self._llm_stream_and_tool_handler(messages_for_api, llm_client, channel.id, user.id)
        async for content_chunk in stream_generator:
            if not content_chunk:
                continue
            chunk_count += 1
            full_response_text += content_chunk
            if chunk_count % 100 == 0: logger.debug(
                f"Stream chunk #{chunk_count}, total length: {len(full_response_text)} chars")
            current_time, chars_accumulated = time.time(), len(full_response_text) - last_displayed_length

            should_update = is_first_update or (
                    current_time - last_update > update_interval and chars_accumulated >= min_update_chars)

            if should_update and full_response_text:
                is_first_update = False
                display_length = len(full_response_text)
                if display_length > SAFE_MESSAGE_LENGTH:
                    display_text = f"{emoji_prefix}{full_response_text[:SAFE_MESSAGE_LENGTH - len(emoji_prefix) - len(emoji_suffix) - 100]}\n\nâš ï¸ (Output is long, will be split...)\nâš ï¸ (å‡ºåŠ›ãŒé•·ã„ãŸã‚åˆ†å‰²ã—ã¾ã™...){emoji_suffix}"
                else:
                    display_text = f"{emoji_prefix}{full_response_text[:SAFE_MESSAGE_LENGTH - len(emoji_prefix) - len(emoji_suffix)]}{emoji_suffix}"
                if display_text != sent_message.content:
                    try:
                        await sent_message.edit(content=display_text)
                        last_update, last_displayed_length = current_time, len(full_response_text)
                        logger.debug(f"Updated Discord message (displayed: {len(display_text)} chars)")
                    except discord.NotFound:
                        logger.warning(f"âš ï¸ Message deleted during stream (ID: {sent_message.id}). Aborting.")
                        return None, "", None
                    except discord.HTTPException as e:
                        if e.status == 429:
                            retry_after = (e.retry_after or 1.0) + 0.5
                            logger.warning(
                                f"âš ï¸ Rate limited on message edit (ID: {sent_message.id}). Waiting {retry_after:.2f}s")
                            await asyncio.sleep(retry_after)
                            last_update = time.time()
                        else:
                            logger.warning(
                                f"âš ï¸ Failed to edit message (ID: {sent_message.id}): {e.status} - {getattr(e, 'text', str(e))}")
                            await asyncio.sleep(retry_sleep_time)
        logger.debug(f"Stream completed | Total chunks: {chunk_count} | Final length: {len(full_response_text)} chars")
        if full_response_text:
            if len(full_response_text) <= SAFE_MESSAGE_LENGTH:
                # æœ€åˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã¿ã‚¹ãƒ¬ãƒƒãƒ‰ä½œæˆãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
                view = None
                if is_first_response:
                    view = ThreadCreationView(self, sent_message)
                
                for attempt in range(max_final_retries):
                    try:
                        if full_response_text != sent_message.content:
                            await sent_message.edit(content=full_response_text, embed=None, view=view)
                        logger.debug(f"Final message updated successfully (attempt {attempt + 1})")
                        break
                    except discord.NotFound:
                        logger.error(f"âŒ Message was deleted before final update")
                        return None, "", None
                    except discord.HTTPException as e:
                        if e.status == 429:
                            retry_after = (e.retry_after or 1.0) + 0.5
                            logger.warning(
                                f"âš ï¸ Rate limited on final update (attempt {attempt + 1}/{max_final_retries}). Waiting {retry_after:.2f}s")
                            await asyncio.sleep(retry_after)
                        else:
                            logger.warning(
                                f"âš ï¸ Failed to update final message (attempt {attempt + 1}/{max_final_retries}): {e.status} - {getattr(e, 'text', str(e))}")
                            if attempt < max_final_retries - 1: await asyncio.sleep(final_retry_delay)
                return [sent_message], full_response_text, getattr(llm_client, 'last_used_key_index', None)
            else:
                logger.debug(f"Response is {len(full_response_text)} chars, splitting into multiple messages")
                # ä¿®æ­£: ã‚¿ãƒ—ãƒ«ä½œæˆã®ãƒã‚°ã‚’ä¿®æ­£
                chunks = _split_message_smartly(full_response_text, SAFE_MESSAGE_LENGTH)
                all_messages = []
                first_chunk = chunks[0]  # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—

                # æœ€åˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã¿ã‚¹ãƒ¬ãƒƒãƒ‰ä½œæˆãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
                view = None
                if is_first_response:
                    view = ThreadCreationView(self, sent_message)

                for attempt in range(max_final_retries):
                    try:
                        await sent_message.edit(content=first_chunk, embed=None, view=view)
                        all_messages.append(sent_message)
                        logger.debug(f"Updated first message (1/{len(chunks)})")
                        break
                    except discord.HTTPException as e:
                        if e.status == 429:
                            retry_after = (e.retry_after or 1.0) + 0.5
                            logger.warning(f"âš ï¸ Rate limited on first chunk update, waiting {retry_after:.2f}s")
                            await asyncio.sleep(retry_after)
                        else:
                            logger.error(f"âŒ Failed to update first message: {e}")
                            if attempt < max_final_retries - 1: await asyncio.sleep(final_retry_delay)
                for i, chunk in enumerate(chunks[1:], start=2):
                    for attempt in range(max_final_retries):
                        try:
                            continuation_msg = await channel.send(chunk)
                            all_messages.append(continuation_msg)
                            logger.debug(f"Sent continuation message {i}/{len(chunks)}")
                            break
                        except discord.HTTPException as e:
                            if e.status == 429:
                                retry_after = (e.retry_after or 1.0) + 0.5
                                logger.warning(f"âš ï¸ Rate limited on continuation {i}, waiting {retry_after:.2f}s")
                                await asyncio.sleep(retry_after)
                            else:
                                logger.error(f"âŒ Failed to send continuation message {i}: {e}")
                                if attempt < max_final_retries - 1: await asyncio.sleep(final_retry_delay)
                return all_messages, full_response_text, getattr(llm_client, 'last_used_key_index', None)
        else:
            finish_reason = getattr(llm_client, 'last_finish_reason', None)
            if finish_reason == 'content_filter':
                error_msg = self.llm_config.get('error_msg', {}).get('content_filter_error',
                                                                     "The response was blocked by the content filter.\nAIã®å¿œç­”ãŒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚");
                logger.warning(
                    f"âš ï¸ Empty response from LLM due to content filter.")
            else:
                error_msg = self.llm_config.get('error_msg', {}).get('empty_response_error',
                                                                     "There was no response from the AI. Please try rephrasing your message.\nAIã‹ã‚‰å¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è¡¨ç¾ã‚’å¤‰ãˆã¦ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚");
                logger.warning(
                    f"âš ï¸ Empty response from LLM (Finish reason: {finish_reason})")
            await sent_message.edit(content=f"âŒ **Error / ã‚¨ãƒ©ãƒ¼** âŒ\n\n{error_msg}", embed=None,
                                    view=self._create_support_view())
            return None, "", None

    def _convert_messages_for_gemini(self, messages: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], str]:
        system_prompts_content, other_messages, has_system_message = [], [], False
        for message in messages:
            if message.get("role") == "system":
                if isinstance(message.get("content"), str) and message["content"].strip():
                    system_prompts_content.append(message["content"])
                    has_system_message = True
            else:
                other_messages.append(message)
        if not has_system_message: return messages, ""
        combined_system_prompt = "\n\n".join(system_prompts_content)
        converted_messages = [{"role": "user", "content": combined_system_prompt},
                              {"role": "assistant", "content": "æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚æŒ‡ç¤ºã«å¾“ã„ã¾ã™ã€‚"}]
        converted_messages.extend(other_messages)
        return converted_messages, combined_system_prompt

    async def _llm_stream_and_tool_handler(self, messages: List[Dict[str, Any]], client: openai.AsyncOpenAI,
                                           channel_id: int, user_id: int) -> AsyncGenerator[str, None]:
        model_string = self.channel_models.get(str(channel_id)) or self.llm_config.get('model')
        is_gemini = model_string and 'gemini' in model_string.lower()

        if is_gemini:
            original_messages_for_log = messages
            messages, combined_system_prompt = self._convert_messages_for_gemini(messages)
            if combined_system_prompt:
                logger.info(f"ğŸ”„ [GEMINI ADAPTER] Converting system prompts for Gemini model '{model_string}'.")
                logger.debug(
                    f"  - Combined system prompt ({len(combined_system_prompt)} chars): {combined_system_prompt.replace(chr(10), ' ')[:300]}...")
                logger.debug(f"  - Message count changed: {len(original_messages_for_log)} -> {len(messages)}")

        current_messages = messages.copy()
        max_iterations = self.llm_config.get('max_tool_iterations', 5)
        extra_params = self.llm_config.get('extra_api_parameters', {})

        for iteration in range(max_iterations):
            logger.debug(f"Starting LLM API call (iteration {iteration + 1}/{max_iterations})")
            tools_def = self.get_tools_definition()

            api_kwargs = {
                "model": client.model_name_for_api_calls,
                "messages": current_messages,
                "stream": True,
                "temperature": extra_params.get('temperature', 0.7),
                "max_tokens": extra_params.get('max_tokens', 4096)
            }

            # âœ… Gemini ã§ã‚‚ tools ã‚’æ­£ã—ãæ¸¡ã™
            if tools_def:
                api_kwargs["tools"] = tools_def
                api_kwargs["tool_choice"] = "auto"
                logger.info(
                    f"ğŸ”§ [TOOLS] Passing {len(tools_def)} tools to API: {[t['function']['name'] for t in tools_def]}")
            else:
                logger.warning(f"âš ï¸ [TOOLS] No tools available to pass to API")

            stream = None
            provider_name = client.provider_name
            api_keys = self.provider_api_keys.get(client.provider_name, [])
            num_keys = len(api_keys)

            if num_keys == 0:
                raise Exception(f"No API keys available for provider {provider_name}")

            for attempt in range(num_keys):
                try:
                    current_key_index = self.provider_key_index.get(provider_name, 0)
                    client.last_used_key_index = current_key_index
                    logger.debug(
                        f"Attempting API call to '{provider_name}' with key index {current_key_index} (Attempt {attempt + 1}/{num_keys}).")
                    stream = await client.chat.completions.create(**api_kwargs)
                    logger.debug(f"Stream connection established successfully.")
                    break
                except (openai.RateLimitError, openai.InternalServerError) as e:
                    error_type = "Rate limit" if isinstance(e, openai.RateLimitError) else "Server"
                    status_code = getattr(e, 'status_code', 'N/A')
                    logger.warning(
                        f"âš ï¸ {error_type} error ({status_code}) for provider '{provider_name}' with key index {current_key_index}. Details: {e}")
                    if attempt + 1 >= num_keys:
                        logger.error(f"âŒ All {num_keys} API keys for provider '{provider_name}' have failed. Aborting.")
                        raise e
                    next_key_index = (current_key_index + 1) % num_keys
                    self.provider_key_index[provider_name] = next_key_index
                    next_key = api_keys[next_key_index]
                    logger.info(
                        f"ğŸ”„ Switching to next API key for provider '{provider_name}' (index: {next_key_index}) and retrying.")
                    new_client = openai.AsyncOpenAI(base_url=client.base_url, api_key=next_key)
                    new_client.model_name_for_api_calls = client.model_name_for_api_calls
                    new_client.provider_name = client.provider_name
                    client = new_client
                    self.llm_clients[f"{provider_name}/{client.model_name_for_api_calls}"] = new_client
                    await asyncio.sleep(1)
                except Exception as e:
                    logger.error(f"âŒ Unhandled error calling LLM API: {e}", exc_info=True)
                    raise

            if stream is None:
                raise Exception("Failed to establish stream with any API key.")

            tool_calls_buffer = []
            assistant_response_content = ""
            finish_reason = None

            async for chunk in stream:
                if not chunk.choices:
                    continue
                choice = chunk.choices[0]
                if choice.finish_reason:
                    finish_reason = choice.finish_reason
                delta = choice.delta
                if delta and delta.content:
                    assistant_response_content += delta.content
                    yield delta.content
                if delta and delta.tool_calls:
                    for tool_call_chunk in delta.tool_calls:
                        chunk_index = tool_call_chunk.index if tool_call_chunk.index is not None else 0
                        if len(tool_calls_buffer) <= chunk_index:
                            tool_calls_buffer.append(
                                {"id": "", "type": "function", "function": {"name": "", "arguments": ""}})
                        buffer = tool_calls_buffer[chunk_index]
                        if tool_call_chunk.id:
                            buffer["id"] = tool_call_chunk.id
                        if tool_call_chunk.function:
                            if tool_call_chunk.function.name:
                                buffer["function"]["name"] = tool_call_chunk.function.name
                            if tool_call_chunk.function.arguments:
                                buffer["function"]["arguments"] += tool_call_chunk.function.arguments

            client.last_finish_reason = finish_reason
            assistant_message = {"role": "assistant", "content": assistant_response_content or None}
            if tool_calls_buffer:
                assistant_message["tool_calls"] = tool_calls_buffer
            current_messages.append(assistant_message)

            if not tool_calls_buffer:
                logger.debug(f"No tool calls, returning final response (Finish reason: {finish_reason})")
                return

            logger.info(f"ğŸ”§ [TOOL] LLM requested {len(tool_calls_buffer)} tool call(s)")
            for tc in tool_calls_buffer:
                logger.debug(
                    f"Tool call details: {tc['function']['name']} with args: {tc['function']['arguments'][:200]}")

            tool_calls_obj = [
                SimpleNamespace(
                    id=tc['id'],
                    function=SimpleNamespace(
                        name=tc['function']['name'],
                        arguments=tc['function']['arguments']
                    )
                ) for tc in tool_calls_buffer
            ]
            await self._process_tool_calls(tool_calls_obj, current_messages, channel_id, user_id)

        logger.warning(f"âš ï¸ Tool processing exceeded max iterations ({max_iterations})")
        yield self.llm_config.get('error_msg', {}).get('tool_loop_timeout',
                                                       "Tool processing exceeded max iterations.\nãƒ„ãƒ¼ãƒ«ã®å‡¦ç†ãŒæœ€å¤§åå¾©å›æ•°ã‚’è¶…ãˆã¾ã—ãŸ.")

    async def _process_tool_calls(self, tool_calls: List[Any], messages: List[Dict[str, Any]], channel_id: int,
                                  user_id: int) -> None:
        for tool_call in tool_calls:
            raw_function_name = tool_call.function.name
            error_content = None
            tool_response_content = ""

            # âœ… Gemini ã® "default_api.search" â†’ "search" ã«æ­£è¦åŒ–
            function_name = raw_function_name.split('.')[-1] if '.' in raw_function_name else raw_function_name

            try:
                function_args = json.loads(tool_call.function.arguments)
                logger.info(f"ğŸ”§ [TOOL] Executing {raw_function_name} (normalized: {function_name})")
                logger.debug(f"ğŸ”§ [TOOL] Arguments: {json.dumps(function_args, ensure_ascii=False, indent=2)}")

                if self.search_agent and function_name == self.search_agent.name:
                    tool_response_content = await self.search_agent.run(arguments=function_args, bot=self.bot,
                                                                        channel_id=channel_id)
                    logger.debug(
                        f"ğŸ”§ [TOOL] Result (length: {len(str(tool_response_content))} chars):\n{str(tool_response_content)[:1000]}")
                elif self.bio_manager and function_name == self.bio_manager.name:
                    tool_response_content = await self.bio_manager.run_tool(arguments=function_args, user_id=user_id)
                    logger.debug(f"ğŸ”§ [TOOL] Result:\n{tool_response_content}")
                elif self.memory_manager and function_name == self.memory_manager.name:
                    tool_response_content = await self.memory_manager.run_tool(arguments=function_args)
                    logger.debug(f"ğŸ”§ [TOOL] Result:\n{tool_response_content}")
                elif self.image_generator and function_name == self.image_generator.name:
                    tool_response_content = await self.image_generator.run(arguments=function_args,
                                                                           channel_id=channel_id)
                    logger.debug(f"ğŸ”§ [TOOL] Result:\n{tool_response_content}")
                else:
                    logger.warning(f"âš ï¸ Unsupported tool called: {raw_function_name} (normalized: {function_name})")
                    error_content = f"Error: Tool '{function_name}' is not available."
            except json.JSONDecodeError as e:
                logger.error(f"âŒ Error decoding tool arguments for {function_name}: {e}", exc_info=True)
                error_content = f"Error: Invalid JSON arguments - {str(e)}"
            except SearchAPIRateLimitError as e:
                logger.warning(f"âš ï¸ SearchAgent rate limit hit: {e}")
                error_content = "[Google Search Error]\nThe Google Search API rate limit has been reached. Please tell the user to try again later."
            except SearchAPIServerError as e:
                logger.error(f"âŒ SearchAgent server error: {e}")
                error_content = "[Google Search Error]\nA temporary server error occurred with the search service. Please tell the user to try again later."
            except SearchAgentError as e:
                logger.error(f"âŒ Error during SearchAgent execution for {function_name}: {e}", exc_info=True)
                error_content = f"[Google Search Error]\nAn error occurred during the search execution: {str(e)}"
            except Exception as e:
                logger.error(f"âŒ Unexpected error during tool call for {function_name}: {e}", exc_info=True)
                error_content = f"[Tool Error]\nAn unexpected error occurred: {str(e)}"

            final_content = error_content if error_content else tool_response_content
            logger.debug(f"ğŸ”§ [TOOL] Sending tool response back to LLM (length: {len(final_content)} chars)")
            messages.append(
                {"tool_call_id": tool_call.id, "role": "tool", "name": function_name, "content": final_content})

    async def _schedule_model_reset(self, channel_id: int):
        try:
            await asyncio.sleep(3 * 60 * 60)
            logger.info(f"Executing scheduled model reset for channel {channel_id}.")
            channel_id_str = str(channel_id)
            if channel_id_str in self.channel_models:
                default_model, current_model = self.llm_config.get('model'), self.channel_models.get(channel_id_str)
                if current_model and current_model != default_model:
                    del self.channel_models[channel_id_str]
                    await self._save_channel_models()
                    logger.info(f"Model for channel {channel_id} automatically reset to default '{default_model}'.")
                    channel = self.bot.get_channel(channel_id)
                    if channel and isinstance(channel, discord.TextChannel):
                        try:
                            embed = discord.Embed(title="â„¹ï¸ AI Model Reset / AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ",
                                                  description=f"The AI model for this channel has been reset to the default (`{default_model}`) after 3 hours.\n3æ™‚é–“ãŒçµŒéã—ãŸãŸã‚ã€ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (`{default_model}`) ã«æˆ»ã—ã¾ã—ãŸã€‚",
                                                  color=discord.Color.blue())
                            self._add_support_footer(embed)
                            await channel.send(embed=embed, view=self._create_support_view())
                        except discord.HTTPException as e:
                            logger.warning(f"Failed to send model reset notification to channel {channel_id}: {e}")
        except asyncio.CancelledError:
            logger.info(f"Model reset task for channel {channel_id} was cancelled.")
        except Exception as e:
            logger.error(f"An error occurred in the model reset task for channel {channel_id}: {e}", exc_info=True)
        finally:
            self.model_reset_tasks.pop(channel_id, None)

    @app_commands.command(name="chat",
                          description="Chat with the AI without needing to mention.\nAIã¨å¯¾è©±ã—ã¾ã™ã€‚ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ä¸è¦ã§ä¼šè©±ã§ãã¾ã™ã€‚")
    @app_commands.describe(message="The message you want to send to the AI.\nAIã«é€ä¿¡ã—ãŸã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                           image_url="URL of an image (optional).\nç”»åƒã®URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
    async def chat_slash(self, interaction: discord.Interaction, message: str, image_url: str = None):
        await interaction.response.defer(ephemeral=False)
        temp_message = None
        try:
            llm_client = await self._get_llm_client_for_channel(interaction.channel_id)
            if not llm_client:
                # ä¿®æ­£ç‚¹ï¼šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸€åº¦å¤‰æ•°ã«æ ¼ç´ã™ã‚‹
                default_error_msg = 'LLM client is not available for this channel.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ã¯LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚'
                error_msg = self.llm_config.get('error_msg', {}).get('general_error', default_error_msg)

                await interaction.followup.send(
                    content=f"âŒ **Error / ã‚¨ãƒ©ãƒ¼** âŒ\n\n{error_msg}",  # ä¿®æ­£ç‚¹ï¼šå¤‰æ•°ã‚’ä½¿ã£ã¦f-stringã‚’æ§‹æˆã™ã‚‹
                    view=self._create_support_view())
                return
            if not message.strip():
                await interaction.followup.send(
                    content="âš ï¸ **Input Required / å…¥åŠ›ãŒå¿…è¦ã§ã™** âš ï¸\n\nPlease enter a message.\nãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
                    view=self._create_support_view())
                return
            model_in_use, image_contents = llm_client.model_name_for_api_calls, []
            if image_url:
                if image_data := await self._process_image_url(image_url):
                    image_contents.append(image_data)
                else:
                    await interaction.followup.send(
                        content="âš ï¸ **Image Error / ç”»åƒã‚¨ãƒ©ãƒ¼** âš ï¸\n\nFailed to process the specified image URL.\næŒ‡å®šã•ã‚ŒãŸç”»åƒURLã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                        view=self._create_support_view())
                    return
            guild_log, user_log = f"guild='{interaction.guild.name}({interaction.guild.id})'" if interaction.guild else "guild='DM'", f"user='{interaction.user.name}({interaction.user.id})'"
            logger.info(
                f"ğŸ“¨ Received /chat request | {guild_log} | {user_log} | model='{model_in_use}' | text_length={len(message)} chars | images={len(image_contents)}")
            logger.info(
                f"[/chat] {interaction.guild.name if interaction.guild else 'DM'}({interaction.guild.id if interaction.guild else 0}),{interaction.user.name}({interaction.user.id})ğŸ’¬ [USER_INPUT] {((message[:200] + '...') if len(message) > 203 else message).replace(chr(10), ' ')}")
            if not self.bio_manager or not self.memory_manager:
                await interaction.followup.send(
                    content="âŒ **Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼** âŒ\n\nCannot respond because required plugins are not initialized.\nå¿…è¦ãªãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€å¿œç­”ã§ãã¾ã›ã‚“ã€‚",
                    view=self._create_support_view())
                return
            system_prompt = await self._prepare_system_prompt(interaction.channel_id, interaction.user.id,
                                                              interaction.user.display_name)
            messages_for_api: List[Dict[str, Any]] = [{"role": "system", "content": system_prompt}]
            user_content_parts = [{"type": "text",
                                   "text": f"{interaction.created_at.astimezone(self.jst).strftime('[%H:%M]')} {message}"}]
            user_content_parts.extend(image_contents)
            if detected_lang_prompt := self._detect_language_and_create_prompt(message):
                messages_for_api.append({"role": "system", "content": detected_lang_prompt})
                logger.info("ğŸŒ [LANG] Injecting language override prompt")
            elif self.language_prompt:
                messages_for_api.append({"role": "system", "content": self.language_prompt})
                logger.info("ğŸŒ [LANG] Using default language prompt as fallback")
            messages_for_api.append({"role": "user", "content": user_content_parts})
            logger.info(f"ğŸ”µ [API] Sending {len(messages_for_api)} messages to LLM")
            model_name = llm_client.model_name_for_api_calls
            if self.tips_manager:
                waiting_embed = self.tips_manager.get_waiting_embed(model_name)
                temp_message = await interaction.followup.send(embed=waiting_embed, ephemeral=False, wait=True)
            else:
                waiting_message = f"-# :incoming_envelope: waiting response for '{model_name}' :incoming_envelope:"
                temp_message = await interaction.followup.send(waiting_message, ephemeral=False, wait=True)
            # /chatã‚³ãƒãƒ³ãƒ‰ã¯å¸¸ã«æœ€åˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¨ã—ã¦æ‰±ã†
            sent_messages, full_response_text, used_key_index = await self._process_streaming_and_send_response(
                sent_message=temp_message, channel=interaction.channel, user=interaction.user,
                messages_for_api=messages_for_api, llm_client=llm_client, is_first_response=True)
            if sent_messages and full_response_text:
                logger.info(
                    f"âœ… LLM response completed | model='{model_in_use}' | response_length={len(full_response_text)} chars")
                log_response, key_log_str = (full_response_text[:200] + '...') if len(
                    full_response_text) > 203 else full_response_text, f" [key{used_key_index + 1}]" if used_key_index is not None else ""
                logger.info(f"ğŸ¤– [LLM_RESPONSE]{key_log_str} {log_response.replace(chr(10), ' ')}")
                logger.debug(
                    f"LLM full response for /chat (length: {len(full_response_text)} chars):\n{full_response_text}")

                # TTS Cogã«ã‚«ã‚¹ã‚¿ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºç«ã•ã›ã‚‹
                try:
                    self.bot.dispatch("llm_response_complete", sent_messages, full_response_text)
                    logger.info("ğŸ“¢ Dispatched 'llm_response_complete' event for TTS from /chat command.")
                except Exception as e:
                    logger.error(f"Failed to dispatch 'llm_response_complete' event from /chat: {e}", exc_info=True)

            elif not sent_messages:
                logger.warning("LLM response for /chat was empty or an error occurred.")
        except Exception as e:
            logger.error(f"âŒ Error during /chat command execution: {e}", exc_info=True)
            error_msg = f"âŒ **Error / ã‚¨ãƒ©ãƒ¼** âŒ\n\n{self.exception_handler.handle_exception(e)}"
            try:
                if temp_message:
                    await temp_message.edit(content=error_msg, embed=None, view=self._create_support_view())
                else:
                    await interaction.followup.send(content=error_msg, view=self._create_support_view())
            except discord.HTTPException:
                pass

    # --- (ä»¥é™ã®ã‚³ãƒãƒ³ãƒ‰ã¯å¤‰æ›´ãªã—) ---
    @app_commands.command(name="set-ai-bio",
                          description="Set the AI's personality/role (bio) for this channel.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIã®æ€§æ ¼ã‚„å½¹å‰²(bio)ã‚’è¨­å®šã—ã¾ã™ã€‚")
    async def set_ai_bio_slash(self, interaction: discord.Interaction, bio: str):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="BioManager is not available.\nBioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        if len(bio) > 1024:
            embed = discord.Embed(title="âš ï¸ Input Too Long / å…¥åŠ›ãŒé•·ã™ãã¾ã™",
                                  description="The AI bio is too long. Please set it within 1024 characters.\nAIã®bioãŒé•·ã™ãã¾ã™ã€‚1024æ–‡å­—ä»¥å†…ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚",
                                  color=discord.Color.gold())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        try:
            await self.bio_manager.set_channel_bio(interaction.channel_id, bio)
            logger.info(f"AI bio for channel {interaction.channel_id} set by {interaction.user.name}")
            embed = discord.Embed(title="âœ… AI Bio Set / AIã®bioã‚’è¨­å®šã—ã¾ã—ãŸ",
                                  description=f"The AI's role in this channel has been set as follows.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ã®AIã®å½¹å‰²ãŒä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã•ã‚Œã¾ã—ãŸã€‚\n\n**New AI Bio / æ–°ã—ã„AIã®bio:**\n```\n{bio}\n```",
                                  color=discord.Color.green())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save channel AI bio settings: {e}", exc_info=True)
            embed = discord.Embed(title="âŒ Save Error / ä¿å­˜ã‚¨ãƒ©ãƒ¼",
                                  description="Failed to save AI bio settings.\nAIã®bioè¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    @app_commands.command(name="show-ai-bio",
                          description="Show the AI's current bio for this channel.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIã«ç¾åœ¨è¨­å®šã•ã‚Œã¦ã„ã‚‹bioã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
    async def show_ai_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="BioManager is not available.\nBioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        current_bio = self.bio_manager.get_channel_bio(interaction.channel_id)
        if current_bio:
            title, description, color = "Current AI Bio / ç¾åœ¨ã®AIã®bio", f"In this channel, the AI has the following role set.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ã¯ã€AIã«ä»¥ä¸‹ã®å½¹å‰²ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚\n\n**AI Bio / AIã®bio:**\n```\n{current_bio}\n```", discord.Color.blue()
        else:
            default_prompt = self.llm_config.get('system_prompt', "Not set. / è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            try:
                formatted_prompt = default_prompt.format(current_date=datetime.now(self.jst).strftime('%Yå¹´%mæœˆ%dæ—¥'),
                                                         current_time=datetime.now(self.jst).strftime('%H:%M'))
            except (KeyError, ValueError):
                formatted_prompt = default_prompt
            title, description, color = "Current AI Bio / ç¾åœ¨ã®AIã®bio", f"No specific AI bio is set for this channel. The server's default setting is used.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«ã¯å°‚ç”¨ã®AI bioãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚µãƒ¼ãƒãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n\n**Default Setting / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š:**\n```\n{formatted_prompt}\n```", discord.Color.greyple()
        embed = discord.Embed(title=title, description=description, color=color)
        self._add_support_footer(embed)
        await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    @app_commands.command(name="reset-ai-bio",
                          description="Reset the AI's bio to default for this channel.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIã®bioã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã«æˆ»ã—ã¾ã™ã€‚")
    async def reset_ai_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="BioManager is not available.\nBioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        try:
            if await self.bio_manager.reset_channel_bio(interaction.channel_id):
                logger.info(f"AI bio for channel {interaction.channel_id} reset by {interaction.user.name}")
                default_prompt = self.llm_config.get('system_prompt', 'Not set / æœªè¨­å®š')
                try:
                    formatted_prompt = default_prompt.format(
                        current_date=datetime.now(self.jst).strftime('%Yå¹´%mæœˆ%dæ—¥'),
                        current_time=datetime.now(self.jst).strftime('%H:%M'))
                except (KeyError, ValueError):
                    formatted_prompt = default_prompt
                display_prompt = (formatted_prompt[:100] + '...') if len(formatted_prompt) > 103 else formatted_prompt
                embed = discord.Embed(title="âœ… AI Bio Reset / AIã®bioã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ",
                                      description=f"The AI bio for this channel has been reset to the default.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIã®bioã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã«æˆ»ã—ã¾ã—ãŸã€‚\n> Current Default / ç¾åœ¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `{display_prompt}`",
                                      color=discord.Color.green())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            else:
                embed = discord.Embed(title="â„¹ï¸ No Custom AI Bio / å°‚ç”¨ã®AI bioã¯ã‚ã‚Šã¾ã›ã‚“",
                                      description="No custom AI bio is set for this channel.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«ã¯å°‚ç”¨ã®AI bioãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                                      color=discord.Color.blue())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save channel AI bio settings after reset: {e}", exc_info=True)
            embed = discord.Embed(title="âŒ Save Error / ä¿å­˜ã‚¨ãƒ©ãƒ¼",
                                  description="Failed to save AI bio settings.\nAIã®bioè¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    @app_commands.command(name="set-user-bio",
                          description="Save your information for the AI to remember.\nAIã«ã‚ãªãŸã®æƒ…å ±ã‚’è¨˜æ†¶ã•ã›ã¾ã™ã€‚")
    @app_commands.describe(
        bio="Information about you for the AI to remember (e.g., My name is Tanaka. My hobby is reading.).\nAIã«è¦šãˆã¦ã»ã—ã„ã‚ãªãŸã®æƒ…å ±ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚(ä¾‹: ç§ã®åå‰ã¯ç”°ä¸­ã§ã™ã€‚è¶£å‘³ã¯èª­æ›¸ã§ã™ã€‚)",
        mode="Select save mode. 'Overwrite' or 'Append' is available.\nä¿å­˜ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚'ä¸Šæ›¸ã'ã¾ãŸã¯'è¿½è¨˜'ãŒå¯èƒ½ã§ã™ã€‚")
    @app_commands.choices(mode=[app_commands.Choice(name="Overwrite / ä¸Šæ›¸ã", value="overwrite"),
                                app_commands.Choice(name="Append / è¿½è¨˜", value="append"), ])
    async def set_user_bio_slash(self, interaction: discord.Interaction, bio: str, mode: app_commands.Choice[str]):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="BioManager is not available.\nBioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        if len(bio) > 1024:
            embed = discord.Embed(title="âš ï¸ Input Too Long / å…¥åŠ›ãŒé•·ã™ãã¾ã™",
                                  description="User bio is too long. Please set it within 1024 characters.\nãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±(bio)ãŒé•·ã™ãã¾ã™ã€‚1024æ–‡å­—ä»¥å†…ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚",
                                  color=discord.Color.gold())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        try:
            await self.bio_manager.set_user_bio(interaction.user.id, bio, mode=mode.value)
            logger.info(
                f"User bio for {interaction.user.name} ({interaction.user.id}) was set with mode '{mode.value}'.")
            updated_bio = self.bio_manager.get_user_bio(interaction.user.id)
            embed = discord.Embed(
                title=f"âœ… Your information has been saved ({mode.name}).\nâœ… ã‚ãªãŸã®æƒ…å ±ã‚’è¨˜æ†¶ã—ã¾ã—ãŸ ({mode.name})",
                description=f"The AI has stored your information as follows.\nAIã¯ã‚ãªãŸã®æƒ…å ±ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¨˜æ†¶ã—ã¾ã—ãŸã€‚\n\n**Your Bio / ã‚ãªãŸã®bio:**\n```\n{updated_bio}\n```",
                color=discord.Color.green())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save user bio settings: {e}", exc_info=True)
            embed = discord.Embed(title="âŒ Save Error / ä¿å­˜ã‚¨ãƒ©ãƒ¼",
                                  description="Failed to save your information.\nã‚ãªãŸã®æƒ…å ±ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    @app_commands.command(name="show-user-bio",
                          description="Show the information the AI has stored about you.\nAIãŒè¨˜æ†¶ã—ã¦ã„ã‚‹ã‚ãªãŸã®æƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
    async def show_user_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="BioManager is not available.\nBioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        current_bio = self.bio_manager.get_user_bio(interaction.user.id)
        if current_bio:
            embed = discord.Embed(
                title=f"ğŸ’¡ {interaction.user.display_name}'s Information / {interaction.user.display_name}ã•ã‚“ã®æƒ…å ±",
                description=f"**Bio:**\n```\n{current_bio}\n```", color=discord.Color.blue())
        else:
            embed = discord.Embed(
                title=f"ğŸ’¡ {interaction.user.display_name}'s Information / {interaction.user.display_name}ã•ã‚“ã®æƒ…å ±",
                description="Currently, no information is stored about you.\nYou can set it using the `/set-user-bio` command or by asking the AI to remember it in conversation.\nç¾åœ¨ã€ã‚ãªãŸã«é–¢ã™ã‚‹æƒ…å ±ã¯ä½•ã‚‚è¨˜æ†¶ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n`/set-user-bio` ã‚³ãƒãƒ³ãƒ‰ã‹ã€ä¼šè©±ã®ä¸­ã§AIã«è¨˜æ†¶ã‚’é ¼ã‚€ã“ã¨ã§è¨­å®šã§ãã¾ã™ã€‚",
                color=discord.Color.greyple())
        self._add_support_footer(embed)
        await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    @app_commands.command(name="reset-user-bio",
                          description="Delete all information the AI has stored about you.\nAIãŒè¨˜æ†¶ã—ã¦ã„ã‚‹ã‚ãªãŸã®æƒ…å ±ã‚’ã™ã¹ã¦å‰Šé™¤ã—ã¾ã™ã€‚")
    async def reset_user_bio_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.bio_manager:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="BioManager is not available.\nBioManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        try:
            if await self.bio_manager.reset_user_bio(interaction.user.id):
                logger.info(f"User bio for {interaction.user.name} ({interaction.user.id}) was reset.")
                embed = discord.Embed(title="âœ… Information Deleted / æƒ…å ±ã‚’å‰Šé™¤ã—ã¾ã—ãŸ",
                                      description=f"All information about {interaction.user.display_name} has been deleted.\n{interaction.user.display_name}ã•ã‚“ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ã™ã¹ã¦å‰Šé™¤ã—ã¾ã—ãŸã€‚",
                                      color=discord.Color.green())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            else:
                embed = discord.Embed(title="â„¹ï¸ No Information Stored / æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“",
                                      description="No information is stored about you.\nã‚ãªãŸã«é–¢ã™ã‚‹æƒ…å ±ã¯ä½•ã‚‚è¨˜æ†¶ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                                      color=discord.Color.blue())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save user bio settings after reset: {e}", exc_info=True)
            embed = discord.Embed(title="âŒ Deletion Error / å‰Šé™¤ã‚¨ãƒ©ãƒ¼",
                                  description="Failed to delete your information.\nã‚ãªãŸã®æƒ…å ±ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    @app_commands.command(name="memory-save",
                          description="Save information to the global shared memory.\nã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã«æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã™ã€‚")
    @app_commands.describe(
        key="The key for the information (e.g., 'Developer Announcement').\næƒ…å ±ã®ã‚­ãƒ¼ï¼ˆé …ç›®åï¼‰ ä¾‹: 'é–‹ç™ºè€…ã‹ã‚‰ã®ãŠçŸ¥ã‚‰ã›'",
        value="The content of the information (e.g., 'Next maintenance is...').\næƒ…å ±ã®å†…å®¹ ä¾‹: 'æ¬¡å›ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã¯...'")
    async def memory_save_slash(self, interaction: discord.Interaction, key: str, value: str):
        await interaction.response.defer(ephemeral=False)
        if not self.memory_manager:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="MemoryManager is not available.\nMemoryManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        try:
            await self.memory_manager.save_memory(key, value)
            embed = discord.Embed(title="âœ… Saved to Global Shared Memory / ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã—ã¾ã—ãŸ",
                                  color=discord.Color.green())
            embed.add_field(name="Key / ã‚­ãƒ¼", value=f"```{key}```", inline=False)
            embed.add_field(name="Value / å€¤", value=f"```{value}```", inline=False)
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save global memory via command: {e}", exc_info=True)
            embed = discord.Embed(title="âŒ Save Error / ä¿å­˜ã‚¨ãƒ©ãƒ¼",
                                  description="Failed to save to global shared memory.\nã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã¸ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    @app_commands.command(name="memory-list",
                          description="List all global shared memories.\nã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã®æƒ…å ±ã‚’ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚")
    async def memory_list_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.memory_manager:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="MemoryManager is not available.\nMemoryManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        memories = self.memory_manager.list_memories()
        if not memories:
            embed = discord.Embed(title="â„¹ï¸ No Memories / ãƒ¡ãƒ¢ãƒªã«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“",
                                  description="Nothing is saved in the global shared memory.\nã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã«ã¯ä½•ã‚‚ä¿å­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                                  color=discord.Color.blue())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        embed = discord.Embed(title="ğŸŒ Global Shared Memory / ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒª", color=discord.Color.blue())
        description = ""
        for key, value in memories.items():
            field_text = f"**{key}**: {value}\n"
            if len(description) + len(field_text) > 4000:
                description += "\n... (partially omitted due to display limit / è¡¨ç¤ºåˆ¶é™ã®ãŸã‚ä¸€éƒ¨çœç•¥)"
                break
            description += field_text
        embed.description = description
        self._add_support_footer(embed)
        await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    async def memory_key_autocomplete(self, interaction: discord.Interaction, current: str) -> List[
        app_commands.Choice[str]]:
        if not self.memory_manager: return []
        keys = self.memory_manager.list_memories().keys()
        return [app_commands.Choice(name=key, value=key) for key in keys if current.lower() in key.lower()][:25]

    @app_commands.command(name="memory-delete",
                          description="Delete a global shared memory.\nã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã‹ã‚‰æƒ…å ±ã‚’å‰Šé™¤ã—ã¾ã™ã€‚")
    @app_commands.describe(key="The key of the memory to delete.\nå‰Šé™¤ã—ãŸã„æƒ…å ±ã®ã‚­ãƒ¼")
    @app_commands.autocomplete(key=memory_key_autocomplete)
    async def memory_delete_slash(self, interaction: discord.Interaction, key: str):
        await interaction.response.defer(ephemeral=False)
        if not self.memory_manager:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="MemoryManager is not available.\nMemoryManagerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            return
        try:
            if await self.memory_manager.delete_memory(key):
                embed = discord.Embed(title="âœ… Memory Deleted / ãƒ¡ãƒ¢ãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ",
                                      description=f"Deleted key '{key}' from global shared memory.\nã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã‹ã‚‰ã‚­ãƒ¼ '{key}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚",
                                      color=discord.Color.green())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            else:
                embed = discord.Embed(title="âš ï¸ Key Not Found / ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                                      description=f"Key '{key}' does not exist in global shared memory.\nã‚­ãƒ¼ '{key}' ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚",
                                      color=discord.Color.gold())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to delete global memory via command: {e}", exc_info=True)
            embed = discord.Embed(title="âŒ Deletion Error / å‰Šé™¤ã‚¨ãƒ©ãƒ¼",
                                  description="Failed to delete from global shared memory.\nã‚°ãƒ­ãƒ¼ãƒãƒ«å…±æœ‰ãƒ¡ãƒ¢ãƒªã‹ã‚‰ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    async def model_autocomplete(self, interaction: discord.Interaction, current: str) -> List[
        app_commands.Choice[str]]:
        available_models = self.llm_config.get('available_models', [])
        return [app_commands.Choice(name=model, value=model) for model in available_models if
                current.lower() in model.lower()][:25]

    @app_commands.command(name="switch-models",
                          description="Switches the AI model used for this channel.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
    @app_commands.describe(model="Select the model you want to use.\nä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    @app_commands.autocomplete(model=model_autocomplete)
    async def switch_model_slash(self, interaction: discord.Interaction, model: str):
        await interaction.response.defer(ephemeral=False)
        available_models = self.llm_config.get('available_models', [])
        if model not in available_models:
            embed = discord.Embed(title="âš ï¸ Invalid Model / ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«",
                                  description=f"The specified model '{model}' is not available.\næŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ« '{model}' ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.gold())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())
            return
        channel_id, channel_id_str, default_model = interaction.channel_id, str(
            interaction.channel_id), self.llm_config.get('model')
        if channel_id in self.model_reset_tasks:
            self.model_reset_tasks[channel_id].cancel()
            self.model_reset_tasks.pop(channel_id, None)
            logger.info(f"Cancelled previous model reset task for channel {channel_id}.")
        self.channel_models[channel_id_str] = model
        try:
            await self._save_channel_models()
            await self._get_llm_client_for_channel(interaction.channel_id)
            if model != default_model:
                task = asyncio.create_task(self._schedule_model_reset(channel_id))
                self.model_reset_tasks[channel_id] = task
                embed = discord.Embed(title="âœ… Model Switched / ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ",
                                      description=f"The AI model for this channel has been switched to `{model}`.\nIt will automatically revert to the default model (`{default_model}`) **after 3 hours**.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ãŒ `{model}` ã«åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¾ã—ãŸã€‚\n**3æ™‚é–“å¾Œ**ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ« (`{default_model}`) ã«è‡ªå‹•çš„ã«æˆ»ã‚Šã¾ã™ã€‚",
                                      color=discord.Color.green())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
                logger.info(
                    f"Model for channel {channel_id} switched to '{model}' by {interaction.user.name}. Reset scheduled in 3 hours.")
            else:
                embed = discord.Embed(title="âœ… Model Reset to Default / ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã—ã¾ã—ãŸ",
                                      description=f"The AI model for this channel has been reset to the default `{model}`.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® `{model}` ã«æˆ»ã•ã‚Œã¾ã—ãŸã€‚",
                                      color=discord.Color.green())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
                logger.info(f"Model for channel {channel_id} switched to default '{model}' by {interaction.user.name}.")
        except Exception as e:
            logger.error(f"Failed to save channel model settings: {e}", exc_info=True)
            embed = discord.Embed(title="âŒ Save Error / ä¿å­˜ã‚¨ãƒ©ãƒ¼",
                                  description="Failed to save settings.\nè¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())

    @app_commands.command(name="switch-models-default-server",
                          description="Resets the AI model for this channel to the server default.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒ¼ãƒãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã«æˆ»ã—ã¾ã™ã€‚")
    async def reset_model_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        channel_id, channel_id_str = interaction.channel_id, str(interaction.channel_id)
        if channel_id in self.model_reset_tasks:
            self.model_reset_tasks[channel_id].cancel()
            self.model_reset_tasks.pop(channel_id, None)
            logger.info(f"Cancelled scheduled model reset for channel {channel_id} due to manual reset.")
        if channel_id_str in self.channel_models:
            del self.channel_models[channel_id_str]
            try:
                await self._save_channel_models()
                default_model = self.llm_config.get('model', 'Not set / æœªè¨­å®š')
                embed = discord.Embed(title="âœ… Model Reset to Default / ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã—ã¾ã—ãŸ",
                                      description=f"The AI model for this channel has been reset to the default (`{default_model}`).\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®AIãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (`{default_model}`) ã«æˆ»ã—ã¾ã—ãŸã€‚",
                                      color=discord.Color.green())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
                logger.info(f"Model for channel {interaction.channel_id} reset to default by {interaction.user.name}")
            except Exception as e:
                logger.error(f"Failed to save channel model settings after reset: {e}", exc_info=True)
                embed = discord.Embed(title="âŒ Save Error / ä¿å­˜ã‚¨ãƒ©ãƒ¼",
                                      description="Failed to save settings.\nè¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                                      color=discord.Color.red())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view())
        else:
            embed = discord.Embed(title="â„¹ï¸ No Custom Model Set / å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“",
                                  description="No custom model is set for this channel.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«ã¯å°‚ç”¨ã®ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                                  color=discord.Color.blue())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    @switch_model_slash.error
    async def switch_model_error(self, interaction: discord.Interaction, error: app_commands.AppCommandError):
        logger.error(f"Error in /switch-model command: {error}", exc_info=True)
        error_message = f"An unexpected error occurred: {error}\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}"
        embed = discord.Embed(title="âŒ Unexpected Error / äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼", description=error_message,
                              color=discord.Color.red())
        self._add_support_footer(embed)
        view = self._create_support_view()
        if not interaction.response.is_done():
            await interaction.response.send_message(embed=embed, view=view, ephemeral=False)
        else:
            await interaction.followup.send(embed=embed, view=view, ephemeral=False)

    async def image_model_autocomplete(self, interaction: discord.Interaction, current: str) -> List[
        app_commands.Choice[str]]:
        if not self.image_generator: return []
        available_models, current_lower = self.image_generator.get_available_models(), current.lower()
        filtered = [model for model in available_models if current_lower in model.lower()]
        if len(filtered) > 25:
            models_by_provider, choices = self.image_generator.get_models_by_provider(), []
            for provider, models in sorted(models_by_provider.items()):
                if current_lower in provider.lower():
                    for model in models[:5]:
                        if len(choices) >= 25: break
                        choices.append(app_commands.Choice(name=model, value=model))
                    if len(choices) >= 25: break
            return choices[:25]
        return [app_commands.Choice(name=model, value=model) for model in filtered][:25]

    @app_commands.command(name="switch-image-model",
                          description="Switch the image generation model for this channel. / ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
    @app_commands.describe(
        model="Select the image generation model you want to use. / ä½¿ç”¨ã—ãŸã„ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    @app_commands.autocomplete(model=image_model_autocomplete)
    async def switch_image_model_slash(self, interaction: discord.Interaction, model: str):
        await interaction.response.defer(ephemeral=False)
        if not self.image_generator:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="ImageGenerator is not available.\nImageGeneratorãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())
            return
        available_models = self.image_generator.get_available_models()
        if model not in available_models:
            embed = discord.Embed(title="âš ï¸ Invalid Model / ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«",
                                  description=f"The specified model `{model}` is not available.\næŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ« `{model}` ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.gold())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())
            return
        try:
            await self.image_generator.set_model_for_channel(interaction.channel_id, model)
            default_model = self.image_generator.default_model
            try:
                provider, model_name = model.split('/', 1)
            except ValueError:
                provider, model_name = "unknown", model
            if model != default_model:
                embed = discord.Embed(title="âœ… Image Model Switched / ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ",
                                      description="The image generation model for this channel has been switched.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸã€‚",
                                      color=discord.Color.green())
                embed.add_field(name="New Model / æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«", value=f"```\n{model}\n```", inline=False)
                embed.add_field(name="Provider / ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼", value=f"`{provider}`", inline=True)
                embed.add_field(name="Model Name / ãƒ¢ãƒ‡ãƒ«å", value=f"`{model_name}`", inline=True)
                embed.add_field(name="ğŸ’¡ Tip / ãƒ’ãƒ³ãƒˆ",
                                value=f"To reset to default (`{default_model}`), use `/reset-image-model`\nãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (`{default_model}`) ã«æˆ»ã™ã«ã¯ `/reset-image-model`",
                                inline=False)
            else:
                embed = discord.Embed(title="âœ… Image Model Set to Default / ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«è¨­å®šã—ã¾ã—ãŸ",
                                      description="The image generation model for this channel is now the default.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ãªã‚Šã¾ã—ãŸã€‚",
                                      color=discord.Color.green())
                embed.add_field(name="Model / ãƒ¢ãƒ‡ãƒ«", value=f"```\n{model}\n```", inline=False)
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
            logger.info(
                f"Image model for channel {interaction.channel_id} switched to '{model}' by {interaction.user.name}")
        except Exception as e:
            logger.error(f"Failed to save channel image model settings: {e}", exc_info=True)
            embed = discord.Embed(title="âŒ Save Error / ä¿å­˜ã‚¨ãƒ©ãƒ¼",
                                  description="Failed to save settings.\nè¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())

    @app_commands.command(name="reset-image-model",
                          description="Reset the image generation model to default for this channel. / ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã—ã¾ã™ã€‚")
    async def reset_image_model_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.image_generator:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="ImageGenerator is not available.\nImageGeneratorãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())
            return
        try:
            if await self.image_generator.reset_model_for_channel(interaction.channel_id):
                default_model = self.image_generator.default_model
                embed = discord.Embed(title="âœ… Image Model Reset to Default / ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã—ã¾ã—ãŸ",
                                      description="The image generation model for this channel has been reset to the default.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã—ã¾ã—ãŸã€‚",
                                      color=discord.Color.green())
                embed.add_field(name="Default Model / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«", value=f"```\n{default_model}\n```",
                                inline=False)
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
                logger.info(
                    f"Image model for channel {interaction.channel_id} reset to default by {interaction.user.name}")
            else:
                embed = discord.Embed(title="â„¹ï¸ No Custom Model Set / å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“",
                                      description="No custom image generation model is set for this channel.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«ã¯å°‚ç”¨ã®ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                                      color=discord.Color.blue())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)
        except Exception as e:
            logger.error(f"Failed to save channel image model settings after reset: {e}", exc_info=True)
            embed = discord.Embed(title="âŒ Save Error / ä¿å­˜ã‚¨ãƒ©ãƒ¼",
                                  description="Failed to save settings.\nè¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())

    @app_commands.command(name="show-image-model",
                          description="Show the current image generation model for this channel. / ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ç¾åœ¨ã®ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
    async def show_image_model_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        if not self.image_generator:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="ImageGenerator is not available.\nImageGeneratorãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())
            return
        current_model, default_model, is_default = self.image_generator.get_model_for_channel(
            interaction.channel_id), self.image_generator.default_model, self.image_generator.get_model_for_channel(
            interaction.channel_id) == self.image_generator.default_model
        try:
            provider, model_name = current_model.split('/', 1)
        except ValueError:
            provider, model_name = "unknown", current_model
        embed = discord.Embed(title="ğŸ¨ Current Image Generation Model / ç¾åœ¨ã®ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«",
                              color=discord.Color.blue() if is_default else discord.Color.purple())
        embed.add_field(name="Current Model / ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«", value=f"```\n{current_model}\n```", inline=False)
        embed.add_field(name="Provider / ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼", value=f"`{provider}`", inline=True)
        embed.add_field(name="Status / çŠ¶æ…‹", value='`Default / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ`' if is_default else '`Custom / ã‚«ã‚¹ã‚¿ãƒ `',
                        inline=True)
        models_by_provider = self.image_generator.get_models_by_provider()
        for provider_name, models in sorted(models_by_provider.items()):
            model_list = "\n".join([f"â€¢ `{m.split('/', 1)[1]}`" for m in models[:5]])
            if len(models) > 5: model_list += f"\nâ€¢ ... and {len(models) - 5} more"
            embed.add_field(name=f"ğŸ“¦ {provider_name.title()} Models", value=model_list or "None", inline=True)
        embed.add_field(name="ğŸ’¡ Commands / ã‚³ãƒãƒ³ãƒ‰",
                        value="â€¢ `/switch-image-model` - Change model / ãƒ¢ãƒ‡ãƒ«å¤‰æ›´\nâ€¢ `/reset-image-model` - Reset to default / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™",
                        inline=False)
        self._add_support_footer(embed)
        await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    @app_commands.command(name="list-image-models",
                          description="List all available image generation models. / åˆ©ç”¨å¯èƒ½ãªç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
    @app_commands.describe(provider="Filter by provider (optional). / ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§çµã‚Šè¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
    async def list_image_models_slash(self, interaction: discord.Interaction, provider: str = None):
        await interaction.response.defer(ephemeral=False)
        if not self.image_generator:
            embed = discord.Embed(title="âŒ Plugin Error / ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
                                  description="ImageGenerator is not available.\nImageGeneratorãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚",
                                  color=discord.Color.red())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())
            return
        models_by_provider = self.image_generator.get_models_by_provider()
        if provider:
            provider_lower = provider.lower()
            models_by_provider = {k: v for k, v in models_by_provider.items() if provider_lower in k.lower()}
            if not models_by_provider:
                embed = discord.Embed(title="âš ï¸ No Models Found / ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                                      description=f"No models found for provider: `{provider}`\nãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ `{provider}` ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚",
                                      color=discord.Color.gold())
                self._add_support_footer(embed)
                await interaction.followup.send(embed=embed, view=self._create_support_view())
                return
        total_models = sum(len(models) for models in models_by_provider.values())
        embed = discord.Embed(title="ğŸ¨ Available Image Generation Models / åˆ©ç”¨å¯èƒ½ãªç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«",
                              description=f"Total: {total_models} models across {len(models_by_provider)} provider(s)\nåˆè¨ˆ: {len(models_by_provider)}ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã€{total_models}ãƒ¢ãƒ‡ãƒ«",
                              color=discord.Color.blue())
        for provider_name, models in sorted(models_by_provider.items()):
            model_names = [m.split('/', 1) for m in models]
            if len(model_names) > 10:
                model_text = "\n".join([f"{i + 1}. `{m}`" for i, m in enumerate(model_names[:10])])
                model_text += f"\n... and {len(model_names) - 10} more"
            else:
                model_text = "\n".join([f"{i + 1}. `{m}`" for i, m in enumerate(model_names)])
            embed.add_field(name=f"ğŸ“¦ {provider_name.title()} ({len(models)} models)", value=model_text or "None",
                            inline=False)
        embed.add_field(name="ğŸ’¡ How to Use / ä½¿ã„æ–¹",
                        value="Use `/switch-image-model` to change the model for this channel.\n`/switch-image-model` ã§ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã§ãã¾ã™ã€‚",
                        inline=False)
        self._add_support_footer(embed)
        await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    @switch_image_model_slash.error
    async def switch_image_model_error(self, interaction: discord.Interaction, error: app_commands.AppCommandError):
        logger.error(f"Error in /switch-image-model command: {error}", exc_info=True)
        error_message = f"An unexpected error occurred: {error}\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}"
        embed = discord.Embed(title="âŒ Unexpected Error / äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼", description=error_message,
                              color=discord.Color.red())
        self._add_support_footer(embed)
        view = self._create_support_view()
        if not interaction.response.is_done():
            await interaction.response.send_message(embed=embed, view=view, ephemeral=False)
        else:
            await interaction.followup.send(embed=embed, view=view, ephemeral=False)

    @app_commands.command(name="llm_help",
                          description="Displays help and usage guidelines for LLM (AI Chat) features.\nLLM (AIå¯¾è©±) æ©Ÿèƒ½ã®ãƒ˜ãƒ«ãƒ—ã¨åˆ©ç”¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
    async def llm_help_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        bot_user = self.bot.user or interaction.client.user
        bot_name = bot_user.name if bot_user else "This Bot / å½“Bot"
        embed = discord.Embed(title=f"ğŸ’¡ {bot_name} AI Chat Help & Guidelines / AIå¯¾è©±æ©Ÿèƒ½ãƒ˜ãƒ«ãƒ—ï¼†ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
                              description=f"Explanation and terms of use for the AI chat features.\n{bot_name}ã®AIå¯¾è©±æ©Ÿèƒ½ã«ã¤ã„ã¦ã®èª¬æ˜ã¨åˆ©ç”¨è¦ç´„ã§ã™ã€‚",
                              color=discord.Color.purple())
        if bot_user and bot_user.avatar: embed.set_thumbnail(url=bot_user.avatar.url)
        embed.add_field(name="Basic Usage / åŸºæœ¬çš„ãªä½¿ã„æ–¹",
                        value=f"â€¢ Mention the bot (`@{bot_name}`) to get a response from the AI.\n  Botã«ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ (`@{bot_name}`) ã—ã¦è©±ã—ã‹ã‘ã‚‹ã¨ã€AIãŒå¿œç­”ã—ã¾ã™ã€‚\nâ€¢ **You can also continue the conversation by replying to the bot's messages (no mention needed).**\n  **Botã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿”ä¿¡ã™ã‚‹ã“ã¨ã§ã‚‚ä¼šè©±ã‚’ç¶šã‘ã‚‰ã‚Œã¾ã™ï¼ˆãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ä¸è¦ï¼‰ã€‚**\nâ€¢ If you ask the AI to remember something, it will try to store that information.\n  ã€Œç§ã®åå‰ã¯ã€‡ã€‡ã§ã™ã€‚è¦šãˆã¦ãŠã„ã¦ã€ã®ã‚ˆã†ã«è©±ã—ã‹ã‘ã‚‹ã¨ã€AIãŒã‚ãªãŸã®æƒ…å ±ã‚’è¨˜æ†¶ã—ã‚ˆã†ã¨ã—ã¾ã™ã€‚\nâ€¢ Attach images or paste image URLs with your message, and the AI will try to understand them.\n  ç”»åƒã¨ä¸€ç·’ã«è©±ã—ã‹ã‘ã‚‹ã¨ã€AIãŒç”»åƒã®å†…å®¹ã‚‚ç†è§£ã—ã‚ˆã†ã¨ã—ã¾ã™ã€‚",
                        inline=False)
        embed.add_field(name="Useful Commands / ä¾¿åˆ©ãªã‚³ãƒãƒ³ãƒ‰",
                        value="**[AI Settings (Per Channel) / AIã®è¨­å®š (ãƒãƒ£ãƒ³ãƒãƒ«ã”ã¨)]**\nâ€¢ `/switch-models`: Change the AI model used in this channel. / ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã§ä½¿ã†AIãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã—ã¾ã™ã€‚\nâ€¢ `/set-ai-bio`: Set a custom personality/role for the AI in this channel. / ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«å°‚ç”¨ã®AIã®æ€§æ ¼ã‚„å½¹å‰²ã‚’è¨­å®šã—ã¾ã™ã€‚\nâ€¢ `/show-ai-bio`: Check the current AI bio setting. / ç¾åœ¨ã®AIã®bioè¨­å®šã‚’ç¢ºèªã—ã¾ã™ã€‚\nâ€¢ `/reset-ai-bio`: Reset the AI bio to the default. / AIã®bioè¨­å®šã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã—ã¾ã™ã€‚\n**[Your Information / ã‚ãªãŸã®æƒ…å ±]**\nâ€¢ `/set-user-bio`: Set information about you for the AI to remember. / AIã«è¦šãˆã¦ã»ã—ã„ã‚ãªãŸã®æƒ…å ±ã‚’è¨­å®šã—ã¾ã™ã€‚\nâ€¢ `/show-user-bio`: Check the information the AI has stored about you. / AIãŒè¨˜æ†¶ã—ã¦ã„ã‚‹ã‚ãªãŸã®æƒ…å ±ã‚’ç¢ºèªã—ã¾ã™ã€‚\nâ€¢ `/reset-user-bio`: Delete your information from the AI's memory. / ã‚ãªãŸã®æƒ…å ±ã‚’AIã®è¨˜æ†¶ã‹ã‚‰å‰Šé™¤ã—ã¾ã™ã€‚\n**[Global Memory / ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒª]**\nâ€¢ `/memory-save`: Save information to the global shared memory. / å…¨ã‚µãƒ¼ãƒãƒ¼å…±é€šã®ãƒ¡ãƒ¢ãƒªã«æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã™ã€‚\nâ€¢ `/memory-list`: List all information in the global memory. / ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã®æƒ…å ±ã‚’ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚\nâ€¢ `/memory-delete`: Delete information from the global memory. / ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã‹ã‚‰æƒ…å ±ã‚’å‰Šé™¤ã—ã¾ã™ã€‚\n**[Other / ãã®ä»–]**\nâ€¢ `/clear_history`: Reset the conversation history. / ä¼šè©±å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã™ã€‚",
                        inline=False)
        channel_model_str = self.channel_models.get(str(interaction.channel_id))
        model_display = f"`{channel_model_str}` (Channel-specific / ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«å°‚ç”¨)" if channel_model_str else f"`{self.llm_config.get('model', 'Not set / æœªè¨­å®š')}` (Default / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)"
        ai_bio_display, user_bio_display = "N/A", "N/A"
        if self.bio_manager:
            ai_bio_display = "âœ… (Custom / å°‚ç”¨è¨­å®šã‚ã‚Š)" if self.bio_manager.get_channel_bio(
                interaction.channel_id) else "Default / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ"
            user_bio_display = "âœ… (Stored / è¨˜æ†¶ã‚ã‚Š)" if self.bio_manager.get_user_bio(
                interaction.user.id) else "None / ãªã—"
        active_tools = self.llm_config.get('active_tools', [])
        tools_info = "â€¢ None / ãªã—" if not active_tools else "â€¢ " + ", ".join(active_tools)
        embed.add_field(name="Current AI Settings / ç¾åœ¨ã®AIè¨­å®š",
                        value=f"â€¢ **Model in Use / ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:** {model_display}\nâ€¢ **AI Role (Channel) / AIã®å½¹å‰²(ãƒãƒ£ãƒ³ãƒãƒ«):** {ai_bio_display} (see `/show-ai-bio`)\nâ€¢ **Your Info / ã‚ãªãŸã®æƒ…å ±:** {user_bio_display} (see `/show-user-bio`)\nâ€¢ **Max Conversation History / ä¼šè©±å±¥æ­´ã®æœ€å¤§ä¿æŒæ•°:** {self.llm_config.get('max_messages', 'Not set / æœªè¨­å®š')} pairs\nâ€¢ **Max Images at Once / ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹æœ€å¤§ç”»åƒæšæ•°:** {self.llm_config.get('max_images', 'Not set / æœªè¨­å®š')} image(s)\nâ€¢ **Available Tools / åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:** {tools_info}",
                        inline=False)
        embed.add_field(name="--- ğŸ“œ AI Usage Guidelines / AIåˆ©ç”¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ ---",
                        value="Please review the following to ensure safe use of the AI features.\nAIæ©Ÿèƒ½ã‚’å®‰å…¨ã«ã”åˆ©ç”¨ã„ãŸã ããŸã‚ã€ä»¥ä¸‹ã®å†…å®¹ã‚’å¿…ãšã”ç¢ºèªãã ã•ã„ã€‚",
                        inline=False)
        embed.add_field(name="âš ï¸ 1. Data Input Precautions / ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ™‚ã®æ³¨æ„",
                        value="**NEVER include personal or confidential information** such as your name, contact details, or passwords.\nAIã«è¨˜æ†¶ã•ã›ã‚‹æƒ…å ±ã«ã¯ã€æ°åã€é€£çµ¡å…ˆã€ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãªã©ã®**å€‹äººæƒ…å ±ã‚„ç§˜å¯†æƒ…å ±ã‚’çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚**",
                        inline=False)
        embed.add_field(name="âœ… 2. Precautions for Using Generated Output / ç”Ÿæˆç‰©åˆ©ç”¨æ™‚ã®æ³¨æ„",
                        value="The AI's responses may contain inaccuracies or biases. **Always fact-check and use them at your own risk.**\nAIã®å¿œç­”ã«ã¯è™šå½ã‚„åè¦‹ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚**å¿…ãšãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã€è‡ªå·±ã®è²¬ä»»ã§åˆ©ç”¨ã—ã¦ãã ã•ã„ã€‚**",
                        inline=False)
        embed.set_footer(
            text="These guidelines are subject to change without notice.\nã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¯äºˆå‘Šãªãå¤‰æ›´ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")
        self._add_support_footer(embed)
        await interaction.followup.send(embed=embed, view=self._create_support_view(), ephemeral=False)

    @app_commands.command(name="clear_history",
                          description="Clears the history of the current conversation thread.\nç¾åœ¨ã®ä¼šè©±ã‚¹ãƒ¬ãƒƒãƒ‰ã®å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚")
    async def clear_history_slash(self, interaction: discord.Interaction):
        await interaction.response.defer(ephemeral=False)
        guild_id = interaction.guild.id if interaction.guild else 0  # DMã®å ´åˆã¯0
        cleared_count, threads_to_clear = 0, set()
        
        try:
            async for msg in interaction.channel.history(limit=200):
                if guild_id in self.message_to_thread and msg.id in self.message_to_thread[guild_id]: 
                    threads_to_clear.add(self.message_to_thread[guild_id][msg.id])
        except (discord.Forbidden, discord.HTTPException):
            embed = discord.Embed(title="âš ï¸ Permission Error / æ¨©é™ã‚¨ãƒ©ãƒ¼",
                                  description="Could not read the channel's message history.\nãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚",
                                  color=discord.Color.gold())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())
            return
        
        for thread_id in threads_to_clear:
            if guild_id in self.conversation_threads and thread_id in self.conversation_threads[guild_id]:
                del self.conversation_threads[guild_id][thread_id]
                if guild_id in self.message_to_thread:
                    self.message_to_thread[guild_id] = {
                        k: v for k, v in self.message_to_thread[guild_id].items() 
                        if v != thread_id
                    }
                cleared_count += 1
        
        if cleared_count > 0:
            embed = discord.Embed(title="âœ… History Cleared / å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ",
                                  description=f"Cleared the history of {cleared_count} conversation thread(s) related to this channel.\nã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«é–¢é€£ã™ã‚‹ {cleared_count} å€‹ã®ä¼šè©±ã‚¹ãƒ¬ãƒƒãƒ‰ã®å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚",
                                  color=discord.Color.green())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())
        else:
            embed = discord.Embed(title="â„¹ï¸ No History Found / å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“",
                                  description="No conversation history to clear was found.\nã‚¯ãƒªã‚¢å¯¾è±¡ã®ä¼šè©±å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
                                  color=discord.Color.blue())
            self._add_support_footer(embed)
            await interaction.followup.send(embed=embed, view=self._create_support_view())


async def setup(bot: commands.Bot):
    """Sets up the LLMCog."""
    try:
        await bot.add_cog(LLMCog(bot))
        logger.info("LLMCog loaded successfully.")
    except Exception as e:
        logger.critical(f"Failed to set up LLMCog: {e}", exc_info=True)
        raise